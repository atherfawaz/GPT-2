{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /mnt/batch/tasks/shared/LS_root/mounts/clusters/gptcomputeinstance/code/users/atherfawaz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:filelock:Lock 140215449061976 acquired on cache_dir/f20f05d3ae37c4e3cd56764d48e566ea5adeba153dcee6eb82a18822c9c731ec.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71.lock\n",
      "Downloading: 100%|█████████████████████████| 1.04M/1.04M [00:00<00:00, 37.2MB/s]\n",
      "INFO:filelock:Lock 140215449061976 released on cache_dir/f20f05d3ae37c4e3cd56764d48e566ea5adeba153dcee6eb82a18822c9c731ec.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71.lock\n",
      "INFO:filelock:Lock 140213192870432 acquired on cache_dir/6d882670c55563617571fe0c97df88626fb5033927b40fc18a8acf98dafd4946.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
      "Downloading: 100%|███████████████████████████| 456k/456k [00:00<00:00, 27.6MB/s]\n",
      "INFO:filelock:Lock 140213192870432 released on cache_dir/6d882670c55563617571fe0c97df88626fb5033927b40fc18a8acf98dafd4946.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda.lock\n",
      "INFO:filelock:Lock 140215449061976 acquired on cache_dir/98aa65385e18b0efd17acd8bf64dcdf21406bb0c99c801c2d3c9f6bfd1f48f29.250d6dc755ccb17d19c7c1a7677636683aa35f0f6cb5461b3c0587bc091551a0.lock\n",
      "Downloading: 100%|█████████████████████████████| 718/718 [00:00<00:00, 1.06MB/s]\n",
      "INFO:filelock:Lock 140215449061976 released on cache_dir/98aa65385e18b0efd17acd8bf64dcdf21406bb0c99c801c2d3c9f6bfd1f48f29.250d6dc755ccb17d19c7c1a7677636683aa35f0f6cb5461b3c0587bc091551a0.lock\n",
      "INFO:filelock:Lock 140215449061976 acquired on cache_dir/64652c50e84ddabb9bad81a37ff82624ab70053f402f8d9a58c0e90fb8289fb6.8769029be4f66a5ae1055eefdd1d11621b901d510654266b8681719fff492d6e.lock\n",
      "Downloading: 100%|█████████████████████████| 1.52G/1.52G [00:24<00:00, 61.3MB/s]\n",
      "INFO:filelock:Lock 140215449061976 released on cache_dir/64652c50e84ddabb9bad81a37ff82624ab70053f402f8d9a58c0e90fb8289fb6.8769029be4f66a5ae1055eefdd1d11621b901d510654266b8681719fff492d6e.lock\n",
      "WARNING:transformers.modeling_utils:Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "=============================================================================\n",
      "['Despite the recent successes of deep learning, such models are still far from some human abilities like learning from few examples, reasoning and explaining decisions. In this paper, we focus on organ annotation in medical images and we introduce a reasoning framework that is based on learning fuzzy relations on a small dataset for generating explanations. With our framework, we can explain about 1 to 2 times more explanations in a given image than simple rules.\\n\\nIn this paper, we show that this is possible since our approach is very fast and low on dependencies. We demonstrate that we can solve some challenges for non-deep learning models, like reasoning about faces in images and understand the behavior of deep neural networks.\\n\\nWe show that this approach could be used in a general purpose deep learning framework in terms of text analysis (or text classification) and visual prediction systems (which are often used in natural language processing applications).\\n\\nResults\\n\\nThis paper gives an overview about the ideas and our work on it. Below we present']\n",
      "=============================================================================\n",
      "WARNING:transformers.modeling_utils:Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "=============================================================================\n",
      "[\"There is a growing interest and literature on intrinsic motivations and open-ended learning in both cognitive robotics and machine learning on one side, and in psychology and neuroscience on the other. This paper aims to review some relevant contributions from the two literature threads and to draw links between them. In this analysis, an implicit measure of motivation emerges out of the field's focus on motivation. There is a lot that has been learned from psychology and neuroscience of what motivates us – about how much we enjoy the activities we engage in, how much we care about others, and about our relationships with others – even when we are engaged in other less important activities.\\n\\nThe field has a special insight into human motivations that has been developed over many decades of research, and has the potential to extend the power of neuroscientific findings beyond cognition.\\n\\nHowever, when asked the question of the intrinsic motivations of learning, some researchers (as well as social scientists, for example) have suggested a variety of reasons. There is\"]\n",
      "=============================================================================\n",
      "WARNING:transformers.modeling_utils:Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "=============================================================================\n",
      "['Recent success of pre-trained language models (LMs) has spurred widespread interest in the language capabilities that they possess. However, efforts to understand whether LM representations are useful for symbolic reasoning tasks have been limited and scattered. Recent work has looked specifically at perceptual reasoning. This project uses the fact that language models encode information in context-specific manner to investigate the neural mechanisms involved in representation of word-level symbolic information, specifically the semantic content. In particular, this research aims to further explore the neural mechanisms underlying the representation of word-level symbolic information.\\n\\nObjective 1 – Establishing and using functional MRI tools to map semantic representations to percepts A functional magnetic resonance imaging (fMRI) technique to study semantic representations in human language models has recently gained widespread use (Langdon 2005). We have shown that using a method based on functional magnetic resonance imaging (fMRI) and the BOLD signal-to-noise ratio technique (Vanden Berghe 2008), we can identify']\n",
      "=============================================================================\n",
      "WARNING:transformers.modeling_utils:Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "=============================================================================\n",
      "['Many theories, based on neuroscientific and psychological empirical evidence and on computational concepts, have been elaborated to explain the emergence of consciousness in the central nervous system. These theories propose key fundamental mechanisms to explain consciousness, but they only partially connect such mechanisms to the possible functional and adaptive role of consciousness. However, for the present review, the following models will be considered.\\n\\nIn the early 1990s, the term \"neurology\" (i.e., the study of consciousness) was introduced to describe a system of brain and nervous system processes and processes that are the key elements of consciousness, which is described in the concept of an \"emotional brain.\" This definition is important because the concept of consciousness is one of the most common and widely understood, but it is also controversial, and scientists have difficulty defining it and explaining its complexity. For instance, it has long been known that \"consciousness\" is a psychological phenomenon, but the term \"consciousness\" itself is controversial. It might']\n",
      "=============================================================================\n",
      "WARNING:transformers.modeling_utils:Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "=============================================================================\n",
      "['I failed the first quarter of a class in middle school, so I made a fake report card. I did this every quarter that year. I forgot that they mail home the end-of-year cards, and my mom got it before I could intercept with my fake. She was PISSED—at the school for their error. Then she was pissed at me. The day before I met her the school told me my mom was going out to dinner. She hadn\\'t seen me for two years. When I told her my mother was moving in with her dad, I was told I was wrong, that I should tell her they never sent her that fake. She never had that fake, and I didn\\'t get it at all. My mom told me to tell the school they never sent me that fake. She said, \"I didn\\'t send you that fake.\" At lunch one morning—I think it was this year—they showed me what the report card looks like. I didn']\n",
      "=============================================================================\n",
      "WARNING:transformers.modeling_utils:Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "=============================================================================\n",
      "['The firm has reportedly agreed to pay over $1 billion to acquire Zoox. This will give Amazon control of the 1000-person startup; however, Zoox is expected to operate as an independent subsidiary.\\n\\nAmazon already operates online retailers such as the popular AmazonFresh and the Prime Day delivery service. It announced at the end of October that it was expanding to two more countries to bring Alexa devices and Alexa voice control to its customers.<|endoftext|>']\n",
      "=============================================================================\n",
      "WARNING:transformers.modeling_utils:Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "=============================================================================\n",
      "[\"Google is rolling out a new Verified Calls feature for the Phone app to let a user know that they are receiving a call from a business and the reason behind it. The company explains that the Verified Calls feature is meant to avoid scams and fraud calls by verifying the identity of the business that is calling, display their reason for calling as well as their business logo. All these details will be shown on the incoming call screen so a user knows that they are receiving a genuine call from a business and its purpose without picking the phone up. Google also confirms that a user's private data is not shared with the businesses at any time during this process.\\n\\nThis is just the tip of the iceberg when it comes to security with Phone applications. We'll keep updating and sharing any new info about phone security so keep your eyes and ears peeled for any new tips and tricks you may find on our Twitter, Facebook and Google + pages!<|endoftext|>\"]\n",
      "=============================================================================\n",
      "WARNING:transformers.modeling_utils:Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "=============================================================================\n",
      "[\"According to a new report, Qualcomm could be readying a new Snapdragon 8cx+ chipset for Windows 10 PCs. As is typically the case with Qualcomm 'plus' chips, it offers a higher clock speed. Both 8cx processors feature support for the latest USB 3.0 connectivity technology. The Snapdragon 8x chip includes 10nm manufacturing process, a larger LPDDR2 battery and larger GPU, and it runs on its own hardware.\\n\\nIt seems that while the 8cx chip will have an unlocked bootloader, the Snapdragon 8cx+ will have software only. We're hearing that, in addition, it's still unconfirmed whether the Snapdragon 8cx chip would have an unlocked bootloader like the 8c chip.\\n\\nIn fact, we're not sure if the Snapdragon 8cx+ chip would be unlocked like the 8c chip. It's still unclear if the Snapdragon 8+ chip will have an unlockable bootloader or if it has only software\"]\n",
      "=============================================================================\n",
      "WARNING:transformers.modeling_utils:Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "=============================================================================\n",
      "[\"According to a new report, Qualcomm could be readying a new Snapdragon 8cx+ chipset for Windows 10 PCs. As is typically the case with Qualcomm 'plus' chips, it offers a higher clock speed. The high-end models are said to deliver over 20% better than their predecessors. The 8cx chip supports OpenGL 4.3, DirectX 12, and Vulkan APIs. The Snapdragon 8cx+ chipset would provide a decent performance boost for Windows 10 PC gaming, but it is not recommended for long term use.<|endoftext|>\"]\n",
      "=============================================================================\n"
     ]
    }
   ],
   "source": [
    "!python generate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python data_prep.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.language_modeling.language_modeling_utils: Creating features from dataset file at cache_dir/\n",
      " 48%|████████████████▍                 | 290001/598845 [01:06<05:20, 963.49it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (145329 > 100000). Running this sequence through the model will result in indexing errors\n",
      " 88%|████████████████████████████▉    | 524954/598845 [02:00<00:26, 2815.77it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (177591 > 100000). Running this sequence through the model will result in indexing errors\n",
      "100%|█████████████████████████████████| 598845/598845 [02:17<00:00, 4358.33it/s]\n",
      "100%|████████████████████████████████| 338101/338101 [00:09<00:00, 37393.20it/s]\n",
      "INFO:simpletransformers.language_modeling.language_modeling_utils: Saving features into cached file cache_dir/gpt2_cached_lm_508_train.txt\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda/envs/azureml_py36/lib/python3.6/site-packages/simpletransformers/language_modeling/language_modeling_model.py\", line 457, in train\n",
      "    from apex import amp\n",
      "ModuleNotFoundError: No module named 'apex'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"fine_tune.py\", line 24, in <module>\n",
      "    model.train_model(\"data/train.txt\", eval_file=\"data/test.txt\")\n",
      "  File \"/anaconda/envs/azureml_py36/lib/python3.6/site-packages/simpletransformers/language_modeling/language_modeling_model.py\", line 383, in train_model\n",
      "    **kwargs,\n",
      "  File \"/anaconda/envs/azureml_py36/lib/python3.6/site-packages/simpletransformers/language_modeling/language_modeling_model.py\", line 459, in train\n",
      "    raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
      "ImportError: Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\n"
     ]
    }
   ],
   "source": [
    "!python fine_tune.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'apex'...\n",
      "remote: Enumerating objects: 38, done.\u001b[K\n",
      "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
      "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
      "remote: Total 7293 (delta 20), reused 19 (delta 6), pack-reused 7255\u001b[K\n",
      "Receiving objects: 100% (7293/7293), 13.87 MiB | 6.68 MiB/s, done.\n",
      "Resolving deltas: 100% (4919/4919), done.\n",
      "Checking connectivity... done.\n",
      "Checking out files: 100% (290/290), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/NVIDIA/apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/gptcomputeinstance/code/users/atherfawaz/simpletransformers/examples/language_generation/apex\n"
     ]
    }
   ],
   "source": [
    "%cd apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sudo: pip: command not found\n"
     ]
    }
   ],
   "source": [
    "!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" --user ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -v --no-cache-dir ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting setup.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup.sh\n",
    "\n",
    "pip install -v --no-cache-dir ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/gptcomputeinstance/code/Users/atherfawaz/simpletransformers/examples/language_generation\n"
     ]
    }
   ],
   "source": [
    "%cd simpletransformers/examples/language_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/gptcomputeinstance/code/Users/atherfawaz/simpletransformers/examples/language_generation/apex\n"
     ]
    }
   ],
   "source": [
    "%cd apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: nvcc: not found\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement nvcc (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for nvcc\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nvcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.4+cu100\n",
      "  Downloading https://download.pytorch.org/whl/cu100/torch-1.4.0%2Bcu100-cp36-cp36m-linux_x86_64.whl (723.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 723.9 MB 4.4 kB/s  eta 0:00:01    |█▍                              | 32.5 MB 51.4 MB/s eta 0:00:14     |██████▋                         | 149.3 MB 87.4 MB/s eta 0:00:07     |████████▉                       | 198.9 MB 87.4 MB/s eta 0:00:07     |███████████                     | 251.3 MB 69.9 MB/s eta 0:00:07     |█████████████████▎              | 391.3 MB 82.7 MB/s eta 0:00:05�▉              | 402.3 MB 82.7 MB/s eta 0:00:04\n",
      "\u001b[?25hCollecting torchvision==0.5.0+cu100\n",
      "  Downloading https://download.pytorch.org/whl/cu100/torchvision-0.5.0%2Bcu100-cp36-cp36m-linux_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 27.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from torchvision==0.5.0+cu100) (6.2.1)\n",
      "Requirement already satisfied: six in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from torchvision==0.5.0+cu100) (1.12.0)\n",
      "Requirement already satisfied: numpy in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from torchvision==0.5.0+cu100) (1.16.2)\n",
      "Installing collected packages: torch, torchvision\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.4.0\n",
      "    Uninstalling torch-1.4.0:\n",
      "      Successfully uninstalled torch-1.4.0\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.5.0\n",
      "    Uninstalling torchvision-0.5.0:\n",
      "      Successfully uninstalled torchvision-0.5.0\n",
      "Successfully installed torch-1.4.0+cu100 torchvision-0.5.0+cu100\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.4+cu100 torchvision==0.5.0+cu100 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "torch.__version__  = 1.4.0+cu100\n",
      "\n",
      "\n",
      "setup.py:51: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
      "  warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
      "\n",
      "Compiling cuda extensions with\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2018 NVIDIA Corporation\n",
      "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
      "Cuda compilation tools, release 10.0, V10.0.130\n",
      "from /usr/local/cuda/bin\n",
      "\n",
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "creating apex.egg-info\n",
      "writing apex.egg-info/PKG-INFO\n",
      "writing dependency_links to apex.egg-info/dependency_links.txt\n",
      "writing top-level names to apex.egg-info/top_level.txt\n",
      "writing manifest file 'apex.egg-info/SOURCES.txt'\n",
      "writing manifest file 'apex.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib.linux-x86_64-3.6\n",
      "creating build/lib.linux-x86_64-3.6/apex\n",
      "copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\n",
      "creating build/lib.linux-x86_64-3.6/apex/amp\n",
      "copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "creating build/lib.linux-x86_64-3.6/apex/contrib\n",
      "copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib\n",
      "creating build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
      "copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
      "copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
      "copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
      "copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
      "creating build/lib.linux-x86_64-3.6/apex/mlp\n",
      "copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
      "copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
      "creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
      "copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
      "copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
      "creating build/lib.linux-x86_64-3.6/apex/normalization\n",
      "copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
      "copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
      "creating build/lib.linux-x86_64-3.6/apex/optimizers\n",
      "copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
      "copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
      "copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
      "copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
      "copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
      "copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
      "creating build/lib.linux-x86_64-3.6/apex/parallel\n",
      "copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "creating build/lib.linux-x86_64-3.6/apex/pyprof\n",
      "copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof\n",
      "creating build/lib.linux-x86_64-3.6/apex/reparameterization\n",
      "copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
      "copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
      "copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
      "creating build/lib.linux-x86_64-3.6/apex/RNN\n",
      "copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
      "copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
      "copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
      "copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
      "creating build/lib.linux-x86_64-3.6/apex/amp/lists\n",
      "copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
      "copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
      "copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
      "copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
      "creating build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
      "copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
      "copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
      "creating build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
      "copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
      "copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
      "copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
      "copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
      "copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
      "copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
      "copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
      "copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
      "copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
      "copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
      "creating build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
      "copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
      "copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
      "copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
      "copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
      "copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
      "copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
      "copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
      "copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
      "copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
      "creating build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
      "copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
      "copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
      "copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
      "creating build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
      "copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
      "copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
      "creating build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
      "copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
      "copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
      "creating build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
      "copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
      "copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
      "copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
      "copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
      "copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
      "copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
      "creating build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
      "running build_ext\n",
      "building 'apex_C' extension\n",
      "creating build/temp.linux-x86_64-3.6\n",
      "creating build/temp.linux-x86_64-3.6/csrc\n",
      "gcc -pthread -B /anaconda/envs/azureml_py36/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/TH -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/THC -I/anaconda/envs/azureml_py36/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcommand line option ‘\u001b[01m\u001b[K-Wstrict-prototypes\u001b[m\u001b[K’ is valid for C/ObjC but not for C++\n",
      "g++ -pthread -shared -B /anaconda/envs/azureml_py36/compiler_compat -L/anaconda/envs/azureml_py36/lib -Wl,-rpath=/anaconda/envs/azureml_py36/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\n",
      "building 'amp_C' extension\n",
      "gcc -pthread -B /anaconda/envs/azureml_py36/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/TH -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/anaconda/envs/azureml_py36/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcommand line option ‘\u001b[01m\u001b[K-Wstrict-prototypes\u001b[m\u001b[K’ is valid for C/ObjC but not for C++\n",
      "/usr/local/cuda/bin/nvcc -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/TH -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/anaconda/envs/azureml_py36/include/python3.6m -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++11\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/usr/local/cuda/bin/nvcc -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/TH -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/anaconda/envs/azureml_py36/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++11\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/usr/local/cuda/bin/nvcc -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/TH -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/anaconda/envs/azureml_py36/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++11\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/usr/local/cuda/bin/nvcc -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/TH -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/anaconda/envs/azureml_py36/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++11\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/usr/local/cuda/bin/nvcc -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/TH -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/anaconda/envs/azureml_py36/include/python3.6m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++11\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/usr/local/cuda/bin/nvcc -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/TH -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/anaconda/envs/azureml_py36/include/python3.6m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++11\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/usr/local/cuda/bin/nvcc -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/TH -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/anaconda/envs/azureml_py36/include/python3.6m -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++11\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/usr/local/cuda/bin/nvcc -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/TH -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/anaconda/envs/azureml_py36/include/python3.6m -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++11\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/usr/local/cuda/bin/nvcc -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/TH -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/anaconda/envs/azureml_py36/include/python3.6m -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++11\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/usr/local/cuda/bin/nvcc -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/TH -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/anaconda/envs/azureml_py36/include/python3.6m -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++11\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "g++ -pthread -shared -B /anaconda/envs/azureml_py36/compiler_compat -L/anaconda/envs/azureml_py36/lib -Wl,-rpath=/anaconda/envs/azureml_py36/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\n",
      "building 'syncbn' extension\n",
      "gcc -pthread -B /anaconda/envs/azureml_py36/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/TH -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/anaconda/envs/azureml_py36/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcommand line option ‘\u001b[01m\u001b[K-Wstrict-prototypes\u001b[m\u001b[K’ is valid for C/ObjC but not for C++\n",
      "/usr/local/cuda/bin/nvcc -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/TH -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/anaconda/envs/azureml_py36/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++11\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "g++ -pthread -shared -B /anaconda/envs/azureml_py36/compiler_compat -L/anaconda/envs/azureml_py36/lib -Wl,-rpath=/anaconda/envs/azureml_py36/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\n",
      "building 'fused_layer_norm_cuda' extension\n",
      "gcc -pthread -B /anaconda/envs/azureml_py36/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/TH -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/anaconda/envs/azureml_py36/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcommand line option ‘\u001b[01m\u001b[K-Wstrict-prototypes\u001b[m\u001b[K’ is valid for C/ObjC but not for C++\n",
      "/usr/local/cuda/bin/nvcc -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/TH -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/anaconda/envs/azureml_py36/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++11\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "g++ -pthread -shared -B /anaconda/envs/azureml_py36/compiler_compat -L/anaconda/envs/azureml_py36/lib -Wl,-rpath=/anaconda/envs/azureml_py36/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n",
      "building 'mlp_cuda' extension\n",
      "gcc -pthread -B /anaconda/envs/azureml_py36/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/TH -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/anaconda/envs/azureml_py36/include/python3.6m -c csrc/mlp.cpp -o build/temp.linux-x86_64-3.6/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "\u001b[01m\u001b[Kcc1plus:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcommand line option ‘\u001b[01m\u001b[K-Wstrict-prototypes\u001b[m\u001b[K’ is valid for C/ObjC but not for C++\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:56:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [-Wsign-compare]\n",
      "   for (int i = 0; i < num_layers; i++) {\n",
      "\u001b[01;32m\u001b[K                     ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:65:68:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Knarrowing conversion of ‘\u001b[01m\u001b[Kreserved_size\u001b[m\u001b[K’ from ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’ to ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’ inside { } [-Wnarrowing]\n",
      "   auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
      "\u001b[01;32m\u001b[K                                                                    ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:65:68:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Knarrowing conversion of ‘\u001b[01m\u001b[Kreserved_size\u001b[m\u001b[K’ from ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’ to ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’ inside { } [-Wnarrowing]\n",
      "In file included from \u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In lambda function:\n",
      "\u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:116:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [-Wdeprecated-declarations]\n",
      "     at::ScalarType _st = ::detail::scalar_type(the_type);                    \\\n",
      "\u001b[01;32m\u001b[K                                                        ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:67:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
      "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
      "\u001b[01;32m\u001b[K   ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:31:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      " inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
      "\u001b[01;32m\u001b[K                       ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In lambda function:\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:70:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [-Wsign-compare]\n",
      "     for (int i = 0; i < num_layers; i++) {\n",
      "\u001b[01;32m\u001b[K                       ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
      "     return __VA_ARGS__();                          \\\n",
      "\u001b[01;32m\u001b[K            ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:67:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
      "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
      "\u001b[01;32m\u001b[K   ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:76:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kresult\u001b[m\u001b[K’ [-Wunused-variable]\n",
      "     auto result = mlp_fp<scalar_t>(\n",
      "\u001b[01;32m\u001b[K          ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
      "     return __VA_ARGS__();                          \\\n",
      "\u001b[01;32m\u001b[K            ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:67:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
      "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
      "\u001b[01;32m\u001b[K   ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In lambda function:\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:70:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [-Wsign-compare]\n",
      "     for (int i = 0; i < num_layers; i++) {\n",
      "\u001b[01;32m\u001b[K                       ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
      "     return __VA_ARGS__();                          \\\n",
      "\u001b[01;32m\u001b[K            ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:67:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
      "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
      "\u001b[01;32m\u001b[K   ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:76:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kresult\u001b[m\u001b[K’ [-Wunused-variable]\n",
      "     auto result = mlp_fp<scalar_t>(\n",
      "\u001b[01;32m\u001b[K          ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
      "     return __VA_ARGS__();                          \\\n",
      "\u001b[01;32m\u001b[K            ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:67:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
      "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
      "\u001b[01;32m\u001b[K   ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In lambda function:\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:70:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [-Wsign-compare]\n",
      "     for (int i = 0; i < num_layers; i++) {\n",
      "\u001b[01;32m\u001b[K                       ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
      "     return __VA_ARGS__();                          \\\n",
      "\u001b[01;32m\u001b[K            ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:67:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
      "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
      "\u001b[01;32m\u001b[K   ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:76:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kresult\u001b[m\u001b[K’ [-Wunused-variable]\n",
      "     auto result = mlp_fp<scalar_t>(\n",
      "\u001b[01;32m\u001b[K          ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
      "     return __VA_ARGS__();                          \\\n",
      "\u001b[01;32m\u001b[K            ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:67:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
      "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
      "\u001b[01;32m\u001b[K   ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kstd::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)\u001b[m\u001b[K’:\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:113:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [-Wsign-compare]\n",
      "   for (int i = 0; i < num_layers; i++) {\n",
      "\u001b[01;32m\u001b[K                     ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:119:21:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [-Wsign-compare]\n",
      "   for (int i = 0; i < inputs.size(); i++) {\n",
      "\u001b[01;32m\u001b[K                     ^\u001b[m\u001b[K\n",
      "In file included from \u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/ATen/ATen.h:9:0\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/types.h:3\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/data.h:3\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/all.h:4\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/extension.h:4\u001b[m\u001b[K,\n",
      "                 from \u001b[01m\u001b[Kcsrc/mlp.cpp:1\u001b[m\u001b[K:\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In lambda function:\n",
      "\u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:116:56:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kc10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)\u001b[m\u001b[K’ is deprecated [-Wdeprecated-declarations]\n",
      "     at::ScalarType _st = ::detail::scalar_type(the_type);                    \\\n",
      "\u001b[01;32m\u001b[K                                                        ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
      "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "\u001b[01;32m\u001b[K   ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:31:23:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kdeclared here\n",
      " inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties &t) {\n",
      "\u001b[01;32m\u001b[K                       ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In lambda function:\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:125:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [-Wsign-compare]\n",
      "     for (int i = 0; i < num_layers; i++) {\n",
      "\u001b[01;32m\u001b[K                       ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
      "     return __VA_ARGS__();                          \\\n",
      "\u001b[01;32m\u001b[K            ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
      "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "\u001b[01;32m\u001b[K   ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:129:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [-Wsign-compare]\n",
      "     for (int i = 0; i < inputs.size(); i++) {\n",
      "\u001b[01;32m\u001b[K                       ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
      "     return __VA_ARGS__();                          \\\n",
      "\u001b[01;32m\u001b[K            ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
      "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "\u001b[01;32m\u001b[K   ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:137:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Knarrowing conversion of ‘\u001b[01m\u001b[K(work_size / sizeof (scalar_t))\u001b[m\u001b[K’ from ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’ to ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’ inside { } [-Wnarrowing]\n",
      "     auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type(\n",
      "\u001b[01;32m\u001b[K                                            ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
      "     return __VA_ARGS__();                          \\\n",
      "\u001b[01;32m\u001b[K            ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
      "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "\u001b[01;32m\u001b[K   ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:137:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Knarrowing conversion of ‘\u001b[01m\u001b[K(work_size / sizeof (scalar_t))\u001b[m\u001b[K’ from ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’ to ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’ inside { } [-Wnarrowing]\n",
      "     auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type(\n",
      "\u001b[01;32m\u001b[K                                            ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
      "     return __VA_ARGS__();                          \\\n",
      "\u001b[01;32m\u001b[K            ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
      "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "\u001b[01;32m\u001b[K   ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:139:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kresult\u001b[m\u001b[K’ [-Wunused-variable]\n",
      "     auto result = mlp_bp<scalar_t>(\n",
      "\u001b[01;32m\u001b[K          ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
      "     return __VA_ARGS__();                          \\\n",
      "\u001b[01;32m\u001b[K            ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
      "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "\u001b[01;32m\u001b[K   ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In lambda function:\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:125:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [-Wsign-compare]\n",
      "     for (int i = 0; i < num_layers; i++) {\n",
      "\u001b[01;32m\u001b[K                       ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
      "     return __VA_ARGS__();                          \\\n",
      "\u001b[01;32m\u001b[K            ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
      "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "\u001b[01;32m\u001b[K   ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:129:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [-Wsign-compare]\n",
      "     for (int i = 0; i < inputs.size(); i++) {\n",
      "\u001b[01;32m\u001b[K                       ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
      "     return __VA_ARGS__();                          \\\n",
      "\u001b[01;32m\u001b[K            ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
      "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "\u001b[01;32m\u001b[K   ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:137:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Knarrowing conversion of ‘\u001b[01m\u001b[K(work_size / sizeof (scalar_t))\u001b[m\u001b[K’ from ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’ to ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’ inside { } [-Wnarrowing]\n",
      "     auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type(\n",
      "\u001b[01;32m\u001b[K                                            ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
      "     return __VA_ARGS__();                          \\\n",
      "\u001b[01;32m\u001b[K            ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
      "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "\u001b[01;32m\u001b[K   ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:137:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Knarrowing conversion of ‘\u001b[01m\u001b[K(work_size / sizeof (scalar_t))\u001b[m\u001b[K’ from ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’ to ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’ inside { } [-Wnarrowing]\n",
      "     auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type(\n",
      "\u001b[01;32m\u001b[K                                            ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
      "     return __VA_ARGS__();                          \\\n",
      "\u001b[01;32m\u001b[K            ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
      "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "\u001b[01;32m\u001b[K   ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:139:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kresult\u001b[m\u001b[K’ [-Wunused-variable]\n",
      "     auto result = mlp_bp<scalar_t>(\n",
      "\u001b[01;32m\u001b[K          ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
      "     return __VA_ARGS__();                          \\\n",
      "\u001b[01;32m\u001b[K            ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
      "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "\u001b[01;32m\u001b[K   ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:\u001b[m\u001b[K In lambda function:\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:125:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [-Wsign-compare]\n",
      "     for (int i = 0; i < num_layers; i++) {\n",
      "\u001b[01;32m\u001b[K                       ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
      "     return __VA_ARGS__();                          \\\n",
      "\u001b[01;32m\u001b[K            ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
      "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "\u001b[01;32m\u001b[K   ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:129:23:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison between signed and unsigned integer expressions [-Wsign-compare]\n",
      "     for (int i = 0; i < inputs.size(); i++) {\n",
      "\u001b[01;32m\u001b[K                       ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
      "     return __VA_ARGS__();                          \\\n",
      "\u001b[01;32m\u001b[K            ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
      "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "\u001b[01;32m\u001b[K   ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:137:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Knarrowing conversion of ‘\u001b[01m\u001b[K(work_size / sizeof (scalar_t))\u001b[m\u001b[K’ from ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’ to ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’ inside { } [-Wnarrowing]\n",
      "     auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type(\n",
      "\u001b[01;32m\u001b[K                                            ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
      "     return __VA_ARGS__();                          \\\n",
      "\u001b[01;32m\u001b[K            ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
      "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "\u001b[01;32m\u001b[K   ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:137:44:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Knarrowing conversion of ‘\u001b[01m\u001b[K(work_size / sizeof (scalar_t))\u001b[m\u001b[K’ from ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’ to ‘\u001b[01m\u001b[Klong int\u001b[m\u001b[K’ inside { } [-Wnarrowing]\n",
      "     auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type(\n",
      "\u001b[01;32m\u001b[K                                            ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
      "     return __VA_ARGS__();                          \\\n",
      "\u001b[01;32m\u001b[K            ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
      "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "\u001b[01;32m\u001b[K   ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:139:10:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kresult\u001b[m\u001b[K’ [-Wunused-variable]\n",
      "     auto result = mlp_bp<scalar_t>(\n",
      "\u001b[01;32m\u001b[K          ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[K/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/ATen/Dispatch.h:12:12:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin definition of macro ‘\u001b[01m\u001b[KAT_PRIVATE_CASE_TYPE\u001b[m\u001b[K’\n",
      "     return __VA_ARGS__();                          \\\n",
      "\u001b[01;32m\u001b[K            ^\u001b[m\u001b[K\n",
      "\u001b[01m\u001b[Kcsrc/mlp.cpp:123:3:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kin expansion of macro ‘\u001b[01m\u001b[KAT_DISPATCH_FLOATING_TYPES_AND_HALF\u001b[m\u001b[K’\n",
      "   AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
      "\u001b[01;32m\u001b[K   ^\u001b[m\u001b[K\n",
      "/usr/local/cuda/bin/nvcc -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/TH -I/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/THC -I/usr/local/cuda/include -I/anaconda/envs/azureml_py36/include/python3.6m -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_37,code=sm_37 -std=c++11\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/autograd/profiler.h(97): warning: attribute \"__visibility__\" does not apply here\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/autograd/profiler.h(112): warning: attribute \"__visibility__\" does not apply here\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/enum.h(179): warning: statement is unreachable\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/c10/core/TensorTypeSet.h(44): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/enum.h(179): warning: statement is unreachable\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(14): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(15): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(18): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(19): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(23): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/api/include/torch/nn/functional/padding.h(24): warning: integer conversion resulted in a change of sign\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/autograd/profiler.h(97): warning: attribute \"__visibility__\" does not apply here\n",
      "\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/include/torch/csrc/autograd/profiler.h(112): warning: attribute \"__visibility__\" does not apply here\n",
      "\n",
      "g++ -pthread -shared -B /anaconda/envs/azureml_py36/compiler_compat -L/anaconda/envs/azureml_py36/lib -Wl,-rpath=/anaconda/envs/azureml_py36/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.6/csrc/mlp.o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so\n",
      "creating build/bdist.linux-x86_64\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/apex\n",
      "creating build/bdist.linux-x86_64/egg/apex/amp\n",
      "copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
      "copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
      "copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
      "copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
      "creating build/bdist.linux-x86_64/egg/apex/amp/lists\n",
      "copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> build/bdist.linux-x86_64/egg/apex/amp/lists\n",
      "copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> build/bdist.linux-x86_64/egg/apex/amp/lists\n",
      "copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> build/bdist.linux-x86_64/egg/apex/amp/lists\n",
      "copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> build/bdist.linux-x86_64/egg/apex/amp/lists\n",
      "copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
      "copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
      "copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
      "copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
      "copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
      "copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
      "copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
      "copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
      "copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
      "copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> build/bdist.linux-x86_64/egg/apex/amp\n",
      "creating build/bdist.linux-x86_64/egg/apex/contrib\n",
      "creating build/bdist.linux-x86_64/egg/apex/contrib/groupbn\n",
      "copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/batch_norm.py -> build/bdist.linux-x86_64/egg/apex/contrib/groupbn\n",
      "copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/groupbn\n",
      "creating build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
      "copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
      "copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
      "copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
      "copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
      "copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
      "copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
      "copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
      "copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
      "copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
      "copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn\n",
      "creating build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
      "copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
      "copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
      "copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
      "copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_lamb.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
      "copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fp16_optimizer.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
      "copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_adam.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
      "copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
      "copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
      "copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/optimizers\n",
      "creating build/bdist.linux-x86_64/egg/apex/contrib/sparsity\n",
      "copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/asp.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity\n",
      "copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/sparse_masklib.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity\n",
      "copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/sparsity\n",
      "creating build/bdist.linux-x86_64/egg/apex/contrib/xentropy\n",
      "copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/softmax_xentropy.py -> build/bdist.linux-x86_64/egg/apex/contrib/xentropy\n",
      "copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib/xentropy\n",
      "copying build/lib.linux-x86_64-3.6/apex/contrib/__init__.py -> build/bdist.linux-x86_64/egg/apex/contrib\n",
      "creating build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
      "copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
      "copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
      "copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
      "copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> build/bdist.linux-x86_64/egg/apex/fp16_utils\n",
      "creating build/bdist.linux-x86_64/egg/apex/mlp\n",
      "copying build/lib.linux-x86_64-3.6/apex/mlp/mlp.py -> build/bdist.linux-x86_64/egg/apex/mlp\n",
      "copying build/lib.linux-x86_64-3.6/apex/mlp/__init__.py -> build/bdist.linux-x86_64/egg/apex/mlp\n",
      "creating build/bdist.linux-x86_64/egg/apex/multi_tensor_apply\n",
      "copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> build/bdist.linux-x86_64/egg/apex/multi_tensor_apply\n",
      "copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> build/bdist.linux-x86_64/egg/apex/multi_tensor_apply\n",
      "creating build/bdist.linux-x86_64/egg/apex/normalization\n",
      "copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> build/bdist.linux-x86_64/egg/apex/normalization\n",
      "copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> build/bdist.linux-x86_64/egg/apex/normalization\n",
      "creating build/bdist.linux-x86_64/egg/apex/optimizers\n",
      "copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adagrad.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
      "copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
      "copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_lamb.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
      "copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_novograd.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
      "copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_sgd.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
      "copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> build/bdist.linux-x86_64/egg/apex/optimizers\n",
      "creating build/bdist.linux-x86_64/egg/apex/parallel\n",
      "copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
      "copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
      "copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
      "copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
      "copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
      "copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
      "copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
      "copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> build/bdist.linux-x86_64/egg/apex/parallel\n",
      "creating build/bdist.linux-x86_64/egg/apex/pyprof\n",
      "creating build/bdist.linux-x86_64/egg/apex/pyprof/nvtx\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/nvmarker.py -> build/bdist.linux-x86_64/egg/apex/pyprof/nvtx\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/__init__.py -> build/bdist.linux-x86_64/egg/apex/pyprof/nvtx\n",
      "creating build/bdist.linux-x86_64/egg/apex/pyprof/parse\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/db.py -> build/bdist.linux-x86_64/egg/apex/pyprof/parse\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/kernel.py -> build/bdist.linux-x86_64/egg/apex/pyprof/parse\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/nvvp.py -> build/bdist.linux-x86_64/egg/apex/pyprof/parse\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/parse.py -> build/bdist.linux-x86_64/egg/apex/pyprof/parse\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__init__.py -> build/bdist.linux-x86_64/egg/apex/pyprof/parse\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__main__.py -> build/bdist.linux-x86_64/egg/apex/pyprof/parse\n",
      "creating build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/activation.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/base.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/blas.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/conv.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/convert.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/data.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/dropout.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/embedding.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/index_slice_join_mutate.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/linear.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/loss.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/misc.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/normalization.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/optim.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/output.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pointwise.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pooling.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/prof.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/randomSample.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/recurrentCell.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/reduction.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/softmax.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/usage.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/utility.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__init__.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__main__.py -> build/bdist.linux-x86_64/egg/apex/pyprof/prof\n",
      "copying build/lib.linux-x86_64-3.6/apex/pyprof/__init__.py -> build/bdist.linux-x86_64/egg/apex/pyprof\n",
      "creating build/bdist.linux-x86_64/egg/apex/reparameterization\n",
      "copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> build/bdist.linux-x86_64/egg/apex/reparameterization\n",
      "copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> build/bdist.linux-x86_64/egg/apex/reparameterization\n",
      "copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> build/bdist.linux-x86_64/egg/apex/reparameterization\n",
      "creating build/bdist.linux-x86_64/egg/apex/RNN\n",
      "copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> build/bdist.linux-x86_64/egg/apex/RNN\n",
      "copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> build/bdist.linux-x86_64/egg/apex/RNN\n",
      "copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> build/bdist.linux-x86_64/egg/apex/RNN\n",
      "copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> build/bdist.linux-x86_64/egg/apex/RNN\n",
      "copying build/lib.linux-x86_64-3.6/apex/__init__.py -> build/bdist.linux-x86_64/egg/apex\n",
      "copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
      "copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
      "copying build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
      "copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/amp.py to amp.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/compat.py to compat.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/frontend.py to frontend.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/handle.py to handle.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/opt.py to opt.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/scaler.py to scaler.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/utils.py to utils.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/wrap.py to wrap.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/_initialize.py to _initialize.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/__init__.py to __init__.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/amp/__version__.py to __version__.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/groupbn/__init__.py to __init__.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/multihead_attn/__init__.py to __init__.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/distributed_fused_adam_v2.py to distributed_fused_adam_v2.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/distributed_fused_adam_v3.py to distributed_fused_adam_v3.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/optimizers/__init__.py to __init__.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/asp.py to asp.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/sparsity/__init__.py to __init__.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/xentropy/__init__.py to __init__.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/contrib/__init__.py to __init__.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/mlp/mlp.py to mlp.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/mlp/__init__.py to __init__.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/normalization/__init__.py to __init__.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_novograd.py to fused_novograd.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/optimizers/__init__.py to __init__.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/distributed.py to distributed.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/LARC.py to LARC.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/parallel/__init__.py to __init__.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/nvtx/__init__.py to __init__.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/parse/db.py to db.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/parse/kernel.py to kernel.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/parse/nvvp.py to nvvp.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/parse/parse.py to parse.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/parse/__init__.py to __init__.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/parse/__main__.py to __main__.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/activation.py to activation.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/base.py to base.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/blas.py to blas.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/conv.py to conv.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/convert.py to convert.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/data.py to data.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/dropout.py to dropout.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/embedding.py to embedding.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/linear.py to linear.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/loss.py to loss.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/misc.py to misc.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/normalization.py to normalization.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/optim.py to optim.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/output.py to output.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/pointwise.py to pointwise.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/pooling.py to pooling.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/prof.py to prof.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/randomSample.py to randomSample.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/reduction.py to reduction.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/softmax.py to softmax.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/usage.py to usage.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/utility.py to utility.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/__init__.py to __init__.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/prof/__main__.py to __main__.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/pyprof/__init__.py to __init__.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/RNN/cells.py to cells.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/RNN/models.py to models.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/RNN/__init__.py to __init__.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex/__init__.py to __init__.cpython-36.pyc\n",
      "creating stub loader for apex_C.cpython-36m-x86_64-linux-gnu.so\n",
      "creating stub loader for amp_C.cpython-36m-x86_64-linux-gnu.so\n",
      "creating stub loader for syncbn.cpython-36m-x86_64-linux-gnu.so\n",
      "creating stub loader for fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n",
      "creating stub loader for mlp_cuda.cpython-36m-x86_64-linux-gnu.so\n",
      "byte-compiling build/bdist.linux-x86_64/egg/apex_C.py to apex_C.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/amp_C.py to amp_C.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/syncbn.py to syncbn.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/fused_layer_norm_cuda.py to fused_layer_norm_cuda.cpython-36.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/mlp_cuda.py to mlp_cuda.cpython-36.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying apex.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying apex.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying apex.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying apex.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "__pycache__.amp_C.cpython-36: module references __file__\n",
      "__pycache__.apex_C.cpython-36: module references __file__\n",
      "__pycache__.fused_layer_norm_cuda.cpython-36: module references __file__\n",
      "__pycache__.mlp_cuda.cpython-36: module references __file__\n",
      "__pycache__.syncbn.cpython-36: module references __file__\n",
      "apex.pyprof.nvtx.__pycache__.nvmarker.cpython-36: module references __file__\n",
      "apex.pyprof.nvtx.__pycache__.nvmarker.cpython-36: module references __path__\n",
      "creating dist\n",
      "creating 'dist/apex-0.1-py3.6-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing apex-0.1-py3.6-linux-x86_64.egg\n",
      "creating /anaconda/envs/azureml_py36/lib/python3.6/site-packages/apex-0.1-py3.6-linux-x86_64.egg\n",
      "Extracting apex-0.1-py3.6-linux-x86_64.egg to /anaconda/envs/azureml_py36/lib/python3.6/site-packages\n",
      "Adding apex 0.1 to easy-install.pth file\n",
      "\n",
      "Installed /anaconda/envs/azureml_py36/lib/python3.6/site-packages/apex-0.1-py3.6-linux-x86_64.egg\n",
      "Processing dependencies for apex==0.1\n",
      "Finished processing dependencies for apex==0.1\n"
     ]
    }
   ],
   "source": [
    "!python setup.py install --cuda_ext --cpp_ext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: apex\n",
      "Version: 0.1\n",
      "Summary: PyTorch Extensions written by NVIDIA\n",
      "Home-page: UNKNOWN\n",
      "Author: UNKNOWN\n",
      "Author-email: UNKNOWN\n",
      "License: UNKNOWN\n",
      "Location: /anaconda/envs/azureml_py36/lib/python3.6/site-packages/apex-0.1-py3.6-linux-x86_64.egg\n",
      "Requires: \n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/gptcomputeinstance/code/users/atherfawaz/simpletransformers/examples/language_generation\n"
     ]
    }
   ],
   "source": [
    "%cd simpletransformers/examples/language_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install pytorch torchvision cudatoolkit=10.0 -c pytorch -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda update -n base -c defaults conda -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.language_modeling.language_modeling_utils: Loading features from cached file cache_dir/gpt2_cached_lm_508_train.txt\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ImportError('/anaconda/envs/azureml_py36/lib/python3.6/site-packages/apex-0.1-py3.6-linux-x86_64.egg/amp_C.cpython-36m-x86_64-linux-gnu.so: undefined symbol: THPVariableClass',)\n",
      "INFO:simpletransformers.language_modeling.language_modeling_model: Training started\n",
      "Epoch 1 of 3:   0%|                                       | 0/3 [00:00<?, ?it/s]\n",
      "Running Epoch 0:   0%|                                | 0/42263 [00:00<?, ?it/s]\u001b[A/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Running loss: 4.005244\n",
      "Running loss: 4.027955                     | 1/42263 [00:04<54:56:06,  4.68s/it]\u001b[AGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:114: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "\n",
      "Running loss: 3.754326                     | 2/42263 [00:06<46:04:26,  3.92s/it]\u001b[A\n",
      "Running loss: 3.881482                     | 3/42263 [00:08<39:50:05,  3.39s/it]\u001b[A\n",
      "Running loss: 3.438352                     | 4/42263 [00:11<35:35:41,  3.03s/it]\u001b[A\n",
      "Running loss: 4.076182                     | 5/42263 [00:13<32:34:44,  2.78s/it]\u001b[A\n",
      "Running loss: 3.922425                     | 6/42263 [00:15<30:29:33,  2.60s/it]\u001b[A\n",
      "Running loss: 3.669518                     | 7/42263 [00:17<29:08:50,  2.48s/it]\u001b[A\n",
      "Running loss: 3.782851                     | 8/42263 [00:19<28:10:42,  2.40s/it]\u001b[A\n",
      "Running loss: 4.186358                     | 9/42263 [00:22<27:21:01,  2.33s/it]\u001b[A\n",
      "Running loss: 3.919377                    | 10/42263 [00:24<26:49:25,  2.29s/it]\u001b[A\n",
      "Running loss: 3.912045                    | 11/42263 [00:26<26:24:44,  2.25s/it]\u001b[A\n",
      "Running loss: 3.792727                    | 12/42263 [00:28<26:09:27,  2.23s/it]\u001b[A\n",
      "Running loss: 3.752642                    | 13/42263 [00:30<25:58:09,  2.21s/it]\u001b[A\n",
      "Running loss: 3.650365                    | 14/42263 [00:33<25:58:55,  2.21s/it]\u001b[A\n",
      "Running loss: 3.061633                    | 15/42263 [00:35<25:56:15,  2.21s/it]\u001b[A\n",
      "Running loss: 3.888116                    | 16/42263 [00:37<25:52:47,  2.21s/it]\u001b[A\n",
      "Running loss: 3.801069                    | 17/42263 [00:39<25:48:06,  2.20s/it]\u001b[A\n",
      "Running loss: 3.643809                    | 18/42263 [00:41<25:44:10,  2.19s/it]\u001b[A\n",
      "Running loss: 4.036915                    | 19/42263 [00:43<25:40:08,  2.19s/it]\u001b[A\n",
      "Running loss: 3.864377                    | 20/42263 [00:46<25:43:46,  2.19s/it]\u001b[A\n",
      "Running loss: 3.994369                    | 21/42263 [00:48<25:40:01,  2.19s/it]\u001b[A\n",
      "Running loss: 3.857713                    | 22/42263 [00:50<25:43:37,  2.19s/it]\u001b[A\n",
      "Running loss: 3.790445                    | 23/42263 [00:52<25:40:49,  2.19s/it]\u001b[A\n",
      "Running loss: 3.508208                    | 24/42263 [00:54<25:36:36,  2.18s/it]\u001b[A\n",
      "Running loss: 3.771754                    | 25/42263 [00:57<25:40:27,  2.19s/it]\u001b[A\n",
      "Running loss: 3.823594                    | 26/42263 [00:59<25:40:23,  2.19s/it]\u001b[A\n",
      "Running loss: 3.974866                    | 27/42263 [01:01<25:40:24,  2.19s/it]\u001b[A\n",
      "Running loss: 3.562462                    | 28/42263 [01:03<25:34:59,  2.18s/it]\u001b[A\n",
      "Running loss: 3.932831                    | 29/42263 [01:05<25:31:29,  2.18s/it]\u001b[A\n",
      "Running loss: 3.957173                    | 30/42263 [01:08<25:44:42,  2.19s/it]\u001b[A\n",
      "Running loss: 3.714698                    | 31/42263 [01:10<25:42:58,  2.19s/it]\u001b[A\n",
      "Running loss: 3.756747                    | 32/42263 [01:12<25:43:45,  2.19s/it]\u001b[A\n",
      "Running loss: 3.804039                    | 33/42263 [01:14<25:43:00,  2.19s/it]\u001b[A\n",
      "Running loss: 3.117175                    | 34/42263 [01:16<25:43:27,  2.19s/it]\u001b[A\n",
      "Running loss: 3.545278                    | 35/42263 [01:19<25:40:05,  2.19s/it]\u001b[A\n",
      "Running loss: 3.809972                    | 36/42263 [01:21<25:41:04,  2.19s/it]\u001b[A\n",
      "Running loss: 3.729247                    | 37/42263 [01:23<25:40:16,  2.19s/it]\u001b[A\n",
      "Running loss: 3.931473                    | 38/42263 [01:25<25:48:21,  2.20s/it]\u001b[A\n",
      "Running loss: 3.992695                    | 39/42263 [01:27<25:51:06,  2.20s/it]\u001b[A\n",
      "Running loss: 3.797755                    | 40/42263 [01:30<25:50:51,  2.20s/it]\u001b[A\n",
      "Running loss: 3.696657                    | 41/42263 [01:32<25:48:37,  2.20s/it]\u001b[A\n",
      "Running loss: 3.696611                    | 42/42263 [01:34<25:48:56,  2.20s/it]\u001b[A\n",
      "Running loss: 3.840971                    | 43/42263 [01:36<25:45:21,  2.20s/it]\u001b[A\n",
      "Running loss: 3.593213                    | 44/42263 [01:38<25:41:51,  2.19s/it]\u001b[A\n",
      "Running loss: 3.804440                    | 45/42263 [01:40<25:38:13,  2.19s/it]\u001b[A\n",
      "Running loss: 3.880459                    | 46/42263 [01:43<25:41:57,  2.19s/it]\u001b[A\n",
      "Running loss: 3.863625                    | 47/42263 [01:45<25:40:32,  2.19s/it]\u001b[A\n",
      "Running loss: 3.493943                    | 48/42263 [01:47<25:47:00,  2.20s/it]\u001b[A\n",
      "Running loss: 3.848637                    | 49/42263 [01:49<25:46:10,  2.20s/it]\u001b[A\n",
      "Running loss: 4.012457                    | 50/42263 [01:51<25:46:23,  2.20s/it]\u001b[A\n",
      "Running loss: 3.543595                    | 51/42263 [01:54<25:45:02,  2.20s/it]\u001b[A\n",
      "Running loss: 3.957295                    | 52/42263 [01:56<25:46:27,  2.20s/it]\u001b[A\n",
      "Running loss: 3.774797                    | 53/42263 [01:58<25:48:25,  2.20s/it]\u001b[A\n",
      "Running loss: 3.760327                    | 54/42263 [02:00<25:50:37,  2.20s/it]\u001b[A\n",
      "Running loss: 3.723162                    | 55/42263 [02:04<31:24:36,  2.68s/it]\u001b[A\n",
      "Running loss: 3.737082                    | 56/42263 [02:06<29:45:18,  2.54s/it]\u001b[A\n",
      "Running loss: 3.494437                    | 57/42263 [02:08<28:32:51,  2.44s/it]\u001b[A\n",
      "Running loss: 4.068323                    | 58/42263 [02:11<27:43:23,  2.36s/it]\u001b[A\n",
      "Running loss: 3.609258                    | 59/42263 [02:13<27:06:10,  2.31s/it]\u001b[A\n",
      "Running loss: 3.683655                    | 60/42263 [02:15<26:41:48,  2.28s/it]\u001b[A\n",
      "Running loss: 3.488506                    | 61/42263 [02:17<26:21:02,  2.25s/it]\u001b[A\n",
      "Running loss: 3.560511                    | 62/42263 [02:19<26:13:02,  2.24s/it]\u001b[A\n",
      "Running loss: 3.586651                    | 63/42263 [02:22<26:02:25,  2.22s/it]\u001b[A\n",
      "Running loss: 3.730238                    | 64/42263 [02:24<25:55:29,  2.21s/it]\u001b[A\n",
      "Running loss: 3.491417                    | 65/42263 [02:26<25:46:24,  2.20s/it]\u001b[A\n",
      "Running loss: 3.726875                    | 66/42263 [02:28<25:48:03,  2.20s/it]\u001b[A\n",
      "Running loss: 3.785915                    | 67/42263 [02:30<25:43:00,  2.19s/it]\u001b[A\n",
      "Running loss: 3.581357                    | 68/42263 [02:33<25:42:54,  2.19s/it]\u001b[A\n",
      "Running loss: 3.836726                    | 69/42263 [02:35<25:40:38,  2.19s/it]\u001b[A\n",
      "Running loss: 3.542630                    | 70/42263 [02:37<25:44:26,  2.20s/it]\u001b[A\n",
      "Running loss: 3.443719                    | 71/42263 [02:39<25:40:30,  2.19s/it]\u001b[A\n",
      "Running loss: 3.807570                    | 72/42263 [02:41<25:41:53,  2.19s/it]\u001b[A\n",
      "Running loss: 3.819052                    | 73/42263 [02:44<25:43:21,  2.19s/it]\u001b[A\n",
      "Running loss: 3.716216                    | 74/42263 [02:46<25:48:54,  2.20s/it]\u001b[A\n",
      "Running loss: 3.766661                    | 75/42263 [02:48<25:46:25,  2.20s/it]\u001b[A\n",
      "Running loss: 3.747713                    | 76/42263 [02:50<25:44:13,  2.20s/it]\u001b[A\n",
      "Running loss: 3.611539                    | 77/42263 [02:52<25:42:30,  2.19s/it]\u001b[A\n",
      "Running loss: 3.456330                    | 78/42263 [02:54<25:37:11,  2.19s/it]\u001b[A\n",
      "Running loss: 3.716410                    | 79/42263 [02:57<25:38:24,  2.19s/it]\u001b[A\n",
      "Running loss: 3.704582                    | 80/42263 [02:59<25:39:38,  2.19s/it]\u001b[A\n",
      "Running loss: 3.706654                    | 81/42263 [03:01<25:43:37,  2.20s/it]\u001b[A\n",
      "Running loss: 3.426532                    | 82/42263 [03:03<25:45:02,  2.20s/it]\u001b[A\n",
      "Running loss: 3.888559                    | 83/42263 [03:05<25:38:55,  2.19s/it]\u001b[A\n",
      "Running loss: 3.601349                    | 84/42263 [03:08<25:47:32,  2.20s/it]\u001b[A\n",
      "Running loss: 3.571574                    | 85/42263 [03:10<25:42:18,  2.19s/it]\u001b[A\n",
      "Running loss: 3.616979                    | 86/42263 [03:12<25:39:57,  2.19s/it]\u001b[A\n",
      "Running loss: 3.542574                    | 87/42263 [03:14<25:42:54,  2.19s/it]\u001b[A\n",
      "Running loss: 3.801032                    | 88/42263 [03:16<25:43:00,  2.20s/it]\u001b[A\n",
      "Running loss: 3.837634                    | 89/42263 [03:19<25:43:08,  2.20s/it]\u001b[A\n",
      "Running loss: 3.741165                    | 90/42263 [03:21<25:43:04,  2.20s/it]\u001b[A\n",
      "Running loss: 3.615715                    | 91/42263 [03:23<25:41:21,  2.19s/it]\u001b[A\n",
      "Running loss: 3.626343                    | 92/42263 [03:25<25:47:19,  2.20s/it]\u001b[A\n",
      "Running loss: 3.736784                    | 93/42263 [03:27<25:43:00,  2.20s/it]\u001b[A\n",
      "Running loss: 3.602914                    | 94/42263 [03:30<25:48:26,  2.20s/it]\u001b[A\n",
      "Running loss: 3.622041                    | 95/42263 [03:32<25:48:39,  2.20s/it]\u001b[A\n",
      "Running loss: 3.712171                    | 96/42263 [03:34<25:52:02,  2.21s/it]\u001b[A\n",
      "Running loss: 3.362968                    | 97/42263 [03:36<25:52:10,  2.21s/it]\u001b[A\n",
      "Running loss: 3.740068                    | 98/42263 [03:38<25:52:04,  2.21s/it]\u001b[A\n",
      "Running loss: 3.956888                    | 99/42263 [03:41<26:00:09,  2.22s/it]\u001b[A/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "\n",
      "Running loss: 3.287484                   | 100/42263 [03:43<26:00:36,  2.22s/it]\u001b[A\n",
      "Running loss: 3.939655                   | 101/42263 [03:45<25:56:55,  2.22s/it]\u001b[A\n",
      "Running loss: 3.507808                   | 102/42263 [03:47<25:59:28,  2.22s/it]\u001b[A\n",
      "Running loss: 3.947358                   | 103/42263 [03:50<25:51:44,  2.21s/it]\u001b[A\n",
      "Running loss: 3.721385                   | 104/42263 [03:52<25:52:27,  2.21s/it]\u001b[A\n",
      "Running loss: 3.428468                   | 105/42263 [03:54<25:46:15,  2.20s/it]\u001b[A\n",
      "Running loss: 3.827262                   | 106/42263 [03:56<25:42:21,  2.20s/it]\u001b[A\n",
      "Running loss: 3.408663                   | 107/42263 [03:58<25:45:07,  2.20s/it]\u001b[A\n",
      "Running loss: 3.537426                   | 108/42263 [04:01<25:49:08,  2.20s/it]\u001b[A\n",
      "Running loss: 3.682436                   | 109/42263 [04:03<25:46:45,  2.20s/it]\u001b[A\n",
      "Running loss: 3.849385                   | 110/42263 [04:05<25:44:19,  2.20s/it]\u001b[A\n",
      "Running loss: 3.646027                   | 111/42263 [04:07<25:48:23,  2.20s/it]\u001b[A\n",
      "Running loss: 3.366768                   | 112/42263 [04:09<25:46:09,  2.20s/it]\u001b[A\n",
      "Running loss: 3.943804                   | 113/42263 [04:12<25:44:24,  2.20s/it]\u001b[A\n",
      "Running loss: 3.871745                   | 114/42263 [04:14<25:47:23,  2.20s/it]\u001b[A\n",
      "Running loss: 3.503076                   | 115/42263 [04:16<25:43:15,  2.20s/it]\u001b[A\n",
      "Running loss: 3.624689                   | 116/42263 [04:18<25:46:48,  2.20s/it]\u001b[A\n",
      "Running loss: 3.343293                   | 117/42263 [04:20<25:45:17,  2.20s/it]\u001b[A\n",
      "Running loss: 3.327740                   | 118/42263 [04:23<25:45:12,  2.20s/it]\u001b[A\n",
      "Running loss: 3.714318                   | 119/42263 [04:25<25:42:38,  2.20s/it]\u001b[A\n",
      "Running loss: 3.924302                   | 120/42263 [04:27<25:44:35,  2.20s/it]\u001b[A\n",
      "Running loss: 3.719659                   | 121/42263 [04:29<25:44:55,  2.20s/it]\u001b[A\n",
      "Running loss: 3.715861                   | 122/42263 [04:31<25:43:05,  2.20s/it]\u001b[A\n",
      "Running loss: 3.480097                   | 123/42263 [04:34<25:41:58,  2.20s/it]\u001b[A\n",
      "Running loss: 3.591835                   | 124/42263 [04:36<25:44:31,  2.20s/it]\u001b[A\n",
      "Running loss: 3.621401                   | 125/42263 [04:38<25:43:39,  2.20s/it]\u001b[A\n",
      "Running loss: 3.641149                   | 126/42263 [04:40<25:44:07,  2.20s/it]\u001b[A\n",
      "Running loss: 3.618092                   | 127/42263 [04:42<25:39:26,  2.19s/it]\u001b[A\n",
      "Running loss: 3.855943                   | 128/42263 [04:45<25:41:28,  2.20s/it]\u001b[A\n",
      "Running loss: 3.265384                   | 129/42263 [04:47<25:43:07,  2.20s/it]\u001b[A\n",
      "Running loss: 3.855052                   | 130/42263 [04:49<25:44:18,  2.20s/it]\u001b[A\n",
      "Running loss: 3.683297                   | 131/42263 [04:51<25:41:00,  2.19s/it]\u001b[A\n",
      "Running loss: 3.634836                   | 132/42263 [04:53<25:44:22,  2.20s/it]\u001b[A\n",
      "Running loss: 3.667131                   | 133/42263 [04:56<25:42:10,  2.20s/it]\u001b[A\n",
      "Running loss: 3.919744                   | 134/42263 [04:58<25:42:22,  2.20s/it]\u001b[A\n",
      "Running loss: 3.746804                   | 135/42263 [05:00<25:39:10,  2.19s/it]\u001b[A\n",
      "Running loss: 3.327305                   | 136/42263 [05:02<25:39:49,  2.19s/it]\u001b[A\n",
      "Running loss: 3.623981                   | 137/42263 [05:04<25:41:56,  2.20s/it]\u001b[A\n",
      "Running loss: 3.917824                   | 138/42263 [05:07<25:42:21,  2.20s/it]\u001b[A\n",
      "Running loss: 3.804093                   | 139/42263 [05:09<25:44:02,  2.20s/it]\u001b[A\n",
      "Running loss: 3.434258                   | 140/42263 [05:11<25:44:43,  2.20s/it]\u001b[A\n",
      "Running loss: 3.858141                   | 141/42263 [05:13<25:39:49,  2.19s/it]\u001b[A\n",
      "Running loss: 3.788112                   | 142/42263 [05:15<25:45:43,  2.20s/it]\u001b[A\n",
      "Running loss: 3.567524                   | 143/42263 [05:18<25:44:19,  2.20s/it]\u001b[A\n",
      "Running loss: 3.819848                   | 144/42263 [05:20<25:44:03,  2.20s/it]\u001b[A\n",
      "Running loss: 3.775055                   | 145/42263 [05:22<25:41:11,  2.20s/it]\u001b[A\n",
      "Running loss: 3.711130                   | 146/42263 [05:24<25:40:44,  2.19s/it]\u001b[A\n",
      "Running loss: 3.890507                   | 147/42263 [05:26<25:40:25,  2.19s/it]\u001b[A\n",
      "Running loss: 3.812554                   | 148/42263 [05:28<25:43:38,  2.20s/it]\u001b[A\n",
      "Running loss: 3.418797                   | 149/42263 [05:31<25:39:33,  2.19s/it]\u001b[A\n",
      "Running loss: 3.476104                   | 150/42263 [05:33<25:41:08,  2.20s/it]\u001b[A\n",
      "Running loss: 3.342089                   | 151/42263 [05:35<25:44:06,  2.20s/it]\u001b[A\n",
      "Running loss: 3.538875                   | 152/42263 [05:37<25:43:43,  2.20s/it]\u001b[A\n",
      "Running loss: 3.573681                   | 153/42263 [05:39<25:44:34,  2.20s/it]\u001b[A\n",
      "Running loss: 3.547724                   | 154/42263 [05:42<25:45:58,  2.20s/it]\u001b[A\n",
      "Running loss: 3.720325                   | 155/42263 [05:44<25:45:05,  2.20s/it]\u001b[A\n",
      "Running loss: 3.819678                   | 156/42263 [05:46<25:44:47,  2.20s/it]\u001b[A\n",
      "Running loss: 3.825999                   | 157/42263 [05:48<25:42:39,  2.20s/it]\u001b[A\n",
      "Running loss: 3.886028                   | 158/42263 [05:50<25:41:44,  2.20s/it]\u001b[A\n",
      "Running loss: 3.578100                   | 159/42263 [05:53<25:43:57,  2.20s/it]\u001b[A\n",
      "Running loss: 3.808683                   | 160/42263 [05:55<25:43:45,  2.20s/it]\u001b[A\n",
      "Running loss: 3.185971                   | 161/42263 [05:57<25:45:00,  2.20s/it]\u001b[A\n",
      "Running loss: 3.574422                   | 162/42263 [05:59<25:45:14,  2.20s/it]\u001b[A\n",
      "Running loss: 3.935850                   | 163/42263 [06:01<25:41:37,  2.20s/it]\u001b[A\n",
      "Running loss: 3.552850                   | 164/42263 [06:04<25:39:07,  2.19s/it]\u001b[A\n",
      "Running loss: 3.805479                   | 165/42263 [06:06<25:33:38,  2.19s/it]\u001b[A\n",
      "Running loss: 3.888760                   | 166/42263 [06:08<25:41:41,  2.20s/it]\u001b[A\n",
      "Running loss: 3.285691                   | 167/42263 [06:10<25:39:56,  2.19s/it]\u001b[A\n",
      "Running loss: 3.663826                   | 168/42263 [06:12<25:42:20,  2.20s/it]\u001b[A\n",
      "Running loss: 3.688422                   | 169/42263 [06:15<25:46:13,  2.20s/it]\u001b[A\n",
      "Running loss: 3.441093                   | 170/42263 [06:17<25:43:39,  2.20s/it]\u001b[A\n",
      "Running loss: 3.593429                   | 171/42263 [06:19<25:46:52,  2.20s/it]\u001b[A\n",
      "Running loss: 3.447718                   | 172/42263 [06:21<25:46:12,  2.20s/it]\u001b[A\n",
      "Running loss: 3.822198                   | 173/42263 [06:23<25:39:27,  2.19s/it]\u001b[A\n",
      "Running loss: 3.515584                   | 174/42263 [06:26<25:43:19,  2.20s/it]\u001b[A\n",
      "Running loss: 3.487597                   | 175/42263 [06:28<25:39:50,  2.20s/it]\u001b[A\n",
      "Running loss: 3.757870                   | 176/42263 [06:30<25:45:42,  2.20s/it]\u001b[A\n",
      "Running loss: 3.798282                   | 177/42263 [06:32<25:46:05,  2.20s/it]\u001b[A\n",
      "Running loss: 3.747298                   | 178/42263 [06:34<25:46:19,  2.20s/it]\u001b[A\n",
      "Running loss: 3.584935                   | 179/42263 [06:37<25:41:43,  2.20s/it]\u001b[A\n",
      "Running loss: 3.650273                   | 180/42263 [06:39<25:42:42,  2.20s/it]\u001b[A\n",
      "Running loss: 3.838774                   | 181/42263 [06:41<25:37:12,  2.19s/it]\u001b[A\n",
      "Running loss: 3.939205                   | 182/42263 [06:43<25:41:35,  2.20s/it]\u001b[A\n",
      "Running loss: 3.693261                   | 183/42263 [06:45<25:38:17,  2.19s/it]\u001b[A\n",
      "Running loss: 3.370078                   | 184/42263 [06:48<25:40:51,  2.20s/it]\u001b[A\n",
      "Running loss: 3.565376                   | 185/42263 [06:50<25:40:23,  2.20s/it]\u001b[A\n",
      "Running loss: 3.803367                   | 186/42263 [06:52<25:44:11,  2.20s/it]\u001b[A\n",
      "Running loss: 3.732037                   | 187/42263 [06:54<25:46:20,  2.21s/it]\u001b[A\n",
      "Running loss: 3.714424                   | 188/42263 [06:56<25:49:51,  2.21s/it]\u001b[A\n",
      "Running loss: 3.675784                   | 189/42263 [06:59<25:50:29,  2.21s/it]\u001b[A\n",
      "Running loss: 3.044105                   | 190/42263 [07:01<25:45:59,  2.20s/it]\u001b[A\n",
      "Running loss: 3.497266                   | 191/42263 [07:03<25:40:41,  2.20s/it]\u001b[A\n",
      "Running loss: 3.900202                   | 192/42263 [07:05<25:41:28,  2.20s/it]\u001b[A\n",
      "Running loss: 3.360487                   | 193/42263 [07:07<25:41:32,  2.20s/it]\u001b[A\n",
      "Running loss: 3.454141                   | 194/42263 [07:10<25:42:07,  2.20s/it]\u001b[A\n",
      "Running loss: 3.555409                   | 195/42263 [07:12<25:38:45,  2.19s/it]\u001b[A\n",
      "Running loss: 3.485258                   | 196/42263 [07:14<25:38:44,  2.19s/it]\u001b[A\n",
      "Running loss: 3.510743                   | 197/42263 [07:16<25:31:50,  2.18s/it]\u001b[A\n",
      "Running loss: 3.621600                   | 198/42263 [07:18<25:33:23,  2.19s/it]\u001b[A\n",
      "Running loss: 3.642897                   | 199/42263 [07:21<25:39:29,  2.20s/it]\u001b[A\n",
      "Running loss: 3.614595                   | 200/42263 [07:23<25:43:21,  2.20s/it]\u001b[A\n",
      "Running loss: 3.728342                   | 201/42263 [07:25<25:39:54,  2.20s/it]\u001b[A\n",
      "Running loss: 3.679659                   | 202/42263 [07:27<25:38:01,  2.19s/it]\u001b[A\n",
      "Running loss: 3.681572                   | 203/42263 [07:29<25:36:30,  2.19s/it]\u001b[A\n",
      "Running loss: 3.891810                   | 204/42263 [07:32<25:38:43,  2.20s/it]\u001b[A\n",
      "Running loss: 3.631066                   | 205/42263 [07:34<25:41:11,  2.20s/it]\u001b[A\n",
      "Running loss: 3.551959                   | 206/42263 [07:36<25:47:25,  2.21s/it]\u001b[A\n",
      "Running loss: 3.717636                   | 207/42263 [07:38<25:47:28,  2.21s/it]\u001b[A\n",
      "Running loss: 4.023728                   | 208/42263 [07:40<25:47:56,  2.21s/it]\u001b[A\n",
      "Running loss: 3.559373                   | 209/42263 [07:43<25:46:32,  2.21s/it]\u001b[A\n",
      "Running loss: 3.702871                   | 210/42263 [07:45<25:47:03,  2.21s/it]\u001b[A\n",
      "Running loss: 3.780849                   | 211/42263 [07:47<25:46:32,  2.21s/it]\u001b[A\n",
      "Running loss: 3.554649                   | 212/42263 [07:49<25:45:45,  2.21s/it]\u001b[A\n",
      "Running loss: 3.610053                   | 213/42263 [07:51<25:43:08,  2.20s/it]\u001b[A\n",
      "Running loss: 4.051907                   | 214/42263 [07:54<25:43:10,  2.20s/it]\u001b[A\n",
      "Running loss: 3.707417                   | 215/42263 [07:56<25:40:43,  2.20s/it]\u001b[A\n",
      "Running loss: 3.786168                   | 216/42263 [07:58<25:41:21,  2.20s/it]\u001b[A\n",
      "Running loss: 3.481371                   | 217/42263 [08:00<25:40:43,  2.20s/it]\u001b[A\n",
      "Running loss: 3.624817                   | 218/42263 [08:02<25:48:47,  2.21s/it]\u001b[A\n",
      "Running loss: 3.839341                   | 219/42263 [08:05<25:41:58,  2.20s/it]\u001b[A\n",
      "Running loss: 3.600348                   | 220/42263 [08:07<25:41:54,  2.20s/it]\u001b[A\n",
      "Running loss: 3.370741                   | 221/42263 [08:09<25:43:53,  2.20s/it]\u001b[A\n",
      "Running loss: 3.707264                   | 222/42263 [08:11<25:39:14,  2.20s/it]\u001b[A\n",
      "Running loss: 3.865239                   | 223/42263 [08:13<25:37:49,  2.19s/it]\u001b[A\n",
      "Running loss: 3.776514                   | 224/42263 [08:16<25:37:17,  2.19s/it]\u001b[A\n",
      "Running loss: 3.648339                   | 225/42263 [08:18<25:36:50,  2.19s/it]\u001b[A\n",
      "Running loss: 3.411106                   | 226/42263 [08:20<25:43:27,  2.20s/it]\u001b[A\n",
      "Running loss: 3.617334                   | 227/42263 [08:22<25:41:59,  2.20s/it]\u001b[A\n",
      "Running loss: 3.668820                   | 228/42263 [08:24<25:36:37,  2.19s/it]\u001b[A\n",
      "Running loss: 3.864850                   | 229/42263 [08:27<25:38:35,  2.20s/it]\u001b[A\n",
      "Running loss: 3.651917                   | 230/42263 [08:29<25:37:00,  2.19s/it]\u001b[A\n",
      "Running loss: 3.484409                   | 231/42263 [08:31<25:35:36,  2.19s/it]\u001b[A\n",
      "Running loss: 3.428034                   | 232/42263 [08:33<25:38:08,  2.20s/it]\u001b[A\n",
      "Running loss: 3.587173                   | 233/42263 [08:35<25:41:28,  2.20s/it]\u001b[A\n",
      "Running loss: 3.709695                   | 234/42263 [08:38<25:41:59,  2.20s/it]\u001b[A\n",
      "Running loss: 3.414225                   | 235/42263 [08:40<25:43:19,  2.20s/it]\u001b[A\n",
      "Running loss: 3.585413                   | 236/42263 [08:42<25:40:36,  2.20s/it]\u001b[A\n",
      "Running loss: 3.552126                   | 237/42263 [08:44<25:42:21,  2.20s/it]\u001b[A\n",
      "Running loss: 3.626078                   | 238/42263 [08:46<25:43:45,  2.20s/it]\u001b[A\n",
      "Running loss: 3.483701                   | 239/42263 [08:49<25:40:50,  2.20s/it]\u001b[A\n",
      "Running loss: 3.856916                   | 240/42263 [08:51<25:43:32,  2.20s/it]\u001b[A\n",
      "Running loss: 3.000170                   | 241/42263 [08:53<25:41:51,  2.20s/it]\u001b[A\n",
      "Running loss: 3.570613                   | 242/42263 [08:55<25:43:48,  2.20s/it]\u001b[A\n",
      "Running loss: 3.952531                   | 243/42263 [08:57<25:44:17,  2.21s/it]\u001b[A\n",
      "Running loss: 3.512224                   | 244/42263 [09:00<25:42:21,  2.20s/it]\u001b[A\n",
      "Running loss: 3.733253                   | 245/42263 [09:02<25:42:25,  2.20s/it]\u001b[A\n",
      "Running loss: 3.778241                   | 246/42263 [09:04<25:42:28,  2.20s/it]\u001b[A\n",
      "Running loss: 4.081011                   | 247/42263 [09:06<25:36:06,  2.19s/it]\u001b[A\n",
      "Running loss: 3.417123                   | 248/42263 [09:08<25:40:03,  2.20s/it]\u001b[A\n",
      "Running loss: 3.709377                   | 249/42263 [09:11<25:38:05,  2.20s/it]\u001b[A\n",
      "Running loss: 3.225924                   | 250/42263 [09:13<25:38:28,  2.20s/it]\u001b[A\n",
      "Running loss: 3.454559                   | 251/42263 [09:15<25:39:38,  2.20s/it]\u001b[A\n",
      "Running loss: 4.117273                   | 252/42263 [09:17<25:38:08,  2.20s/it]\u001b[A\n",
      "Running loss: 3.428096                   | 253/42263 [09:19<25:36:54,  2.20s/it]\u001b[A\n",
      "Running loss: 3.659082                   | 254/42263 [09:22<25:38:25,  2.20s/it]\u001b[A\n",
      "Running loss: 3.541497                   | 255/42263 [09:24<25:36:11,  2.19s/it]\u001b[A\n",
      "Running loss: 3.647466                   | 256/42263 [09:26<25:40:01,  2.20s/it]\u001b[A\n",
      "Running loss: 3.674257                   | 257/42263 [09:28<25:38:54,  2.20s/it]\u001b[A\n",
      "Running loss: 4.026702                   | 258/42263 [09:30<25:41:57,  2.20s/it]\u001b[A\n",
      "Running loss: 3.928875                   | 259/42263 [09:33<25:44:43,  2.21s/it]\u001b[A\n",
      "Running loss: 3.645643                   | 260/42263 [09:35<25:43:42,  2.21s/it]\u001b[A\n",
      "Running loss: 3.806179                   | 261/42263 [09:37<25:46:49,  2.21s/it]\u001b[A\n",
      "Running loss: 3.477344                   | 262/42263 [09:39<25:48:44,  2.21s/it]\u001b[A\n",
      "Running loss: 3.554185                   | 263/42263 [09:41<25:44:08,  2.21s/it]\u001b[A\n",
      "Running loss: 3.523107                   | 264/42263 [09:44<25:43:14,  2.20s/it]\u001b[A\n",
      "Running loss: 3.802410                   | 265/42263 [09:46<25:40:05,  2.20s/it]\u001b[A\n",
      "Running loss: 3.744096                   | 266/42263 [09:48<25:36:13,  2.19s/it]\u001b[A\n",
      "Running loss: 3.611305                   | 267/42263 [09:50<25:40:13,  2.20s/it]\u001b[A\n",
      "Running loss: 3.569156                   | 268/42263 [09:52<25:44:28,  2.21s/it]\u001b[A\n",
      "Running loss: 4.053799                   | 269/42263 [09:55<25:39:36,  2.20s/it]\u001b[A\n",
      "Running loss: 3.654591                   | 270/42263 [09:57<25:40:54,  2.20s/it]\u001b[A\n",
      "Running loss: 3.476576                   | 271/42263 [09:59<25:38:41,  2.20s/it]\u001b[A\n",
      "Running loss: 3.896471                   | 272/42263 [10:01<25:44:04,  2.21s/it]\u001b[A\n",
      "Running loss: 3.626750                   | 273/42263 [10:03<25:44:23,  2.21s/it]\u001b[A\n",
      "Running loss: 3.467926                   | 274/42263 [10:06<25:42:50,  2.20s/it]\u001b[A\n",
      "Running loss: 3.439329                   | 275/42263 [10:08<25:42:00,  2.20s/it]\u001b[A\n",
      "Running loss: 3.979622                   | 276/42263 [10:10<25:41:12,  2.20s/it]\u001b[A\n",
      "Running loss: 3.661030                   | 277/42263 [10:12<25:40:02,  2.20s/it]\u001b[A\n",
      "Running loss: 3.526878                   | 278/42263 [10:15<25:41:12,  2.20s/it]\u001b[A\n",
      "Running loss: 3.419265▏                  | 279/42263 [10:17<25:39:38,  2.20s/it]\u001b[A\n",
      "Running loss: 3.697216▏                  | 280/42263 [10:19<25:44:38,  2.21s/it]\u001b[A\n",
      "Running loss: 3.966530▏                  | 281/42263 [10:21<25:46:48,  2.21s/it]\u001b[A\n",
      "Running loss: 3.742540▏                  | 282/42263 [10:23<25:44:33,  2.21s/it]\u001b[A\n",
      "Running loss: 3.843699▏                  | 283/42263 [10:26<25:40:58,  2.20s/it]\u001b[A\n",
      "Running loss: 3.706481▏                  | 284/42263 [10:28<25:40:23,  2.20s/it]\u001b[A\n",
      "Running loss: 4.075523▏                  | 285/42263 [10:30<25:40:35,  2.20s/it]\u001b[A\n",
      "Running loss: 3.422469▏                  | 286/42263 [10:32<25:43:09,  2.21s/it]\u001b[A\n",
      "Running loss: 3.528310▏                  | 287/42263 [10:34<25:43:31,  2.21s/it]\u001b[A\n",
      "Running loss: 3.155414▏                  | 288/42263 [10:37<25:44:54,  2.21s/it]\u001b[A\n",
      "Running loss: 3.905518▏                  | 289/42263 [10:39<25:42:32,  2.20s/it]\u001b[A\n",
      "Running loss: 3.858391▏                  | 290/42263 [10:41<25:43:23,  2.21s/it]\u001b[A\n",
      "Running loss: 4.060381▏                  | 291/42263 [10:43<25:42:03,  2.20s/it]\u001b[A\n",
      "Running loss: 3.481918▏                  | 292/42263 [10:45<25:47:08,  2.21s/it]\u001b[A\n",
      "Running loss: 3.611724▏                  | 293/42263 [10:48<25:42:25,  2.21s/it]\u001b[A\n",
      "Running loss: 3.542027▏                  | 294/42263 [10:50<25:42:04,  2.20s/it]\u001b[A\n",
      "Running loss: 4.183001▏                  | 295/42263 [10:52<25:42:27,  2.21s/it]\u001b[A\n",
      "Running loss: 3.469894▏                  | 296/42263 [10:54<25:43:49,  2.21s/it]\u001b[A\n",
      "Running loss: 3.815261▏                  | 297/42263 [10:56<25:35:41,  2.20s/it]\u001b[A\n",
      "Running loss: 3.527786▏                  | 298/42263 [10:59<25:40:50,  2.20s/it]\u001b[A\n",
      "Running loss: 3.351317▏                  | 299/42263 [11:01<25:40:40,  2.20s/it]\u001b[A\n",
      "Running loss: 3.816669▏                  | 300/42263 [11:03<25:39:23,  2.20s/it]\u001b[A\n",
      "Running loss: 3.422499▏                  | 301/42263 [11:05<25:40:31,  2.20s/it]\u001b[A\n",
      "Running loss: 3.598469▏                  | 302/42263 [11:07<25:44:04,  2.21s/it]\u001b[A\n",
      "Running loss: 3.553077▏                  | 303/42263 [11:10<25:41:36,  2.20s/it]\u001b[A\n",
      "Running loss: 3.216980▏                  | 304/42263 [11:12<25:42:13,  2.21s/it]\u001b[A\n",
      "Running loss: 3.924371▏                  | 305/42263 [11:14<25:37:07,  2.20s/it]\u001b[A\n",
      "Running loss: 3.764729▏                  | 306/42263 [11:16<25:41:49,  2.20s/it]\u001b[A\n",
      "Running loss: 3.380428▏                  | 307/42263 [11:18<25:40:27,  2.20s/it]\u001b[A\n",
      "Running loss: 3.719792▏                  | 308/42263 [11:21<25:42:55,  2.21s/it]\u001b[A\n",
      "Running loss: 3.642964▏                  | 309/42263 [11:23<25:41:41,  2.20s/it]\u001b[A\n",
      "Running loss: 3.588195▏                  | 310/42263 [11:25<25:43:57,  2.21s/it]\u001b[A\n",
      "Running loss: 3.445537▏                  | 311/42263 [11:27<25:44:35,  2.21s/it]\u001b[A\n",
      "Running loss: 3.332829▏                  | 312/42263 [11:29<25:40:34,  2.20s/it]\u001b[A\n",
      "Running loss: 3.484358▏                  | 313/42263 [11:32<25:37:45,  2.20s/it]\u001b[A\n",
      "Running loss: 3.754524▏                  | 314/42263 [11:34<25:40:41,  2.20s/it]\u001b[A\n",
      "Running loss: 3.966089▏                  | 315/42263 [11:36<25:38:28,  2.20s/it]\u001b[A\n",
      "Running loss: 3.445429▏                  | 316/42263 [11:38<25:37:14,  2.20s/it]\u001b[A\n",
      "Running loss: 3.456768▏                  | 317/42263 [11:40<25:37:58,  2.20s/it]\u001b[A\n",
      "Running loss: 4.003059▏                  | 318/42263 [11:43<25:34:46,  2.20s/it]\u001b[A\n",
      "Running loss: 3.113160▏                  | 319/42263 [11:45<25:31:23,  2.19s/it]\u001b[A\n",
      "Running loss: 3.747100▏                  | 320/42263 [11:47<25:38:01,  2.20s/it]\u001b[A\n",
      "Running loss: 3.700760▏                  | 321/42263 [11:49<25:34:11,  2.19s/it]\u001b[A\n",
      "Running loss: 3.342196▏                  | 322/42263 [11:51<25:38:04,  2.20s/it]\u001b[A\n",
      "Running loss: 3.515046▏                  | 323/42263 [11:54<25:36:00,  2.20s/it]\u001b[A\n",
      "Running loss: 3.926207▏                  | 324/42263 [11:56<25:38:13,  2.20s/it]\u001b[A\n",
      "Running loss: 3.726226▏                  | 325/42263 [11:58<25:38:11,  2.20s/it]\u001b[A\n",
      "Running loss: 3.617776▏                  | 326/42263 [12:00<25:37:05,  2.20s/it]\u001b[A\n",
      "Running loss: 3.623063▏                  | 327/42263 [12:02<25:35:18,  2.20s/it]\u001b[A\n",
      "Running loss: 3.871524▏                  | 328/42263 [12:05<25:35:50,  2.20s/it]\u001b[A\n",
      "Running loss: 3.755306▏                  | 329/42263 [12:07<25:32:15,  2.19s/it]\u001b[A\n",
      "Running loss: 3.464543▏                  | 330/42263 [12:09<25:36:35,  2.20s/it]\u001b[A\n",
      "Running loss: 3.813173▏                  | 331/42263 [12:11<25:40:48,  2.20s/it]\u001b[A\n",
      "Running loss: 3.961540▏                  | 332/42263 [12:13<25:42:18,  2.21s/it]\u001b[A\n",
      "Running loss: 3.572185▏                  | 333/42263 [12:16<25:43:27,  2.21s/it]\u001b[A\n",
      "Running loss: 3.565848▏                  | 334/42263 [12:18<25:40:34,  2.20s/it]\u001b[A\n",
      "Running loss: 3.656597▏                  | 335/42263 [12:20<25:35:26,  2.20s/it]\u001b[A\n",
      "Running loss: 4.171163▏                  | 336/42263 [12:22<25:35:35,  2.20s/it]\u001b[A\n",
      "Running loss: 3.675898▏                  | 337/42263 [12:24<25:31:54,  2.19s/it]\u001b[A\n",
      "Running loss: 3.539452▏                  | 338/42263 [12:27<25:36:01,  2.20s/it]\u001b[A\n",
      "Running loss: 3.727404▏                  | 339/42263 [12:29<25:36:21,  2.20s/it]\u001b[A\n",
      "Running loss: 3.771436▏                  | 340/42263 [12:31<25:41:16,  2.21s/it]\u001b[A\n",
      "Running loss: 3.795042▏                  | 341/42263 [12:33<25:43:09,  2.21s/it]\u001b[A\n",
      "Running loss: 3.337700▏                  | 342/42263 [12:35<25:42:13,  2.21s/it]\u001b[A\n",
      "Running loss: 3.836915▏                  | 343/42263 [12:38<25:37:15,  2.20s/it]\u001b[A\n",
      "Running loss: 3.685994▏                  | 344/42263 [12:40<25:44:23,  2.21s/it]\u001b[A\n",
      "Running loss: 3.653017▏                  | 345/42263 [12:42<25:43:16,  2.21s/it]\u001b[A\n",
      "Running loss: 3.152574▏                  | 346/42263 [12:44<25:42:45,  2.21s/it]\u001b[A\n",
      "Running loss: 3.573334▏                  | 347/42263 [12:46<25:38:24,  2.20s/it]\u001b[A\n",
      "Running loss: 3.736494▏                  | 348/42263 [12:49<25:37:31,  2.20s/it]\u001b[A\n",
      "Running loss: 3.578357▏                  | 349/42263 [12:51<25:36:33,  2.20s/it]\u001b[A\n",
      "Running loss: 3.860750▏                  | 350/42263 [12:53<25:39:42,  2.20s/it]\u001b[A\n",
      "Running loss: 3.614494▏                  | 351/42263 [12:55<25:34:40,  2.20s/it]\u001b[A\n",
      "Running loss: 3.621528▏                  | 352/42263 [12:58<25:40:37,  2.21s/it]\u001b[A\n",
      "Running loss: 3.750831▏                  | 353/42263 [13:00<25:39:37,  2.20s/it]\u001b[A\n",
      "Running loss: 3.514938▏                  | 354/42263 [13:02<25:36:17,  2.20s/it]\u001b[A\n",
      "Running loss: 3.592377▏                  | 355/42263 [13:04<25:29:20,  2.19s/it]\u001b[A\n",
      "Running loss: 3.520007▏                  | 356/42263 [13:06<25:32:19,  2.19s/it]\u001b[A\n",
      "Running loss: 3.574923▏                  | 357/42263 [13:08<25:30:34,  2.19s/it]\u001b[A\n",
      "Running loss: 3.717292▏                  | 358/42263 [13:11<25:37:02,  2.20s/it]\u001b[A\n",
      "Running loss: 3.605045▏                  | 359/42263 [13:13<25:37:32,  2.20s/it]\u001b[A\n",
      "Running loss: 3.606378▏                  | 360/42263 [13:15<25:37:50,  2.20s/it]\u001b[A\n",
      "Running loss: 3.696754▏                  | 361/42263 [13:17<25:40:15,  2.21s/it]\u001b[A\n",
      "Running loss: 3.783254▏                  | 362/42263 [13:19<25:38:08,  2.20s/it]\u001b[A\n",
      "Running loss: 3.575879▏                  | 363/42263 [13:22<25:34:00,  2.20s/it]\u001b[A\n",
      "Running loss: 3.844268▏                  | 364/42263 [13:24<25:37:00,  2.20s/it]\u001b[A\n",
      "Running loss: 3.875822▏                  | 365/42263 [13:26<25:35:21,  2.20s/it]\u001b[A\n",
      "Running loss: 3.746565▏                  | 366/42263 [13:28<25:38:26,  2.20s/it]\u001b[A\n",
      "Running loss: 3.678636▏                  | 367/42263 [13:31<25:39:56,  2.21s/it]\u001b[A\n",
      "Running loss: 3.807024▏                  | 368/42263 [13:33<25:42:04,  2.21s/it]\u001b[A\n",
      "Running loss: 3.837098▏                  | 369/42263 [13:35<25:43:51,  2.21s/it]\u001b[A\n",
      "Running loss: 3.428933▏                  | 370/42263 [13:37<25:40:39,  2.21s/it]\u001b[A\n",
      "Running loss: 3.589996▏                  | 371/42263 [13:39<25:40:04,  2.21s/it]\u001b[A\n",
      "Running loss: 3.929034▏                  | 372/42263 [13:42<25:42:33,  2.21s/it]\u001b[A\n",
      "Running loss: 3.870298▏                  | 373/42263 [13:44<25:36:35,  2.20s/it]\u001b[A\n",
      "Running loss: 3.627500▏                  | 374/42263 [13:46<25:35:24,  2.20s/it]\u001b[A\n",
      "Running loss: 3.202799▏                  | 375/42263 [13:48<25:36:52,  2.20s/it]\u001b[A\n",
      "Running loss: 3.800735▏                  | 376/42263 [13:50<25:39:53,  2.21s/it]\u001b[A\n",
      "Running loss: 3.902418▏                  | 377/42263 [13:53<25:37:56,  2.20s/it]\u001b[A\n",
      "Running loss: 3.610296▏                  | 378/42263 [13:55<25:39:41,  2.21s/it]\u001b[A\n",
      "Running loss: 3.804153▏                  | 379/42263 [13:57<25:38:01,  2.20s/it]\u001b[A\n",
      "Running loss: 3.881283▏                  | 380/42263 [13:59<25:39:21,  2.21s/it]\u001b[A\n",
      "Running loss: 3.729256▏                  | 381/42263 [14:01<25:42:34,  2.21s/it]\u001b[A\n",
      "Running loss: 3.780650▏                  | 382/42263 [14:04<25:40:48,  2.21s/it]\u001b[A\n",
      "Running loss: 3.783149▏                  | 383/42263 [14:06<25:37:48,  2.20s/it]\u001b[A\n",
      "Running loss: 3.532815▏                  | 384/42263 [14:08<25:38:09,  2.20s/it]\u001b[A\n",
      "Running loss: 3.930360▏                  | 385/42263 [14:10<25:33:10,  2.20s/it]\u001b[A\n",
      "Running loss: 3.715329▏                  | 386/42263 [14:12<25:38:51,  2.20s/it]\u001b[A\n",
      "Running loss: 3.641694▏                  | 387/42263 [14:15<25:36:57,  2.20s/it]\u001b[A\n",
      "Running loss: 3.502769▏                  | 388/42263 [14:17<25:42:05,  2.21s/it]\u001b[A\n",
      "Running loss: 3.807273▏                  | 389/42263 [14:19<25:41:54,  2.21s/it]\u001b[A\n",
      "Running loss: 3.789219▏                  | 390/42263 [14:21<25:39:57,  2.21s/it]\u001b[A\n",
      "Running loss: 3.664594▏                  | 391/42263 [14:23<25:40:16,  2.21s/it]\u001b[A\n",
      "Running loss: 3.989270▏                  | 392/42263 [14:26<25:40:32,  2.21s/it]\u001b[A\n",
      "Running loss: 3.335727▏                  | 393/42263 [14:28<25:37:49,  2.20s/it]\u001b[A\n",
      "Running loss: 3.773636▏                  | 394/42263 [14:30<25:35:24,  2.20s/it]\u001b[A\n",
      "Running loss: 3.947401▏                  | 395/42263 [14:32<25:40:47,  2.21s/it]\u001b[A\n",
      "Running loss: 3.995399▏                  | 396/42263 [14:34<25:37:22,  2.20s/it]\u001b[A\n",
      "Running loss: 3.548943▏                  | 397/42263 [14:37<25:36:39,  2.20s/it]\u001b[A\n",
      "Running loss: 3.866089▏                  | 398/42263 [14:39<25:41:39,  2.21s/it]\u001b[A\n",
      "Running loss: 3.538068▏                  | 399/42263 [14:41<25:33:52,  2.20s/it]\u001b[A\n",
      "Running loss: 3.594339▏                  | 400/42263 [14:43<25:38:39,  2.21s/it]\u001b[A\n",
      "Running loss: 3.506545▏                  | 401/42263 [14:45<25:35:19,  2.20s/it]\u001b[A\n",
      "Running loss: 3.816759▏                  | 402/42263 [14:48<25:33:36,  2.20s/it]\u001b[A\n",
      "Running loss: 3.993010▏                  | 403/42263 [14:50<25:31:08,  2.19s/it]\u001b[A\n",
      "Running loss: 3.313546▏                  | 404/42263 [14:52<25:31:40,  2.20s/it]\u001b[A\n",
      "Running loss: 3.731438▏                  | 405/42263 [14:54<25:27:45,  2.19s/it]\u001b[A\n",
      "Running loss: 3.856296▏                  | 406/42263 [14:56<25:34:01,  2.20s/it]\u001b[A\n",
      "Running loss: 3.600182▏                  | 407/42263 [14:59<25:31:10,  2.19s/it]\u001b[A\n",
      "Running loss: 3.763488▏                  | 408/42263 [15:01<25:30:57,  2.19s/it]\u001b[A\n",
      "Running loss: 3.521577▏                  | 409/42263 [15:03<25:30:28,  2.19s/it]\u001b[A\n",
      "Running loss: 3.749349▏                  | 410/42263 [15:05<25:34:57,  2.20s/it]\u001b[A\n",
      "Running loss: 3.762860▏                  | 411/42263 [15:07<25:33:02,  2.20s/it]\u001b[A\n",
      "Running loss: 3.662602▏                  | 412/42263 [15:10<25:33:17,  2.20s/it]\u001b[A\n",
      "Running loss: 3.598881▏                  | 413/42263 [15:12<25:31:46,  2.20s/it]\u001b[A\n",
      "Running loss: 3.615478▏                  | 414/42263 [15:14<25:38:08,  2.21s/it]\u001b[A\n",
      "Running loss: 3.847953▏                  | 415/42263 [15:16<25:35:57,  2.20s/it]\u001b[A\n",
      "Running loss: 3.490842▏                  | 416/42263 [15:18<25:35:30,  2.20s/it]\u001b[A\n",
      "Running loss: 3.929574▏                  | 417/42263 [15:21<25:40:08,  2.21s/it]\u001b[A\n",
      "Running loss: 3.970693▏                  | 418/42263 [15:23<25:37:40,  2.20s/it]\u001b[A\n",
      "Running loss: 3.639075▏                  | 419/42263 [15:25<25:36:43,  2.20s/it]\u001b[A\n",
      "Running loss: 3.636455▏                  | 420/42263 [15:27<25:33:29,  2.20s/it]\u001b[A\n",
      "Running loss: 4.142859▏                  | 421/42263 [15:29<25:28:26,  2.19s/it]\u001b[A\n",
      "Running loss: 3.822333▏                  | 422/42263 [15:32<25:34:23,  2.20s/it]\u001b[A\n",
      "Running loss: 3.691715▏                  | 423/42263 [15:34<25:31:51,  2.20s/it]\u001b[A\n",
      "Running loss: 3.931170▏                  | 424/42263 [15:36<25:30:23,  2.19s/it]\u001b[A\n",
      "Running loss: 3.576602▏                  | 425/42263 [15:38<25:33:03,  2.20s/it]\u001b[A\n",
      "Running loss: 3.861454▏                  | 426/42263 [15:40<25:31:46,  2.20s/it]\u001b[A\n",
      "Running loss: 3.633368▏                  | 427/42263 [15:43<25:26:47,  2.19s/it]\u001b[A\n",
      "Running loss: 3.449889▏                  | 428/42263 [15:45<25:31:05,  2.20s/it]\u001b[A\n",
      "Running loss: 3.542470▏                  | 429/42263 [15:47<25:32:04,  2.20s/it]\u001b[A\n",
      "Running loss: 3.811282▏                  | 430/42263 [15:49<25:36:30,  2.20s/it]\u001b[A\n",
      "Running loss: 3.863811▏                  | 431/42263 [15:51<25:35:06,  2.20s/it]\u001b[A\n",
      "Running loss: 3.609890▏                  | 432/42263 [15:54<25:36:33,  2.20s/it]\u001b[A\n",
      "Running loss: 3.726883▏                  | 433/42263 [15:56<25:36:30,  2.20s/it]\u001b[A\n",
      "Running loss: 3.577801▏                  | 434/42263 [15:58<25:33:54,  2.20s/it]\u001b[A\n",
      "Running loss: 3.283310▏                  | 435/42263 [16:00<25:30:47,  2.20s/it]\u001b[A\n",
      "Running loss: 3.749103▏                  | 436/42263 [16:02<25:35:24,  2.20s/it]\u001b[A\n",
      "Running loss: 3.632113▏                  | 437/42263 [16:05<25:32:27,  2.20s/it]\u001b[A\n",
      "Running loss: 3.581318▏                  | 438/42263 [16:07<25:36:45,  2.20s/it]\u001b[A\n",
      "Running loss: 3.869423▏                  | 439/42263 [16:09<25:35:24,  2.20s/it]\u001b[A\n",
      "Running loss: 4.154521▏                  | 440/42263 [16:11<25:40:47,  2.21s/it]\u001b[A\n",
      "Running loss: 3.868803▏                  | 441/42263 [16:13<25:43:20,  2.21s/it]\u001b[A\n",
      "Running loss: 3.580492▏                  | 442/42263 [16:16<25:37:45,  2.21s/it]\u001b[A\n",
      "Running loss: 3.338989▏                  | 443/42263 [16:18<25:37:22,  2.21s/it]\u001b[A\n",
      "Running loss: 3.422202▏                  | 444/42263 [16:20<25:37:37,  2.21s/it]\u001b[A\n",
      "Running loss: 3.694401▏                  | 445/42263 [16:22<25:37:04,  2.21s/it]\u001b[A\n",
      "Running loss: 3.753733▏                  | 446/42263 [16:24<25:36:42,  2.20s/it]\u001b[A\n",
      "Running loss: 3.888680▏                  | 447/42263 [16:27<25:30:41,  2.20s/it]\u001b[A\n",
      "Running loss: 3.850222▏                  | 448/42263 [16:29<25:28:54,  2.19s/it]\u001b[A\n",
      "Running loss: 3.705166▏                  | 449/42263 [16:31<25:28:36,  2.19s/it]\u001b[A\n",
      "Running loss: 3.734438▏                  | 450/42263 [16:33<25:29:04,  2.19s/it]\u001b[A\n",
      "Running loss: 3.481866▏                  | 451/42263 [16:35<25:31:14,  2.20s/it]\u001b[A\n",
      "Running loss: 3.721453▏                  | 452/42263 [16:38<25:32:55,  2.20s/it]\u001b[A\n",
      "Running loss: 3.640780▏                  | 453/42263 [16:40<25:30:54,  2.20s/it]\u001b[A\n",
      "Running loss: 3.656737▏                  | 454/42263 [16:42<25:29:57,  2.20s/it]\u001b[A\n",
      "Running loss: 3.817456▏                  | 455/42263 [16:44<25:31:53,  2.20s/it]\u001b[A\n",
      "Running loss: 3.774616▏                  | 456/42263 [16:46<25:28:55,  2.19s/it]\u001b[A\n",
      "Running loss: 3.805948▏                  | 457/42263 [16:49<25:34:42,  2.20s/it]\u001b[A\n",
      "Running loss: 4.071864▏                  | 458/42263 [16:51<25:38:19,  2.21s/it]\u001b[A\n",
      "Running loss: 3.625919▏                  | 459/42263 [16:53<25:38:09,  2.21s/it]\u001b[A\n",
      "Running loss: 3.915888▏                  | 460/42263 [16:55<25:37:24,  2.21s/it]\u001b[A\n",
      "Running loss: 3.326241▏                  | 461/42263 [16:57<25:39:45,  2.21s/it]\u001b[A\n",
      "Running loss: 3.316241▏                  | 462/42263 [17:00<25:38:40,  2.21s/it]\u001b[A\n",
      "Running loss: 3.619463▏                  | 463/42263 [17:02<25:35:11,  2.20s/it]\u001b[A\n",
      "Running loss: 3.697331▏                  | 464/42263 [17:04<25:35:20,  2.20s/it]\u001b[A\n",
      "Running loss: 3.528866▏                  | 465/42263 [17:06<25:30:40,  2.20s/it]\u001b[A\n",
      "Running loss: 3.709311▏                  | 466/42263 [17:08<25:34:59,  2.20s/it]\u001b[A\n",
      "Running loss: 3.423747▏                  | 467/42263 [17:11<25:36:04,  2.21s/it]\u001b[A\n",
      "Running loss: 3.688171▏                  | 468/42263 [17:13<25:35:17,  2.20s/it]\u001b[A\n",
      "Running loss: 3.584456▏                  | 469/42263 [17:15<25:40:38,  2.21s/it]\u001b[A\n",
      "Running loss: 3.476264▏                  | 470/42263 [17:17<25:41:28,  2.21s/it]\u001b[A\n",
      "Running loss: 3.321113▏                  | 471/42263 [17:20<25:39:51,  2.21s/it]\u001b[A\n",
      "Running loss: 3.684739▏                  | 472/42263 [17:22<25:37:05,  2.21s/it]\u001b[A\n",
      "Running loss: 3.408367▏                  | 473/42263 [17:24<25:27:43,  2.19s/it]\u001b[A\n",
      "Running loss: 3.915234▏                  | 474/42263 [17:26<25:28:31,  2.19s/it]\u001b[A\n",
      "Running loss: 3.521251▏                  | 475/42263 [17:28<25:27:02,  2.19s/it]\u001b[A\n",
      "Running loss: 3.770446▏                  | 476/42263 [17:30<25:28:15,  2.19s/it]\u001b[A\n",
      "Running loss: 3.579600▏                  | 477/42263 [17:33<25:32:51,  2.20s/it]\u001b[A\n",
      "Running loss: 3.575576▏                  | 478/42263 [17:35<25:30:59,  2.20s/it]\u001b[A\n",
      "Running loss: 4.180031▏                  | 479/42263 [17:37<25:30:05,  2.20s/it]\u001b[A\n",
      "Running loss: 3.672171▏                  | 480/42263 [17:39<25:31:41,  2.20s/it]\u001b[A\n",
      "Running loss: 4.146497▏                  | 481/42263 [17:41<25:29:29,  2.20s/it]\u001b[A\n",
      "Running loss: 3.048231▏                  | 482/42263 [17:44<25:30:48,  2.20s/it]\u001b[A\n",
      "Running loss: 3.312048▏                  | 483/42263 [17:46<25:27:20,  2.19s/it]\u001b[A\n",
      "Running loss: 3.690691▏                  | 484/42263 [17:48<25:32:27,  2.20s/it]\u001b[A\n",
      "Running loss: 3.874456▏                  | 485/42263 [17:50<25:31:58,  2.20s/it]\u001b[A\n",
      "Running loss: 3.672487▏                  | 486/42263 [17:52<25:31:02,  2.20s/it]\u001b[A\n",
      "Running loss: 3.379176▏                  | 487/42263 [17:55<25:28:24,  2.20s/it]\u001b[A\n",
      "Running loss: 3.592288▏                  | 488/42263 [17:57<25:31:54,  2.20s/it]\u001b[A\n",
      "Running loss: 3.869350▏                  | 489/42263 [17:59<25:27:20,  2.19s/it]\u001b[A\n",
      "Running loss: 4.154986▏                  | 490/42263 [18:01<25:30:29,  2.20s/it]\u001b[A\n",
      "Running loss: 3.950181▏                  | 491/42263 [18:03<25:29:50,  2.20s/it]\u001b[A\n",
      "Running loss: 3.910779▏                  | 492/42263 [18:06<25:29:07,  2.20s/it]\u001b[A\n",
      "Running loss: 3.754089▏                  | 493/42263 [18:08<25:31:50,  2.20s/it]\u001b[A\n",
      "Running loss: 3.551800▏                  | 494/42263 [18:10<25:31:44,  2.20s/it]\u001b[A\n",
      "Running loss: 3.777039▏                  | 495/42263 [18:12<25:32:26,  2.20s/it]\u001b[A\n",
      "Running loss: 3.538636▏                  | 496/42263 [18:14<25:31:26,  2.20s/it]\u001b[A\n",
      "Running loss: 3.923776▏                  | 497/42263 [18:17<25:28:51,  2.20s/it]\u001b[A\n",
      "Running loss: 3.171212▏                  | 498/42263 [18:19<25:31:30,  2.20s/it]\u001b[A\n",
      "Running loss: 3.988676▏                  | 499/42263 [18:21<25:30:47,  2.20s/it]\u001b[A\n",
      "Running loss: 3.620759▏                  | 500/42263 [18:23<25:28:12,  2.20s/it]\u001b[A\n",
      "Running loss: 3.572613▏                  | 501/42263 [18:25<25:28:35,  2.20s/it]\u001b[A\n",
      "Running loss: 3.511850▏                  | 502/42263 [18:28<25:29:56,  2.20s/it]\u001b[A\n",
      "Running loss: 3.806631▏                  | 503/42263 [18:30<25:27:33,  2.19s/it]\u001b[A\n",
      "Running loss: 3.989108▏                  | 504/42263 [18:32<25:30:10,  2.20s/it]\u001b[A\n",
      "Running loss: 3.729144▏                  | 505/42263 [18:34<25:26:00,  2.19s/it]\u001b[A\n",
      "Running loss: 3.703351▏                  | 506/42263 [18:36<25:27:02,  2.19s/it]\u001b[A\n",
      "Running loss: 3.778771▏                  | 507/42263 [18:39<25:31:32,  2.20s/it]\u001b[A\n",
      "Running loss: 3.671512▏                  | 508/42263 [18:41<25:29:05,  2.20s/it]\u001b[A\n",
      "Running loss: 3.612997▏                  | 509/42263 [18:43<25:32:10,  2.20s/it]\u001b[A\n",
      "Running loss: 3.511442▏                  | 510/42263 [18:45<25:37:41,  2.21s/it]\u001b[A\n",
      "Running loss: 3.745734▏                  | 511/42263 [18:47<25:36:33,  2.21s/it]\u001b[A\n",
      "Running loss: 3.755657▏                  | 512/42263 [18:50<25:35:29,  2.21s/it]\u001b[A\n",
      "Running loss: 3.719154▏                  | 513/42263 [18:52<25:31:54,  2.20s/it]\u001b[A\n",
      "Running loss: 4.089905▏                  | 514/42263 [18:54<25:28:07,  2.20s/it]\u001b[A\n",
      "Running loss: 3.580404▏                  | 515/42263 [18:56<25:40:41,  2.21s/it]\u001b[A\n",
      "Running loss: 4.024483▏                  | 516/42263 [18:59<25:38:10,  2.21s/it]\u001b[A\n",
      "Running loss: 3.782284▏                  | 517/42263 [19:01<25:38:57,  2.21s/it]\u001b[A\n",
      "Running loss: 3.966230▏                  | 518/42263 [19:03<25:42:08,  2.22s/it]\u001b[A\n",
      "Running loss: 3.953883▏                  | 519/42263 [19:05<25:41:25,  2.22s/it]\u001b[A\n",
      "Running loss: 3.765378▏                  | 520/42263 [19:07<25:41:10,  2.22s/it]\u001b[A\n",
      "Running loss: 3.870112▏                  | 521/42263 [19:10<25:39:31,  2.21s/it]\u001b[A\n",
      "Running loss: 3.710017▏                  | 522/42263 [19:12<25:38:27,  2.21s/it]\u001b[A\n",
      "Running loss: 3.531670▏                  | 523/42263 [19:14<25:35:59,  2.21s/it]\u001b[A\n",
      "Running loss: 3.943555▏                  | 524/42263 [19:16<25:32:40,  2.20s/it]\u001b[A\n",
      "Running loss: 3.413063▏                  | 525/42263 [19:18<25:31:58,  2.20s/it]\u001b[A\n",
      "Running loss: 3.697975▏                  | 526/42263 [19:21<25:35:45,  2.21s/it]\u001b[A\n",
      "Running loss: 3.837129▏                  | 527/42263 [19:23<25:32:35,  2.20s/it]\u001b[A\n",
      "Running loss: 3.598701▏                  | 528/42263 [19:25<25:33:09,  2.20s/it]\u001b[A\n",
      "Running loss: 3.693558▏                  | 529/42263 [19:27<25:29:43,  2.20s/it]\u001b[A\n",
      "Running loss: 3.410290▏                  | 530/42263 [19:29<25:29:30,  2.20s/it]\u001b[A\n",
      "Running loss: 3.675769▏                  | 531/42263 [19:32<25:29:57,  2.20s/it]\u001b[A\n",
      "Running loss: 3.414134▏                  | 532/42263 [19:34<25:31:12,  2.20s/it]\u001b[A\n",
      "Running loss: 3.871024▏                  | 533/42263 [19:36<25:27:50,  2.20s/it]\u001b[A\n",
      "Running loss: 3.717086▏                  | 534/42263 [19:38<25:33:53,  2.21s/it]\u001b[A\n",
      "Running loss: 3.743694▏                  | 535/42263 [19:40<25:33:21,  2.20s/it]\u001b[A\n",
      "Running loss: 3.681509▏                  | 536/42263 [19:43<25:35:22,  2.21s/it]\u001b[A\n",
      "Running loss: 3.788684▏                  | 537/42263 [19:45<25:31:48,  2.20s/it]\u001b[A\n",
      "Running loss: 3.681674▏                  | 538/42263 [19:47<25:35:05,  2.21s/it]\u001b[A\n",
      "Running loss: 3.471497▏                  | 539/42263 [19:49<25:31:59,  2.20s/it]\u001b[A\n",
      "Running loss: 3.568375▏                  | 540/42263 [19:51<25:34:55,  2.21s/it]\u001b[A\n",
      "Running loss: 3.937855▏                  | 541/42263 [19:54<25:40:53,  2.22s/it]\u001b[A\n",
      "Running loss: 3.734650▏                  | 542/42263 [19:56<25:37:43,  2.21s/it]\u001b[A\n",
      "Running loss: 3.809692▏                  | 543/42263 [19:58<25:32:04,  2.20s/it]\u001b[A\n",
      "Running loss: 3.681987▏                  | 544/42263 [20:00<25:34:27,  2.21s/it]\u001b[A\n",
      "Running loss: 3.778183▏                  | 545/42263 [20:02<25:28:58,  2.20s/it]\u001b[A\n",
      "Running loss: 3.719740▏                  | 546/42263 [20:05<25:34:42,  2.21s/it]\u001b[A\n",
      "Running loss: 3.470089▏                  | 547/42263 [20:07<25:31:23,  2.20s/it]\u001b[A\n",
      "Running loss: 3.505447▏                  | 548/42263 [20:09<25:32:52,  2.20s/it]\u001b[A\n",
      "Running loss: 3.642244▏                  | 549/42263 [20:11<25:32:10,  2.20s/it]\u001b[A\n",
      "Running loss: 3.673094▏                  | 550/42263 [20:14<25:32:59,  2.21s/it]\u001b[A\n",
      "Running loss: 3.684736▏                  | 551/42263 [20:16<25:33:07,  2.21s/it]\u001b[A\n",
      "Running loss: 3.959146▏                  | 552/42263 [20:18<25:32:45,  2.20s/it]\u001b[A\n",
      "Running loss: 3.616918▏                  | 553/42263 [20:20<25:32:18,  2.20s/it]\u001b[A\n",
      "Running loss: 3.672786▏                  | 554/42263 [20:22<25:33:37,  2.21s/it]\u001b[A\n",
      "Running loss: 3.825856▏                  | 555/42263 [20:25<25:30:35,  2.20s/it]\u001b[A\n",
      "Running loss: 3.950025▏                  | 556/42263 [20:27<25:28:17,  2.20s/it]\u001b[A\n",
      "Running loss: 3.727961▎                  | 557/42263 [20:29<25:25:58,  2.20s/it]\u001b[A\n",
      "Running loss: 3.790064▎                  | 558/42263 [20:31<25:24:45,  2.19s/it]\u001b[A\n",
      "Running loss: 3.265111▎                  | 559/42263 [20:33<25:30:29,  2.20s/it]\u001b[A\n",
      "Running loss: 3.703257▎                  | 560/42263 [20:36<25:31:47,  2.20s/it]\u001b[A\n",
      "Running loss: 3.539126▎                  | 561/42263 [20:38<25:34:08,  2.21s/it]\u001b[A\n",
      "Running loss: 3.562871▎                  | 562/42263 [20:40<25:32:52,  2.21s/it]\u001b[A\n",
      "Running loss: 3.648723▎                  | 563/42263 [20:42<25:32:24,  2.20s/it]\u001b[A\n",
      "Running loss: 3.858939▎                  | 564/42263 [20:44<25:32:48,  2.21s/it]\u001b[A\n",
      "Running loss: 3.718951▎                  | 565/42263 [20:47<25:28:33,  2.20s/it]\u001b[A\n",
      "Running loss: 3.924309▎                  | 566/42263 [20:49<25:27:33,  2.20s/it]\u001b[A\n",
      "Running loss: 3.656915▎                  | 567/42263 [20:51<25:30:01,  2.20s/it]\u001b[A\n",
      "Running loss: 3.781161▎                  | 568/42263 [20:53<25:31:49,  2.20s/it]\u001b[A\n",
      "Running loss: 3.863622▎                  | 569/42263 [20:57<30:05:16,  2.60s/it]\u001b[A\n",
      "Running loss: 3.590330▎                  | 570/42263 [20:59<28:42:25,  2.48s/it]\u001b[A\n",
      "Running loss: 3.751888▎                  | 571/42263 [21:01<27:42:25,  2.39s/it]\u001b[A\n",
      "Running loss: 3.664766▎                  | 572/42263 [21:03<27:06:05,  2.34s/it]\u001b[A\n",
      "Running loss: 3.867612▎                  | 573/42263 [21:05<26:34:05,  2.29s/it]\u001b[A\n",
      "Running loss: 3.475937▎                  | 574/42263 [21:08<26:18:24,  2.27s/it]\u001b[A\n",
      "Running loss: 3.390967▎                  | 575/42263 [21:10<26:03:20,  2.25s/it]\u001b[A\n",
      "Running loss: 3.723859▎                  | 576/42263 [21:12<25:50:46,  2.23s/it]\u001b[A\n",
      "Running loss: 3.309871▎                  | 577/42263 [21:14<25:44:02,  2.22s/it]\u001b[A\n",
      "Running loss: 3.515728▎                  | 578/42263 [21:16<25:39:54,  2.22s/it]\u001b[A\n",
      "Running loss: 3.807993▎                  | 579/42263 [21:19<25:33:44,  2.21s/it]\u001b[A\n",
      "Running loss: 3.319815▎                  | 580/42263 [21:21<25:32:23,  2.21s/it]\u001b[A\n",
      "Running loss: 3.389545▎                  | 581/42263 [21:23<25:35:30,  2.21s/it]\u001b[A\n",
      "Running loss: 3.320772▎                  | 582/42263 [21:25<25:33:37,  2.21s/it]\u001b[A\n",
      "Running loss: 3.632348▎                  | 583/42263 [21:27<25:31:01,  2.20s/it]\u001b[A\n",
      "Running loss: 3.298367▎                  | 584/42263 [21:30<25:34:59,  2.21s/it]\u001b[A\n",
      "Running loss: 3.591412▎                  | 585/42263 [21:32<25:31:22,  2.20s/it]\u001b[A\n",
      "Running loss: 3.731290▎                  | 586/42263 [21:34<25:31:08,  2.20s/it]\u001b[A\n",
      "Running loss: 3.619570▎                  | 587/42263 [21:36<25:29:32,  2.20s/it]\u001b[A\n",
      "Running loss: 3.636036▎                  | 588/42263 [21:39<25:31:41,  2.21s/it]\u001b[A\n",
      "Running loss: 3.886517▎                  | 589/42263 [21:41<25:31:29,  2.20s/it]\u001b[A\n",
      "Running loss: 3.457843▎                  | 590/42263 [21:43<25:29:17,  2.20s/it]\u001b[A\n",
      "Running loss: 3.638778▎                  | 591/42263 [21:45<25:28:54,  2.20s/it]\u001b[A\n",
      "Running loss: 3.740407▎                  | 592/42263 [21:47<25:29:26,  2.20s/it]\u001b[A\n",
      "Running loss: 4.288764▎                  | 593/42263 [21:50<25:29:36,  2.20s/it]\u001b[A\n",
      "Running loss: 4.042322▎                  | 594/42263 [21:52<25:32:12,  2.21s/it]\u001b[A\n",
      "Running loss: 4.059442▎                  | 595/42263 [21:54<25:32:26,  2.21s/it]\u001b[A\n",
      "Running loss: 3.589590▎                  | 596/42263 [21:56<25:31:45,  2.21s/it]\u001b[A\n",
      "Running loss: 3.859420▎                  | 597/42263 [21:58<25:30:34,  2.20s/it]\u001b[A\n",
      "Running loss: 3.788246▎                  | 598/42263 [22:01<25:31:24,  2.21s/it]\u001b[A\n",
      "Running loss: 3.946837▎                  | 599/42263 [22:03<25:30:49,  2.20s/it]\u001b[A\n",
      "Running loss: 4.104553▎                  | 600/42263 [22:05<25:37:49,  2.21s/it]\u001b[A\n",
      "Running loss: 3.417026▎                  | 601/42263 [22:07<25:35:15,  2.21s/it]\u001b[A\n",
      "Running loss: 3.667710▎                  | 602/42263 [22:09<25:37:35,  2.21s/it]\u001b[A\n",
      "Running loss: 3.733471▎                  | 603/42263 [22:12<25:34:14,  2.21s/it]\u001b[A\n",
      "Running loss: 3.614590▎                  | 604/42263 [22:14<25:30:26,  2.20s/it]\u001b[A\n",
      "Running loss: 3.636909▎                  | 605/42263 [22:16<25:30:35,  2.20s/it]\u001b[A\n",
      "Running loss: 3.681950▎                  | 606/42263 [22:18<25:33:33,  2.21s/it]\u001b[A\n",
      "Running loss: 3.611398▎                  | 607/42263 [22:20<25:30:11,  2.20s/it]\u001b[A\n",
      "Running loss: 3.600869▎                  | 608/42263 [22:23<25:29:17,  2.20s/it]\u001b[A\n",
      "Running loss: 3.270925▎                  | 609/42263 [22:25<25:31:18,  2.21s/it]\u001b[A\n",
      "Running loss: 3.534237▎                  | 610/42263 [22:27<25:30:03,  2.20s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "!python fine_tune.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
