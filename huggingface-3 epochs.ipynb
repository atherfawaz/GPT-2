{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install simpletransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /mnt/batch/tasks/shared/LS_root/mounts/clusters/gptcomputeinstance/code/users/atherfawaz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python data_prep.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVIDIA/apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" --user ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -v --no-cache-dir ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile setup.sh\n",
    "\n",
    "pip install -v --no-cache-dir ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd simpletransformers/examples/language_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%cd apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "!nvcc --version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "!pip install nvcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.4+cu100 torchvision==0.5.0+cu100 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python setup.py install --cuda_ext --cpp_ext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd simpletransformers/examples/language_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install pytorch torchvision cudatoolkit=10.0 -c pytorch -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda update -n base -c defaults conda -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python fine_tune.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/gptcomputeinstance/code/users/atherfawaz/simpletransformers/examples/language_generation\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.language_modeling.language_modeling_utils: Creating features from dataset file at cache_dir/\n",
      "  0%|                                                  | 0/1999 [00:00<?, ?it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1156 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1164 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1268 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1558 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1038 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1131 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1191 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (2624 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1300 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1163 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1465 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1128 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1261 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1475 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1258 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1035 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1044 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1079 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1128 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1174 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (6815 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1405 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (2071 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1322 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1122 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1483 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1527 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1280 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (2416 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1374 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (2519 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1165 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1596 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1619 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1313 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (2515 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (2045 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1106 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1039 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1828 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1032 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (2188 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1175 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1119 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1180 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1041 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1075 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1085 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1427 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1043 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1761 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1124 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1475 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1041 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (2012 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1103 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1297 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1348 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1184 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1190 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1331 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1469 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1243 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1308 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1035 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1690 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1350 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1395 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1181 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1238 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1564 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1112 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1107 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (2724 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|█████████████████████████████████████| 1999/1999 [00:01<00:00, 1409.52it/s]\n",
      "100%|████████████████████████████████████| 1118/1118 [00:00<00:00, 67155.00it/s]\n",
      "INFO:simpletransformers.language_modeling.language_modeling_utils: Saving features into cached file cache_dir/gpt2_cached_lm_508_train.txt\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ImportError('/anaconda/envs/azureml_py36/lib/python3.6/site-packages/apex-0.1-py3.6-linux-x86_64.egg/amp_C.cpython-36m-x86_64-linux-gnu.so: undefined symbol: THPVariableClass',)\n",
      "INFO:simpletransformers.language_modeling.language_modeling_model: Training started\n",
      "Epoch 1 of 3:   0%|                                       | 0/3 [00:00<?, ?it/s]\n",
      "Running Epoch 0:   0%|                                  | 0/140 [00:00<?, ?it/s]\u001b[A/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Running loss: 4.176511Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:114: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "\n",
      "Running loss: 3.894776▏                         | 1/140 [00:04<11:29,  4.96s/it]\u001b[AGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "\n",
      "Running loss: 4.014006▎                         | 2/140 [00:07<09:25,  4.10s/it]\u001b[A\n",
      "Running loss: 3.832789▌                         | 3/140 [00:09<08:08,  3.56s/it]\u001b[A\n",
      "Running loss: 3.780836▋                         | 4/140 [00:11<07:10,  3.17s/it]\u001b[A\n",
      "Running loss: 3.860696▉                         | 5/140 [00:13<06:27,  2.87s/it]\u001b[A\n",
      "Running loss: 3.895560█                         | 6/140 [00:15<05:58,  2.67s/it]\u001b[A\n",
      "Running loss: 3.969404█▎                        | 7/140 [00:18<05:36,  2.53s/it]\u001b[A\n",
      "Running loss: 3.741759█▍                        | 8/140 [00:20<05:20,  2.43s/it]\u001b[A\n",
      "Running loss: 3.914816█▋                        | 9/140 [00:22<05:10,  2.37s/it]\u001b[A\n",
      "Running loss: 3.816343█▊                       | 10/140 [00:24<05:01,  2.32s/it]\u001b[A\n",
      "Running loss: 3.934941█▉                       | 11/140 [00:27<04:54,  2.28s/it]\u001b[A\n",
      "Running loss: 3.886886██▏                      | 12/140 [00:29<04:48,  2.26s/it]\u001b[A\n",
      "Running loss: 3.852580██▎                      | 13/140 [00:31<04:43,  2.24s/it]\u001b[A\n",
      "Running loss: 3.603018██▌                      | 14/140 [00:33<04:40,  2.23s/it]\u001b[A\n",
      "Running loss: 3.817444██▋                      | 15/140 [00:35<04:37,  2.22s/it]\u001b[A\n",
      "Running loss: 3.901212██▊                      | 16/140 [00:38<04:35,  2.22s/it]\u001b[A\n",
      "Running loss: 3.584228███                      | 17/140 [00:40<04:32,  2.22s/it]\u001b[A\n",
      "Running loss: 3.853185███▏                     | 18/140 [00:42<04:30,  2.22s/it]\u001b[A\n",
      "Running loss: 3.885549███▍                     | 19/140 [00:44<04:28,  2.22s/it]\u001b[A\n",
      "Running loss: 3.706438███▌                     | 20/140 [00:46<04:26,  2.22s/it]\u001b[A\n",
      "Running loss: 3.868477███▊                     | 21/140 [00:49<04:24,  2.22s/it]\u001b[A\n",
      "Running loss: 3.924040███▉                     | 22/140 [00:51<04:22,  2.22s/it]\u001b[A\n",
      "Running loss: 3.600245████                     | 23/140 [00:53<04:20,  2.22s/it]\u001b[A\n",
      "Running loss: 3.735664████▎                    | 24/140 [00:55<04:18,  2.23s/it]\u001b[A\n",
      "Running loss: 3.569247████▍                    | 25/140 [00:58<04:15,  2.22s/it]\u001b[A\n",
      "Running loss: 3.863712████▋                    | 26/140 [01:00<04:13,  2.22s/it]\u001b[A\n",
      "Running loss: 3.859853████▊                    | 27/140 [01:02<04:10,  2.22s/it]\u001b[A\n",
      "Running loss: 3.815168█████                    | 28/140 [01:04<04:09,  2.23s/it]\u001b[A\n",
      "Running loss: 3.784356█████▏                   | 29/140 [01:06<04:07,  2.23s/it]\u001b[A\n",
      "Running loss: 3.837805█████▎                   | 30/140 [01:09<04:04,  2.23s/it]\u001b[A\n",
      "Running loss: 3.559393█████▌                   | 31/140 [01:11<04:02,  2.23s/it]\u001b[A\n",
      "Running loss: 3.601747█████▋                   | 32/140 [01:13<04:00,  2.23s/it]\u001b[A\n",
      "Running loss: 3.787097█████▉                   | 33/140 [01:15<03:58,  2.22s/it]\u001b[A\n",
      "Running loss: 3.866370██████                   | 34/140 [01:18<03:56,  2.23s/it]\u001b[A\n",
      "Running loss: 3.755565██████▎                  | 35/140 [01:20<03:54,  2.23s/it]\u001b[A\n",
      "Running loss: 4.047587██████▍                  | 36/140 [01:22<03:51,  2.23s/it]\u001b[A\n",
      "Running loss: 3.983801██████▌                  | 37/140 [01:24<03:48,  2.22s/it]\u001b[A\n",
      "Running loss: 3.936525██████▊                  | 38/140 [01:26<03:46,  2.22s/it]\u001b[A\n",
      "Running loss: 3.763622██████▉                  | 39/140 [01:29<03:43,  2.22s/it]\u001b[A\n",
      "Running loss: 3.587405███████▏                 | 40/140 [01:31<03:41,  2.21s/it]\u001b[A\n",
      "Running loss: 3.802850███████▎                 | 41/140 [01:33<03:39,  2.22s/it]\u001b[A\n",
      "Running loss: 3.459133███████▌                 | 42/140 [01:35<03:37,  2.22s/it]\u001b[A\n",
      "Running loss: 3.321438███████▋                 | 43/140 [01:38<03:35,  2.22s/it]\u001b[A\n",
      "Running loss: 3.741206███████▊                 | 44/140 [01:40<03:33,  2.22s/it]\u001b[A\n",
      "Running loss: 3.571344████████                 | 45/140 [01:42<03:30,  2.22s/it]\u001b[A\n",
      "Running loss: 3.874550████████▏                | 46/140 [01:44<03:28,  2.22s/it]\u001b[A\n",
      "Running loss: 3.912819████████▍                | 47/140 [01:46<03:26,  2.22s/it]\u001b[A\n",
      "Running loss: 3.931778████████▌                | 48/140 [01:49<03:24,  2.22s/it]\u001b[A\n",
      "Running loss: 3.719913████████▊                | 49/140 [01:51<03:22,  2.23s/it]\u001b[A/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "\n",
      "Running loss: 3.776485████████▉                | 50/140 [01:53<03:20,  2.22s/it]\u001b[A\n",
      "Running loss: 3.369112█████████                | 51/140 [01:55<03:17,  2.22s/it]\u001b[A\n",
      "Running loss: 3.538939█████████▎               | 52/140 [01:58<03:15,  2.22s/it]\u001b[A\n",
      "Running loss: 3.783559█████████▍               | 53/140 [02:00<03:13,  2.23s/it]\u001b[A\n",
      "Running loss: 3.716040█████████▋               | 54/140 [02:02<03:11,  2.23s/it]\u001b[A\n",
      "Running loss: 3.766410█████████▊               | 55/140 [02:04<03:09,  2.23s/it]\u001b[A\n",
      "Running loss: 3.490636██████████               | 56/140 [02:06<03:07,  2.23s/it]\u001b[A\n",
      "Running loss: 3.855468██████████▏              | 57/140 [02:09<03:04,  2.22s/it]\u001b[A\n",
      "Running loss: 3.836782██████████▎              | 58/140 [02:11<03:02,  2.22s/it]\u001b[A\n",
      "Running loss: 3.730306██████████▌              | 59/140 [02:13<03:00,  2.23s/it]\u001b[A\n",
      "Running loss: 3.739736██████████▋              | 60/140 [02:15<02:57,  2.22s/it]\u001b[A\n",
      "Running loss: 3.891434██████████▉              | 61/140 [02:18<02:55,  2.22s/it]\u001b[A\n",
      "Running loss: 3.610576███████████              | 62/140 [02:20<02:53,  2.22s/it]\u001b[A\n",
      "Running loss: 3.829657███████████▎             | 63/140 [02:22<02:50,  2.22s/it]\u001b[A\n",
      "Running loss: 3.661235███████████▍             | 64/140 [02:24<02:48,  2.22s/it]\u001b[A\n",
      "Running loss: 3.780015███████████▌             | 65/140 [02:26<02:46,  2.22s/it]\u001b[A\n",
      "Running loss: 3.508149███████████▊             | 66/140 [02:29<02:44,  2.22s/it]\u001b[A\n",
      "Running loss: 3.520714███████████▉             | 67/140 [02:31<02:42,  2.22s/it]\u001b[A\n",
      "Running loss: 3.814346████████████▏            | 68/140 [02:33<02:39,  2.22s/it]\u001b[A\n",
      "Running loss: 3.726942████████████▎            | 69/140 [02:35<02:37,  2.22s/it]\u001b[A\n",
      "Running loss: 3.651502████████████▌            | 70/140 [02:38<02:35,  2.22s/it]\u001b[A\n",
      "Running loss: 3.647239████████████▋            | 71/140 [02:40<02:33,  2.22s/it]\u001b[A\n",
      "Running loss: 3.669193████████████▊            | 72/140 [02:42<02:31,  2.22s/it]\u001b[A\n",
      "Running loss: 3.555893█████████████            | 73/140 [02:44<02:28,  2.22s/it]\u001b[A\n",
      "Running loss: 3.690189█████████████▏           | 74/140 [02:46<02:26,  2.22s/it]\u001b[A\n",
      "Running loss: 3.829639█████████████▍           | 75/140 [02:49<02:24,  2.23s/it]\u001b[A\n",
      "Running loss: 3.528627█████████████▌           | 76/140 [02:51<02:22,  2.23s/it]\u001b[A\n",
      "Running loss: 3.692484█████████████▊           | 77/140 [02:53<02:20,  2.23s/it]\u001b[A\n",
      "Running loss: 3.924304█████████████▉           | 78/140 [02:55<02:18,  2.23s/it]\u001b[A\n",
      "Running loss: 3.464850██████████████           | 79/140 [02:58<02:15,  2.23s/it]\u001b[A\n",
      "Running loss: 3.696363██████████████▎          | 80/140 [03:00<02:13,  2.22s/it]\u001b[A\n",
      "Running loss: 3.662112██████████████▍          | 81/140 [03:02<02:10,  2.22s/it]\u001b[A\n",
      "Running loss: 3.505960██████████████▋          | 82/140 [03:04<02:09,  2.23s/it]\u001b[A\n",
      "Running loss: 3.163327██████████████▊          | 83/140 [03:06<02:06,  2.22s/it]\u001b[A\n",
      "Running loss: 3.791536███████████████          | 84/140 [03:09<02:04,  2.22s/it]\u001b[A\n",
      "Running loss: 3.769842███████████████▏         | 85/140 [03:11<02:02,  2.22s/it]\u001b[A\n",
      "Running loss: 3.713361███████████████▎         | 86/140 [03:13<02:00,  2.23s/it]\u001b[A\n",
      "Running loss: 3.825993███████████████▌         | 87/140 [03:15<01:57,  2.22s/it]\u001b[A\n",
      "Running loss: 3.438610███████████████▋         | 88/140 [03:18<01:55,  2.22s/it]\u001b[A\n",
      "Running loss: 3.912665███████████████▉         | 89/140 [03:20<01:53,  2.23s/it]\u001b[A\n",
      "Running loss: 3.346229████████████████         | 90/140 [03:22<01:51,  2.22s/it]\u001b[A\n",
      "Running loss: 3.717478████████████████▎        | 91/140 [03:24<01:49,  2.23s/it]\u001b[A\n",
      "Running loss: 3.593933████████████████▍        | 92/140 [03:26<01:46,  2.22s/it]\u001b[A\n",
      "Running loss: 3.598038████████████████▌        | 93/140 [03:29<01:44,  2.23s/it]\u001b[A\n",
      "Running loss: 3.788320████████████████▊        | 94/140 [03:31<01:42,  2.23s/it]\u001b[A\n",
      "Running loss: 3.510333████████████████▉        | 95/140 [03:33<01:40,  2.23s/it]\u001b[A\n",
      "Running loss: 3.541995█████████████████▏       | 96/140 [03:35<01:38,  2.23s/it]\u001b[A\n",
      "Running loss: 3.931744█████████████████▎       | 97/140 [03:38<01:36,  2.23s/it]\u001b[A\n",
      "Running loss: 3.668647█████████████████▌       | 98/140 [03:40<01:33,  2.22s/it]\u001b[A\n",
      "Running loss: 3.900142█████████████████▋       | 99/140 [03:42<01:31,  2.23s/it]\u001b[A\n",
      "Running loss: 3.516160█████████████████▏      | 100/140 [03:44<01:29,  2.23s/it]\u001b[A\n",
      "Running loss: 3.762585█████████████████▎      | 101/140 [03:47<01:26,  2.23s/it]\u001b[A\n",
      "Running loss: 3.777141█████████████████▍      | 102/140 [03:49<01:24,  2.24s/it]\u001b[A\n",
      "Running loss: 3.691881█████████████████▋      | 103/140 [03:51<01:22,  2.23s/it]\u001b[A\n",
      "Running loss: 3.606890█████████████████▊      | 104/140 [03:53<01:20,  2.24s/it]\u001b[A\n",
      "Running loss: 3.652871██████████████████      | 105/140 [03:56<01:18,  2.24s/it]\u001b[A\n",
      "Running loss: 3.475794██████████████████▏     | 106/140 [03:58<01:15,  2.23s/it]\u001b[A\n",
      "Running loss: 3.614062██████████████████▎     | 107/140 [04:00<01:13,  2.23s/it]\u001b[A\n",
      "Running loss: 3.676252██████████████████▌     | 108/140 [04:02<01:11,  2.23s/it]\u001b[A\n",
      "Running loss: 3.620550██████████████████▋     | 109/140 [04:04<01:09,  2.23s/it]\u001b[A\n",
      "Running loss: 3.607483██████████████████▊     | 110/140 [04:07<01:07,  2.23s/it]\u001b[A\n",
      "Running loss: 3.662906███████████████████     | 111/140 [04:09<01:04,  2.22s/it]\u001b[A\n",
      "Running loss: 3.533586███████████████████▏    | 112/140 [04:11<01:02,  2.23s/it]\u001b[A\n",
      "Running loss: 3.823031███████████████████▎    | 113/140 [04:13<01:00,  2.23s/it]\u001b[A\n",
      "Running loss: 3.595505███████████████████▌    | 114/140 [04:16<00:57,  2.22s/it]\u001b[A\n",
      "Running loss: 3.851479███████████████████▋    | 115/140 [04:18<00:55,  2.23s/it]\u001b[A\n",
      "Running loss: 3.744446███████████████████▉    | 116/140 [04:20<00:53,  2.23s/it]\u001b[A\n",
      "Running loss: 3.543859████████████████████    | 117/140 [04:22<00:51,  2.22s/it]\u001b[A\n",
      "Running loss: 3.606750████████████████████▏   | 118/140 [04:24<00:48,  2.22s/it]\u001b[A\n",
      "Running loss: 3.634459████████████████████▍   | 119/140 [04:27<00:46,  2.22s/it]\u001b[A\n",
      "Running loss: 3.669230████████████████████▌   | 120/140 [04:29<00:44,  2.22s/it]\u001b[A\n",
      "Running loss: 3.654141████████████████████▋   | 121/140 [04:31<00:42,  2.22s/it]\u001b[A\n",
      "Running loss: 3.678795████████████████████▉   | 122/140 [04:33<00:39,  2.22s/it]\u001b[A\n",
      "Running loss: 3.943432█████████████████████   | 123/140 [04:36<00:37,  2.23s/it]\u001b[A\n",
      "Running loss: 3.520351█████████████████████▎  | 124/140 [04:38<00:35,  2.22s/it]\u001b[A\n",
      "Running loss: 3.805174█████████████████████▍  | 125/140 [04:40<00:33,  2.22s/it]\u001b[A\n",
      "Running loss: 3.790568█████████████████████▌  | 126/140 [04:42<00:31,  2.23s/it]\u001b[A\n",
      "Running loss: 3.671117█████████████████████▊  | 127/140 [04:44<00:28,  2.22s/it]\u001b[A\n",
      "Running loss: 3.545817█████████████████████▉  | 128/140 [04:47<00:26,  2.23s/it]\u001b[A\n",
      "Running loss: 3.539603██████████████████████  | 129/140 [04:49<00:24,  2.23s/it]\u001b[A\n",
      "Running loss: 3.768201██████████████████████▎ | 130/140 [04:51<00:22,  2.23s/it]\u001b[A\n",
      "Running loss: 3.728083██████████████████████▍ | 131/140 [04:53<00:20,  2.24s/it]\u001b[A\n",
      "Running loss: 3.878529██████████████████████▋ | 132/140 [04:56<00:17,  2.23s/it]\u001b[A\n",
      "Running loss: 3.635631██████████████████████▊ | 133/140 [04:58<00:15,  2.23s/it]\u001b[A\n",
      "Running loss: 3.765838██████████████████████▉ | 134/140 [05:00<00:13,  2.23s/it]\u001b[A\n",
      "Running loss: 3.647573███████████████████████▏| 135/140 [05:02<00:11,  2.23s/it]\u001b[A\n",
      "Running loss: 3.453679███████████████████████▎| 136/140 [05:05<00:08,  2.23s/it]\u001b[A\n",
      "Running loss: 3.769789███████████████████████▍| 137/140 [05:07<00:06,  2.23s/it]\u001b[A\n",
      "Running loss: 3.632118███████████████████████▋| 138/140 [05:09<00:04,  2.24s/it]\u001b[A\n",
      "Running loss: 3.530754███████████████████████▊| 139/140 [05:11<00:02,  2.23s/it]\u001b[A\n",
      "Running Epoch 0: 100%|████████████████████████| 140/140 [05:13<00:00,  2.09s/it]\u001b[A/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "Epoch 2 of 3:  33%|██████████                    | 1/3 [05:58<11:57, 358.85s/it]\n",
      "Running loss: 3.859704                                  | 0/140 [00:00<?, ?it/s]\u001b[A\n",
      "Running loss: 3.773356▏                         | 1/140 [00:02<05:16,  2.28s/it]\u001b[A\n",
      "Running loss: 3.465800▎                         | 2/140 [00:04<05:12,  2.26s/it]\u001b[A\n",
      "Running loss: 3.425005▌                         | 3/140 [00:06<05:07,  2.25s/it]\u001b[A\n",
      "Running loss: 3.634618▋                         | 4/140 [00:08<05:04,  2.24s/it]\u001b[A\n",
      "Running loss: 3.590884▉                         | 5/140 [00:11<05:01,  2.24s/it]\u001b[A\n",
      "Running loss: 3.449756█                         | 6/140 [00:13<04:59,  2.23s/it]\u001b[A\n",
      "Running loss: 3.313916█▎                        | 7/140 [00:15<04:56,  2.23s/it]\u001b[A\n",
      "Running loss: 3.510898█▍                        | 8/140 [00:17<04:53,  2.22s/it]\u001b[A\n",
      "Running loss: 3.677605█▋                        | 9/140 [00:20<04:50,  2.22s/it]\u001b[A\n",
      "Running loss: 3.635163█▊                       | 10/140 [00:22<04:48,  2.22s/it]\u001b[A\n",
      "Running loss: 3.347993█▉                       | 11/140 [00:24<04:46,  2.22s/it]\u001b[A\n",
      "Running loss: 3.512851██▏                      | 12/140 [00:26<04:44,  2.23s/it]\u001b[A\n",
      "Running loss: 3.477285██▎                      | 13/140 [00:28<04:42,  2.23s/it]\u001b[A\n",
      "Running loss: 3.735532██▌                      | 14/140 [00:31<04:39,  2.22s/it]\u001b[A\n",
      "Running loss: 3.498405██▋                      | 15/140 [00:33<04:38,  2.23s/it]\u001b[A\n",
      "Running loss: 3.617908██▊                      | 16/140 [00:35<04:36,  2.23s/it]\u001b[A\n",
      "Running loss: 3.772392███                      | 17/140 [00:37<04:34,  2.23s/it]\u001b[A\n",
      "Running loss: 3.605007███▏                     | 18/140 [00:40<04:32,  2.23s/it]\u001b[A\n",
      "Running loss: 3.590342███▍                     | 19/140 [00:42<04:28,  2.22s/it]\u001b[A\n",
      "Running loss: 3.456407███▌                     | 20/140 [00:44<04:26,  2.22s/it]\u001b[A\n",
      "Running loss: 3.551423███▊                     | 21/140 [00:46<04:24,  2.22s/it]\u001b[A\n",
      "Running loss: 3.586920███▉                     | 22/140 [00:48<04:22,  2.22s/it]\u001b[A\n",
      "Running loss: 3.598548████                     | 23/140 [00:51<04:20,  2.22s/it]\u001b[A\n",
      "Running loss: 3.740192████▎                    | 24/140 [00:53<04:18,  2.23s/it]\u001b[A\n",
      "Running loss: 3.459185████▍                    | 25/140 [00:55<04:17,  2.24s/it]\u001b[A\n",
      "Running loss: 3.593341████▋                    | 26/140 [00:57<04:14,  2.24s/it]\u001b[A\n",
      "Running loss: 3.608880████▊                    | 27/140 [01:00<04:12,  2.23s/it]\u001b[A\n",
      "Running loss: 3.365895█████                    | 28/140 [01:02<04:10,  2.23s/it]\u001b[A\n",
      "Running loss: 3.719743█████▏                   | 29/140 [01:04<04:08,  2.23s/it]\u001b[A\n",
      "Running loss: 3.653409█████▎                   | 30/140 [01:06<04:05,  2.23s/it]\u001b[A\n",
      "Running loss: 3.567470█████▌                   | 31/140 [01:09<04:03,  2.24s/it]\u001b[A\n",
      "Running loss: 3.429012█████▋                   | 32/140 [01:11<04:02,  2.24s/it]\u001b[A\n",
      "Running loss: 3.576012█████▉                   | 33/140 [01:13<03:59,  2.24s/it]\u001b[A\n",
      "Running loss: 3.609675██████                   | 34/140 [01:15<03:57,  2.24s/it]\u001b[A\n",
      "Running loss: 3.561895██████▎                  | 35/140 [01:18<03:54,  2.23s/it]\u001b[A\n",
      "Running loss: 3.351001██████▍                  | 36/140 [01:20<03:52,  2.23s/it]\u001b[A\n",
      "Running loss: 3.660995██████▌                  | 37/140 [01:22<03:49,  2.23s/it]\u001b[A\n",
      "Running loss: 3.393177██████▊                  | 38/140 [01:24<03:47,  2.23s/it]\u001b[A\n",
      "Running loss: 3.155980██████▉                  | 39/140 [01:26<03:45,  2.23s/it]\u001b[A\n",
      "Running loss: 3.357294███████▏                 | 40/140 [01:29<03:43,  2.23s/it]\u001b[A\n",
      "Running loss: 3.507008███████▎                 | 41/140 [01:31<03:41,  2.24s/it]\u001b[A\n",
      "Running loss: 3.535253███████▌                 | 42/140 [01:33<03:39,  2.24s/it]\u001b[A\n",
      "Running loss: 3.353602███████▋                 | 43/140 [01:35<03:36,  2.24s/it]\u001b[A\n",
      "Running loss: 3.824414███████▊                 | 44/140 [01:38<03:34,  2.24s/it]\u001b[A\n",
      "Running loss: 3.807327████████                 | 45/140 [01:40<03:32,  2.24s/it]\u001b[A\n",
      "Running loss: 3.797991████████▏                | 46/140 [01:42<03:30,  2.24s/it]\u001b[A\n",
      "Running loss: 3.610161████████▍                | 47/140 [01:44<03:28,  2.24s/it]\u001b[A\n",
      "Running loss: 3.596004████████▌                | 48/140 [01:47<03:25,  2.24s/it]\u001b[A\n",
      "Running loss: 3.597646████████▊                | 49/140 [01:49<03:23,  2.24s/it]\u001b[A\n",
      "Running loss: 3.507491████████▉                | 50/140 [01:51<03:21,  2.23s/it]\u001b[A\n",
      "Running loss: 3.382251█████████                | 51/140 [01:53<03:18,  2.23s/it]\u001b[A\n",
      "Running loss: 3.445909█████████▎               | 52/140 [01:56<03:16,  2.23s/it]\u001b[A\n",
      "Running loss: 3.543739█████████▍               | 53/140 [01:58<03:14,  2.24s/it]\u001b[A\n",
      "Running loss: 3.372448█████████▋               | 54/140 [02:00<03:12,  2.24s/it]\u001b[A\n",
      "Running loss: 3.527853█████████▊               | 55/140 [02:02<03:10,  2.24s/it]\u001b[A\n",
      "Running loss: 3.530536██████████               | 56/140 [02:05<03:08,  2.24s/it]\u001b[A\n",
      "Running loss: 3.396636██████████▏              | 57/140 [02:07<03:06,  2.24s/it]\u001b[A\n",
      "Running loss: 3.632242██████████▎              | 58/140 [02:09<03:03,  2.24s/it]\u001b[A\n",
      "Running loss: 3.547495██████████▌              | 59/140 [02:11<03:01,  2.24s/it]\u001b[A\n",
      "Running loss: 3.328639██████████▋              | 60/140 [02:14<02:59,  2.25s/it]\u001b[A\n",
      "Running loss: 3.620981██████████▉              | 61/140 [02:16<02:57,  2.24s/it]\u001b[A\n",
      "Running loss: 3.601307███████████              | 62/140 [02:18<02:55,  2.25s/it]\u001b[A\n",
      "Running loss: 3.588982███████████▎             | 63/140 [02:20<02:52,  2.25s/it]\u001b[A\n",
      "Running loss: 3.372546███████████▍             | 64/140 [02:22<02:50,  2.25s/it]\u001b[A\n",
      "Running loss: 3.826444███████████▌             | 65/140 [02:25<02:48,  2.24s/it]\u001b[A\n",
      "Running loss: 3.439142███████████▊             | 66/140 [02:27<02:45,  2.24s/it]\u001b[A\n",
      "Running loss: 3.584414███████████▉             | 67/140 [02:29<02:44,  2.25s/it]\u001b[A\n",
      "Running loss: 3.531600████████████▏            | 68/140 [02:31<02:41,  2.25s/it]\u001b[A\n",
      "Running loss: 3.663580████████████▎            | 69/140 [02:34<02:39,  2.25s/it]\u001b[A\n",
      "Running loss: 3.717425████████████▌            | 70/140 [02:36<02:37,  2.25s/it]\u001b[A\n",
      "Running loss: 3.674766████████████▋            | 71/140 [02:38<02:35,  2.25s/it]\u001b[A\n",
      "Running loss: 3.390766████████████▊            | 72/140 [02:40<02:32,  2.25s/it]\u001b[A\n",
      "Running loss: 3.512894█████████████            | 73/140 [02:43<02:30,  2.25s/it]\u001b[A\n",
      "Running loss: 3.432176█████████████▏           | 74/140 [02:45<02:28,  2.25s/it]\u001b[A\n",
      "Running loss: 3.704968█████████████▍           | 75/140 [02:47<02:26,  2.25s/it]\u001b[A\n",
      "Running loss: 3.623492█████████████▌           | 76/140 [02:49<02:23,  2.25s/it]\u001b[A\n",
      "Running loss: 3.497818█████████████▊           | 77/140 [02:52<02:21,  2.24s/it]\u001b[A\n",
      "Running loss: 3.609438█████████████▉           | 78/140 [02:54<02:18,  2.24s/it]\u001b[A\n",
      "Running loss: 3.599418██████████████           | 79/140 [02:56<02:16,  2.24s/it]\u001b[A\n",
      "Running loss: 3.697514██████████████▎          | 80/140 [02:58<02:14,  2.24s/it]\u001b[A\n",
      "Running loss: 3.462940██████████████▍          | 81/140 [03:01<02:12,  2.24s/it]\u001b[A\n",
      "Running loss: 3.377741██████████████▋          | 82/140 [03:03<02:09,  2.24s/it]\u001b[A\n",
      "Running loss: 3.605095██████████████▊          | 83/140 [03:05<02:08,  2.25s/it]\u001b[A\n",
      "Running loss: 3.803447███████████████          | 84/140 [03:07<02:05,  2.24s/it]\u001b[A\n",
      "Running loss: 3.631766███████████████▏         | 85/140 [03:10<02:03,  2.24s/it]\u001b[A\n",
      "Running loss: 3.564399███████████████▎         | 86/140 [03:12<02:00,  2.24s/it]\u001b[A\n",
      "Running loss: 3.616302███████████████▌         | 87/140 [03:14<01:58,  2.24s/it]\u001b[A\n",
      "Running loss: 3.502397███████████████▋         | 88/140 [03:16<01:56,  2.23s/it]\u001b[A\n",
      "Running loss: 3.683608███████████████▉         | 89/140 [03:19<01:54,  2.24s/it]\u001b[A\n",
      "Running loss: 3.542179████████████████         | 90/140 [03:21<01:52,  2.24s/it]\u001b[A\n",
      "Running loss: 3.572880████████████████▎        | 91/140 [03:23<01:50,  2.25s/it]\u001b[A\n",
      "Running loss: 3.543040████████████████▍        | 92/140 [03:25<01:47,  2.25s/it]\u001b[A\n",
      "Running loss: 3.745576████████████████▌        | 93/140 [03:28<01:45,  2.25s/it]\u001b[A\n",
      "Running loss: 3.340601████████████████▊        | 94/140 [03:30<01:43,  2.25s/it]\u001b[A\n",
      "Running loss: 3.470829████████████████▉        | 95/140 [03:32<01:40,  2.24s/it]\u001b[A\n",
      "Running loss: 3.476215█████████████████▏       | 96/140 [03:34<01:38,  2.24s/it]\u001b[A\n",
      "Running loss: 3.716002█████████████████▎       | 97/140 [03:37<01:36,  2.24s/it]\u001b[A\n",
      "Running loss: 3.545279█████████████████▌       | 98/140 [03:39<01:33,  2.23s/it]\u001b[A\n",
      "Running loss: 3.994586█████████████████▋       | 99/140 [03:41<01:31,  2.23s/it]\u001b[A\n",
      "Running loss: 3.728728█████████████████▏      | 100/140 [03:43<01:29,  2.23s/it]\u001b[A\n",
      "Running loss: 3.554498█████████████████▎      | 101/140 [03:45<01:27,  2.24s/it]\u001b[A\n",
      "Running loss: 3.197861█████████████████▍      | 102/140 [03:48<01:25,  2.25s/it]\u001b[A\n",
      "Running loss: 3.723220█████████████████▋      | 103/140 [03:50<01:23,  2.24s/it]\u001b[A\n",
      "Running loss: 3.774796█████████████████▊      | 104/140 [03:52<01:21,  2.25s/it]\u001b[A\n",
      "Running loss: 3.549918██████████████████      | 105/140 [03:54<01:18,  2.25s/it]\u001b[A\n",
      "Running loss: 3.259779██████████████████▏     | 106/140 [03:57<01:16,  2.24s/it]\u001b[A\n",
      "Running loss: 3.533452██████████████████▎     | 107/140 [03:59<01:14,  2.24s/it]\u001b[A\n",
      "Running loss: 3.908329██████████████████▌     | 108/140 [04:01<01:11,  2.24s/it]\u001b[A\n",
      "Running loss: 3.434732██████████████████▋     | 109/140 [04:03<01:09,  2.24s/it]\u001b[A\n",
      "Running loss: 3.688062██████████████████▊     | 110/140 [04:06<01:07,  2.24s/it]\u001b[A\n",
      "Running loss: 3.484069███████████████████     | 111/140 [04:08<01:05,  2.24s/it]\u001b[A\n",
      "Running loss: 3.449487███████████████████▏    | 112/140 [04:10<01:02,  2.25s/it]\u001b[A\n",
      "Running loss: 3.399737███████████████████▎    | 113/140 [04:12<01:00,  2.24s/it]\u001b[A\n",
      "Running loss: 3.445744███████████████████▌    | 114/140 [04:15<00:58,  2.24s/it]\u001b[A\n",
      "Running loss: 3.665137███████████████████▋    | 115/140 [04:17<00:56,  2.24s/it]\u001b[A\n",
      "Running loss: 3.734368███████████████████▉    | 116/140 [04:19<00:53,  2.24s/it]\u001b[A\n",
      "Running loss: 3.131538████████████████████    | 117/140 [04:21<00:51,  2.24s/it]\u001b[A\n",
      "Running loss: 3.604691████████████████████▏   | 118/140 [04:24<00:49,  2.24s/it]\u001b[A\n",
      "Running loss: 3.701093████████████████████▍   | 119/140 [04:26<00:47,  2.25s/it]\u001b[A\n",
      "Running loss: 3.547840████████████████████▌   | 120/140 [04:28<00:44,  2.24s/it]\u001b[A\n",
      "Running loss: 3.381291████████████████████▋   | 121/140 [04:30<00:42,  2.24s/it]\u001b[A\n",
      "Running loss: 3.483679████████████████████▉   | 122/140 [04:33<00:40,  2.24s/it]\u001b[A\n",
      "Running loss: 3.348719█████████████████████   | 123/140 [04:35<00:38,  2.24s/it]\u001b[A\n",
      "Running loss: 3.712368█████████████████████▎  | 124/140 [04:37<00:35,  2.24s/it]\u001b[A\n",
      "Running loss: 3.394533█████████████████████▍  | 125/140 [04:39<00:33,  2.24s/it]\u001b[A\n",
      "Running loss: 3.368202█████████████████████▌  | 126/140 [04:42<00:31,  2.24s/it]\u001b[A\n",
      "Running loss: 3.518374█████████████████████▊  | 127/140 [04:44<00:29,  2.25s/it]\u001b[A\n",
      "Running loss: 3.488538█████████████████████▉  | 128/140 [04:46<00:27,  2.25s/it]\u001b[A\n",
      "Running loss: 3.415743██████████████████████  | 129/140 [04:48<00:24,  2.24s/it]\u001b[A\n",
      "Running loss: 3.526350██████████████████████▎ | 130/140 [04:50<00:22,  2.24s/it]\u001b[A\n",
      "Running loss: 3.589521██████████████████████▍ | 131/140 [04:53<00:20,  2.23s/it]\u001b[A\n",
      "Running loss: 3.658514██████████████████████▋ | 132/140 [04:55<00:17,  2.23s/it]\u001b[A\n",
      "Running loss: 3.577247██████████████████████▊ | 133/140 [04:57<00:15,  2.24s/it]\u001b[A\n",
      "Running loss: 3.532771██████████████████████▉ | 134/140 [04:59<00:13,  2.23s/it]\u001b[A\n",
      "Running loss: 3.610084███████████████████████▏| 135/140 [05:02<00:11,  2.24s/it]\u001b[A\n",
      "Running loss: 3.525103███████████████████████▎| 136/140 [05:04<00:08,  2.24s/it]\u001b[A\n",
      "Running loss: 3.650620███████████████████████▍| 137/140 [05:06<00:06,  2.24s/it]\u001b[A\n",
      "Running loss: 3.625800███████████████████████▋| 138/140 [05:08<00:04,  2.25s/it]\u001b[A\n",
      "Running loss: 3.472145███████████████████████▊| 139/140 [05:11<00:02,  2.24s/it]\u001b[A\n",
      "Epoch 3 of 3:  67%|████████████████████          | 2/3 [11:54<05:57, 357.99s/it]\u001b[A\n",
      "Running loss: 3.590189                                  | 0/140 [00:00<?, ?it/s]\u001b[A\n",
      "Running loss: 3.378741▏                         | 1/140 [00:02<05:18,  2.29s/it]\u001b[A\n",
      "Running loss: 3.488575▎                         | 2/140 [00:04<05:12,  2.27s/it]\u001b[A\n",
      "Running loss: 3.339293▌                         | 3/140 [00:06<05:09,  2.26s/it]\u001b[A\n",
      "Running loss: 3.451899▋                         | 4/140 [00:08<05:04,  2.24s/it]\u001b[A\n",
      "Running loss: 3.434604▉                         | 5/140 [00:11<05:02,  2.24s/it]\u001b[A\n",
      "Running loss: 3.521369█                         | 6/140 [00:13<04:58,  2.23s/it]\u001b[A\n",
      "Running loss: 3.539982█▎                        | 7/140 [00:15<04:56,  2.23s/it]\u001b[A\n",
      "Running loss: 3.527077█▍                        | 8/140 [00:17<04:54,  2.23s/it]\u001b[A\n",
      "Running loss: 3.567811█▋                        | 9/140 [00:20<04:51,  2.23s/it]\u001b[A\n",
      "Running loss: 3.515595█▊                       | 10/140 [00:22<04:49,  2.23s/it]\u001b[A\n",
      "Running loss: 3.405253█▉                       | 11/140 [00:24<04:47,  2.23s/it]\u001b[A\n",
      "Running loss: 3.604817██▏                      | 12/140 [00:26<04:45,  2.23s/it]\u001b[A\n",
      "Running loss: 3.464736██▎                      | 13/140 [00:28<04:43,  2.23s/it]\u001b[A\n",
      "Running loss: 3.357344██▌                      | 14/140 [00:31<04:40,  2.23s/it]\u001b[A\n",
      "Running loss: 3.485229██▋                      | 15/140 [00:33<04:38,  2.23s/it]\u001b[A\n",
      "Running loss: 3.673743██▊                      | 16/140 [00:35<04:36,  2.23s/it]\u001b[A\n",
      "Running loss: 3.435812███                      | 17/140 [00:37<04:34,  2.23s/it]\u001b[A\n",
      "Running loss: 3.461872███▏                     | 18/140 [00:40<04:32,  2.23s/it]\u001b[A\n",
      "Running loss: 3.578633███▍                     | 19/140 [00:42<04:30,  2.23s/it]\u001b[A\n",
      "Running loss: 3.436733███▌                     | 20/140 [00:44<04:28,  2.24s/it]\u001b[A\n",
      "Running loss: 3.377027███▊                     | 21/140 [00:46<04:26,  2.24s/it]\u001b[A\n",
      "Running loss: 3.541213███▉                     | 22/140 [00:49<04:24,  2.24s/it]\u001b[A\n",
      "Running loss: 3.720929████                     | 23/140 [00:51<04:21,  2.24s/it]\u001b[A\n",
      "Running loss: 3.695486████▎                    | 24/140 [00:53<04:19,  2.24s/it]\u001b[A\n",
      "Running loss: 3.551143████▍                    | 25/140 [00:55<04:17,  2.24s/it]\u001b[A\n",
      "Running loss: 3.399402████▋                    | 26/140 [00:58<04:16,  2.25s/it]\u001b[A\n",
      "Running loss: 3.597426████▊                    | 27/140 [01:00<04:13,  2.24s/it]\u001b[A\n",
      "Running loss: 3.420176█████                    | 28/140 [01:02<04:11,  2.24s/it]\u001b[A\n",
      "Running loss: 3.503169█████▏                   | 29/140 [01:04<04:09,  2.25s/it]\u001b[A\n",
      "Running loss: 3.444603█████▎                   | 30/140 [01:07<04:07,  2.25s/it]\u001b[A\n",
      "Running loss: 3.471087█████▌                   | 31/140 [01:09<04:04,  2.25s/it]\u001b[A\n",
      "Running loss: 3.419060█████▋                   | 32/140 [01:11<04:02,  2.24s/it]\u001b[A\n",
      "Running loss: 3.521617█████▉                   | 33/140 [01:13<03:59,  2.23s/it]\u001b[A\n",
      "Running loss: 3.260500██████                   | 34/140 [01:16<03:57,  2.24s/it]\u001b[A\n",
      "Running loss: 3.659716██████▎                  | 35/140 [01:18<03:54,  2.23s/it]\u001b[A\n",
      "Running loss: 3.387660██████▍                  | 36/140 [01:20<03:52,  2.24s/it]\u001b[A\n",
      "Running loss: 3.256281██████▌                  | 37/140 [01:22<03:50,  2.24s/it]\u001b[A\n",
      "Running loss: 3.609772██████▊                  | 38/140 [01:24<03:48,  2.24s/it]\u001b[A\n",
      "Running loss: 3.513665██████▉                  | 39/140 [01:27<03:46,  2.24s/it]\u001b[A\n",
      "Running loss: 3.170968███████▏                 | 40/140 [01:29<03:44,  2.24s/it]\u001b[A\n",
      "Running loss: 2.956830███████▎                 | 41/140 [01:31<03:41,  2.24s/it]\u001b[A\n",
      "Running loss: 3.350492███████▌                 | 42/140 [01:33<03:40,  2.25s/it]\u001b[A\n",
      "Running loss: 3.327173███████▋                 | 43/140 [01:36<03:37,  2.24s/it]\u001b[A\n",
      "Running loss: 3.394066███████▊                 | 44/140 [01:38<03:35,  2.24s/it]\u001b[A\n",
      "Running loss: 3.483202████████                 | 45/140 [01:40<03:33,  2.24s/it]\u001b[A\n",
      "Running loss: 3.367757████████▏                | 46/140 [01:42<03:30,  2.24s/it]\u001b[A\n",
      "Running loss: 3.385216████████▍                | 47/140 [01:45<03:29,  2.25s/it]\u001b[A\n",
      "Running loss: 3.284096████████▌                | 48/140 [01:47<03:27,  2.25s/it]\u001b[A\n",
      "Running loss: 3.451713████████▊                | 49/140 [01:49<03:24,  2.25s/it]\u001b[A\n",
      "Running loss: 3.709692████████▉                | 50/140 [01:51<03:22,  2.25s/it]\u001b[A\n",
      "Running loss: 3.447026█████████                | 51/140 [01:54<03:19,  2.24s/it]\u001b[A\n",
      "Running loss: 3.385666█████████▎               | 52/140 [01:56<03:17,  2.25s/it]\u001b[A\n",
      "Running loss: 3.527394█████████▍               | 53/140 [01:58<03:15,  2.25s/it]\u001b[A\n",
      "Running loss: 3.674114█████████▋               | 54/140 [02:00<03:13,  2.25s/it]\u001b[A\n",
      "Running loss: 3.474139█████████▊               | 55/140 [02:03<03:11,  2.25s/it]\u001b[A\n",
      "Running loss: 3.471629██████████               | 56/140 [02:05<03:09,  2.26s/it]\u001b[A\n",
      "Running loss: 3.562541██████████▏              | 57/140 [02:07<03:06,  2.25s/it]\u001b[A\n",
      "Running loss: 3.417272██████████▎              | 58/140 [02:09<03:04,  2.25s/it]\u001b[A\n",
      "Running loss: 3.382555██████████▌              | 59/140 [02:12<03:01,  2.25s/it]\u001b[A\n",
      "Running loss: 3.617781██████████▋              | 60/140 [02:14<02:59,  2.24s/it]\u001b[A\n",
      "Running loss: 3.789603██████████▉              | 61/140 [02:16<02:56,  2.24s/it]\u001b[A\n",
      "Running loss: 3.335337███████████              | 62/140 [02:18<02:55,  2.25s/it]\u001b[A\n",
      "Running loss: 3.538539███████████▎             | 63/140 [02:21<02:53,  2.25s/it]\u001b[A\n",
      "Running loss: 3.542172███████████▍             | 64/140 [02:23<02:50,  2.25s/it]\u001b[A\n",
      "Running loss: 3.483004███████████▌             | 65/140 [02:25<02:48,  2.24s/it]\u001b[A\n",
      "Running loss: 3.708218███████████▊             | 66/140 [02:27<02:45,  2.24s/it]\u001b[A\n",
      "Running loss: 3.651305███████████▉             | 67/140 [02:30<02:43,  2.24s/it]\u001b[A\n",
      "Running loss: 3.296051████████████▏            | 68/140 [02:32<02:41,  2.24s/it]\u001b[A\n",
      "Running loss: 3.475701████████████▎            | 69/140 [02:34<02:39,  2.24s/it]\u001b[A\n",
      "Running loss: 3.504819████████████▌            | 70/140 [02:36<02:37,  2.25s/it]\u001b[A\n",
      "Running loss: 3.569715████████████▋            | 71/140 [02:39<02:35,  2.25s/it]\u001b[A\n",
      "Running loss: 3.594321████████████▊            | 72/140 [02:41<02:32,  2.25s/it]\u001b[A\n",
      "Running loss: 3.447826█████████████            | 73/140 [02:43<02:30,  2.24s/it]\u001b[A\n",
      "Running loss: 3.421212█████████████▏           | 74/140 [02:45<02:27,  2.24s/it]\u001b[A\n",
      "Running loss: 3.524188█████████████▍           | 75/140 [02:48<02:25,  2.25s/it]\u001b[A\n",
      "Running loss: 3.562499█████████████▌           | 76/140 [02:50<02:23,  2.24s/it]\u001b[A\n",
      "Running loss: 3.531243█████████████▊           | 77/140 [02:52<02:21,  2.24s/it]\u001b[A\n",
      "Running loss: 3.326387█████████████▉           | 78/140 [02:54<02:19,  2.24s/it]\u001b[A\n",
      "Running loss: 3.654380██████████████           | 79/140 [02:57<02:16,  2.25s/it]\u001b[A\n",
      "Running loss: 3.624734██████████████▎          | 80/140 [02:59<02:14,  2.24s/it]\u001b[A\n",
      "Running loss: 3.382635██████████████▍          | 81/140 [03:01<02:12,  2.24s/it]\u001b[A\n",
      "Running loss: 3.566419██████████████▋          | 82/140 [03:03<02:10,  2.24s/it]\u001b[A\n",
      "Running loss: 3.355472██████████████▊          | 83/140 [03:06<02:08,  2.25s/it]\u001b[A\n",
      "Running loss: 3.831302███████████████          | 84/140 [03:08<02:05,  2.25s/it]\u001b[A\n",
      "Running loss: 3.379749███████████████▏         | 85/140 [03:10<02:03,  2.25s/it]\u001b[A\n",
      "Running loss: 3.507135███████████████▎         | 86/140 [03:12<02:01,  2.24s/it]\u001b[A\n",
      "Running loss: 3.416527███████████████▌         | 87/140 [03:15<01:58,  2.24s/it]\u001b[A\n",
      "Running loss: 3.584674███████████████▋         | 88/140 [03:17<01:56,  2.24s/it]\u001b[A\n",
      "Running loss: 3.598376███████████████▉         | 89/140 [03:19<01:54,  2.24s/it]\u001b[A\n",
      "Running loss: 3.268250████████████████         | 90/140 [03:21<01:52,  2.25s/it]\u001b[A\n",
      "Running loss: 3.667467████████████████▎        | 91/140 [03:24<01:50,  2.25s/it]\u001b[A\n",
      "Running loss: 3.556682████████████████▍        | 92/140 [03:26<01:47,  2.25s/it]\u001b[A\n",
      "Running loss: 3.430254████████████████▌        | 93/140 [03:28<01:45,  2.25s/it]\u001b[A\n",
      "Running loss: 3.590678████████████████▊        | 94/140 [03:30<01:43,  2.24s/it]\u001b[A\n",
      "Running loss: 3.353150████████████████▉        | 95/140 [03:32<01:40,  2.24s/it]\u001b[A\n",
      "Running loss: 3.396157█████████████████▏       | 96/140 [03:35<01:38,  2.24s/it]\u001b[A\n",
      "Running loss: 3.508724█████████████████▎       | 97/140 [03:37<01:36,  2.24s/it]\u001b[A\n",
      "Running loss: 3.665558█████████████████▌       | 98/140 [03:39<01:33,  2.24s/it]\u001b[A\n",
      "Running loss: 3.504495█████████████████▋       | 99/140 [03:41<01:31,  2.24s/it]\u001b[A\n",
      "Running loss: 3.552666█████████████████▏      | 100/140 [03:44<01:29,  2.25s/it]\u001b[A\n",
      "Running loss: 3.496067█████████████████▎      | 101/140 [03:46<01:27,  2.24s/it]\u001b[A\n",
      "Running loss: 3.452233█████████████████▍      | 102/140 [03:48<01:25,  2.25s/it]\u001b[A\n",
      "Running loss: 3.396677█████████████████▋      | 103/140 [03:50<01:22,  2.24s/it]\u001b[A\n",
      "Running loss: 3.711905█████████████████▊      | 104/140 [03:53<01:20,  2.24s/it]\u001b[A\n",
      "Running loss: 3.751783██████████████████      | 105/140 [03:55<01:18,  2.25s/it]\u001b[A\n",
      "Running loss: 3.377739██████████████████▏     | 106/140 [03:57<01:16,  2.25s/it]\u001b[A\n",
      "Running loss: 3.590637██████████████████▎     | 107/140 [03:59<01:14,  2.25s/it]\u001b[A\n",
      "Running loss: 3.435033██████████████████▌     | 108/140 [04:02<01:11,  2.25s/it]\u001b[A\n",
      "Running loss: 3.405082██████████████████▋     | 109/140 [04:04<01:09,  2.25s/it]\u001b[A\n",
      "Running loss: 3.513076██████████████████▊     | 110/140 [04:06<01:07,  2.24s/it]\u001b[A\n",
      "Running loss: 3.414172███████████████████     | 111/140 [04:08<01:04,  2.23s/it]\u001b[A\n",
      "Running loss: 3.293722███████████████████▏    | 112/140 [04:11<01:02,  2.23s/it]\u001b[A\n",
      "Running loss: 3.401749███████████████████▎    | 113/140 [04:13<01:00,  2.23s/it]\u001b[A\n",
      "Running loss: 3.273641███████████████████▌    | 114/140 [04:15<00:57,  2.23s/it]\u001b[A\n",
      "Running loss: 3.463834███████████████████▋    | 115/140 [04:17<00:55,  2.23s/it]\u001b[A\n",
      "Running loss: 3.530280███████████████████▉    | 116/140 [04:19<00:53,  2.24s/it]\u001b[A\n",
      "Running loss: 3.444155████████████████████    | 117/140 [04:22<00:51,  2.24s/it]\u001b[A\n",
      "Running loss: 3.491438████████████████████▏   | 118/140 [04:24<00:49,  2.25s/it]\u001b[A\n",
      "Running loss: 3.510019████████████████████▍   | 119/140 [04:26<00:47,  2.24s/it]\u001b[A\n",
      "Running loss: 3.699613████████████████████▌   | 120/140 [04:28<00:44,  2.24s/it]\u001b[A\n",
      "Running loss: 3.694506████████████████████▋   | 121/140 [04:31<00:42,  2.24s/it]\u001b[A\n",
      "Running loss: 3.652409████████████████████▉   | 122/140 [04:33<00:40,  2.24s/it]\u001b[A\n",
      "Running loss: 3.508032█████████████████████   | 123/140 [04:35<00:38,  2.24s/it]\u001b[A\n",
      "Running loss: 3.593691█████████████████████▎  | 124/140 [04:37<00:35,  2.24s/it]\u001b[A\n",
      "Running loss: 3.473479█████████████████████▍  | 125/140 [04:40<00:33,  2.24s/it]\u001b[A\n",
      "Running loss: 3.345879█████████████████████▌  | 126/140 [04:42<00:31,  2.24s/it]\u001b[A\n",
      "Running loss: 3.621488█████████████████████▊  | 127/140 [04:44<00:29,  2.24s/it]\u001b[A\n",
      "Running loss: 3.331292█████████████████████▉  | 128/140 [04:46<00:26,  2.25s/it]\u001b[A\n",
      "Running loss: 3.476863██████████████████████  | 129/140 [04:49<00:24,  2.24s/it]\u001b[A\n",
      "Running loss: 3.472527██████████████████████▎ | 130/140 [04:51<00:22,  2.25s/it]\u001b[A\n",
      "Running loss: 3.510317██████████████████████▍ | 131/140 [04:53<00:20,  2.24s/it]\u001b[A\n",
      "Running loss: 3.439092██████████████████████▋ | 132/140 [04:55<00:17,  2.24s/it]\u001b[A\n",
      "Running loss: 3.437844██████████████████████▊ | 133/140 [04:58<00:15,  2.24s/it]\u001b[A\n",
      "Running loss: 3.972570██████████████████████▉ | 134/140 [05:00<00:13,  2.24s/it]\u001b[A\n",
      "Running loss: 3.611276███████████████████████▏| 135/140 [05:02<00:11,  2.23s/it]\u001b[A\n",
      "Running loss: 3.470676███████████████████████▎| 136/140 [05:04<00:08,  2.24s/it]\u001b[A\n",
      "Running loss: 3.500160███████████████████████▍| 137/140 [05:07<00:06,  2.25s/it]\u001b[A\n",
      "Running loss: 3.453482███████████████████████▋| 138/140 [05:09<00:04,  2.24s/it]\u001b[A\n",
      "Running loss: 3.378061███████████████████████▊| 139/140 [05:11<00:02,  2.24s/it]\u001b[A\n",
      "Epoch 3 of 3: 100%|██████████████████████████████| 3/3 [17:59<00:00, 360.06s/it]\u001b[A\n",
      "INFO:simpletransformers.language_modeling.language_modeling_model: Training of gpt2 model complete. Saved to outputs/fine-tuned/.\n",
      "INFO:simpletransformers.language_modeling.language_modeling_utils: Creating features from dataset file at cache_dir/\n",
      "  0%|                                                   | 0/500 [00:00<?, ?it/s]WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1089 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1744 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1335 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (1137 > 1024). Running this sequence through the model will result in indexing errors\n",
      "WARNING:transformers.tokenization_utils:Token indices sequence length is longer than the specified maximum sequence length for this model (2278 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|████████████████████████████████████████| 500/500 [00:00<00:00, 805.23it/s]\n",
      "100%|█████████████████████████████████████| 110/110 [00:00<00:00, 313433.04it/s]\n",
      "INFO:simpletransformers.language_modeling.language_modeling_utils: Saving features into cached file cache_dir/gpt2_cached_lm_508_test.txt\n",
      "Running Evaluation: 100%|███████████████████████| 14/14 [00:11<00:00,  1.31it/s]\n",
      "INFO:simpletransformers.language_modeling.language_modeling_model:{'eval_loss': 3.3431155341012135, 'perplexity': tensor(28.3072)}\n"
     ]
    }
   ],
   "source": [
    "!python fine_tune.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.modeling_utils:Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "=============================================================================\n",
      "[\"Despite the recent successes of deep learning, such models are still far from some human abilities like learning from few examples, reasoning and explaining decisions. In this paper, we focus on organ annotation in medical images and we introduce a reasoning framework that is based on learning fuzzy relations on a small dataset for generating explanations. At least, that is what we mean. I would like to thank the many people who support our work and to suggest new problems that could lead to new discoveries. We also want to ask you what your feedback is. We can also find and fix bug reports by posting on this website, but I really don't want to give out your own suggestions. Any feedback is welcome!    urlLink     urlLink    I've noticed that many of the questions in my question are completely different from the ones I was asking. What is the most important difference?    urlLink    urlLink  I'm a neuroscientist. I work with\"]\n",
      "=============================================================================\n",
      "WARNING:transformers.modeling_utils:Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "=============================================================================\n",
      "['There is a growing interest and literature on intrinsic motivations and open-ended learning in both cognitive robotics and machine learning on one side, and in psychology and neuroscience on the other. This paper aims to review some relevant contributions from the two literature threads and to draw links between them.  (a) A recent paper by Istvan Sverre called The Personality Indicators of the Brain in Everyday Life, presented at the 19th International Conference on Computer Science, focused on the neurobiology of the brain. The paper argued that, even in nonhuman animals such as humans, people often show the signs of a general decrease in self-awareness in response to external stimuli. However, in reality, human cognition consists of neural networks composed primarily of two-way connections: one between brain regions involved in visual and auditory perception, and the other between those brain areas involved in mood, cognition, and behavior. Thus, the increased use of computer imagery to measure human personality patterns might lead to an increase in the']\n",
      "=============================================================================\n",
      "WARNING:transformers.modeling_utils:Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "=============================================================================\n",
      "['Recent success of pre-trained language models (LMs) has spurred widespread interest in the language capabilities that they possess. However, efforts to understand whether LM representations are useful for symbolic reasoning tasks have been limited and scattered. The aim of this article is to summarize the current knowledge of LMs regarding the properties of mathematical features for ML-like structures, and how they can be used to train language models for symbolic reasoning tasks. The aim of the article is to determine whether LM models are useful for symbolic reasoning tasks and, if so, how. The aim of the article is to provide a theoretical overview of the properties of symbols and their interaction with non-symbolic language models. These features should be used in programming languages where LM models are used. We will then attempt to determine whether or not there exists a need for more than four mathematical features in symbolic reasoning tasks to be learnt for symbolic reasoning tasks. In particular, we will try to identify features that are needed to learn a symbolic language model']\n",
      "=============================================================================\n",
      "WARNING:transformers.modeling_utils:Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "=============================================================================\n",
      "[\"Many theories, based on neuroscientific and psychological empirical evidence and on computational concepts, have been elaborated to explain the emergence of consciousness in the central nervous system. These theories propose key fundamental mechanisms to explain consciousness, but they only partially connect such mechanisms to the possible functional and adaptive role of consciousness. The fundamental principles of neurobiology of consciousness are not, however, consistent. (The following excerpts show basic aspects of neurobiology of consciousness that are not supported by neuroscientific methods.) 1. Brain activity, by way of the structure of the brain's neural circuits 1. The neural network to which our brain and brain connect 2. The neurosurgeon's role 3. Immediate and immediate effects 4. Neurophysiology and effects 5. Immediate effects; neurophysiology and effects 6. Immediate effects and subsequent neurophysiological alterations 7. Brain reorganization of the brain, to cope with the neurophysiological changes 8. Immediate effects 5/6 7/8 7/8 7/8\"]\n",
      "=============================================================================\n",
      "WARNING:transformers.modeling_utils:Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "=============================================================================\n",
      "[\"I failed the first quarter of a class in middle school, so I made a fake report card. I did this every quarter that year. I forgot that they mail home the end-of-year cards, and my mom got it before I could intercept with my fake. She was PISSED—at the school for their error. It's okay, because you know that I don't have to think about them. I have no trouble paying the bills. It's an odd habit. And now I have to deal with that! And they're like 'That's so annoying.' But I'm not supposed to, 'cause they have to get my money back! They're like, 'Your money must be back.'  I feel so bad for them because they've been so unfair. My mom's not a mother! I can't stand the fact that I'm just another stupid person. I mean, no one's gonna want a teacher anymore, and I can't stand the fact\"]\n",
      "=============================================================================\n",
      "WARNING:transformers.modeling_utils:Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "=============================================================================\n",
      "['The firm has reportedly agreed to pay over $1 billion to acquire Zoox. This will give Amazon control of the 1000-person startup; however, Zoox is expected to operate as an independent subsidiary. The deal was first reported by CNBC.\\n\\nIn its current form, the company is said to be at least $300 million in revenue each year - about half what it was at the start of the year. Its share price has dropped over the past week, though it may still rise if Amazon becomes even more aggressive in buying online and selling stock. The company has also been a player in the world of gambling. In 2002, it acquired a $5 million share for $5 million, the largest ever gain on the group. But the deal was never made public. After investors began buying the company in 2002, it took months for an independent manager to secure more stock, and until that point, the stock market was mostly driven by investors, not stockholders. The company was a major']\n",
      "=============================================================================\n",
      "WARNING:transformers.modeling_utils:Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "=============================================================================\n",
      "[\"Google is rolling out a new Verified Calls feature for the Phone app to let a user know that they are receiving a call from a business and the reason behind it. The company explains that the Verified Calls feature is meant to avoid scams and fraud calls by verifying the identity of the business that is calling, display their reason for calling as well as their business logo. All these details will be shown on the incoming call screen so a user knows that they are receiving a genuine call from a business and its purpose without picking the phone up. Google also confirms that a user's private data is not shared with the businesses at any time during this process. That's the crux of the whole thing. Google believes that all its business users will receive a personalized call back from a company within six hours. According to the company, its users will receive the first four digits on their confirmation messages and the last four digits will be automatically passed along to users throughout the day. When an incoming call comes in,\"]\n",
      "=============================================================================\n",
      "WARNING:transformers.modeling_utils:Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "=============================================================================\n",
      "[\"According to a new report, Qualcomm could be readying a new Snapdragon 8cx+ chipset for Windows 10 PCs. As is typically the case with Qualcomm 'plus' chips, it offers a higher clock speed. Of the 8c (not including the quad-core i7-3670K) models we tested, the 8c is the first to run Linux on board, and the new chip has quite similar performance to its more expensive sibling (which will include dual GPU cores) and is able to provide up to 16W power consumption for at least 4 hours of usage. As we've previously reported, the 8c's 32GB version also has 4GB of RAM. In a sense, it's all about price. As the report stated, there's a $20 price tag attached to the 8c chip, but the price seems to go up in the short term, as the new chip seems to be one of the more affordable 10-bit chips on the market. That said\"]\n",
      "=============================================================================\n",
      "WARNING:transformers.modeling_utils:Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "=============================================================================\n",
      "[\"This is the first time unicorns have been found living in a remote, previously unexplored valley, in Chile. The discovery comes just weeks after the Chilean government reported that the herd's DNA from an earlier discovery led it to believe that the unicorn had been living in a remote valley for several months.The discovery of the unicorn is the most important discovery of its kind ever made in the region.The first unicorns in the Andes Mountains were found living in the Andes Mountains in December of 2012.The first unicorn came to the attention of the Chilean government when they discovered a herd of unicorns living in a remote, previously unexplored valley.The researchers began studying the unicorn using a technique called PCR, which is a method that is used to probe genetic material from animal specimens.The researchers decided to study the unicorn using PCR and found that it was closely related to other creatures such as chimpanzees and elephants. In a statement published in the journal Science, the researchers said that the unicorn's DNA contains a unique genetic signature that suggests it used to live in a remote valley for millions of years before it was discovered. The findings in this study and others are important as they contribute to understanding how the unicorn evolved and how it has evolved over the last 2,000 years.Scientists believe that the unicorn has been living in a remote valley for several months.The researchers believe that the unicorn has been living in a remote valley for several months.The unicorn's DNA contains a unique genetic signature that suggests it used to live in a remote valley for millions of years before it was discovered.Scientists believe that the unicorn has been living in a remote valley for several months.The researchers believe that the unicorn has been living in a remote valley for several months.The researchers believe that the unicorn has been living in a remote valley for several months.However, there could be problems.The researchers believe that certain genetic markers could be missing on the unicorn and might not be able to tell the animal's age.They also believe that different regions of the unicorn could be missing.\"]\n",
      "=============================================================================\n",
      "WARNING:transformers.modeling_utils:Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "=============================================================================\n",
      "[\"This is the first time unicorns have been found living in a remote, previously unexplored valley, in Chile. The discovery comes just weeks after the Chilean government reported that the herd's DNA from an earlier discovery led it to believe that the unicorn had been living in a remote valley for several months.The discovery of the unicorn is the most important discovery of its kind ever made in the region.The first unicorns in the Andes Mountains were found living in the Andes Mountains in December of 2012.The first unicorn came to the attention of the Chilean government when they discovered a herd of unicorns living in a remote, previously unexplored valley.The researchers began studying the unicorn using a technique called PCR, which is a method that is used to probe genetic material from animal specimens.The researchers decided to study the unicorn using PCR and found that it was closely related to other creatures such as chimpanzees and elephants. In a statement published in the journal Science, the researchers said that the unicorn's DNA contains a unique genetic signature that suggests it used to live in a remote valley for millions of years before it was discovered. The findings in this study and others are important as they contribute to understanding how the unicorn evolved and how it has evolved over the last 2,000 years.Scientists believe that the unicorn has been living in a remote valley for several months.The researchers believe that the unicorn has been living in a remote valley for several months.The unicorn's DNA contains a unique genetic signature that suggests it used to live in a remote valley for millions of years before it was discovered.Scientists believe that the unicorn has been living in a remote valley for several months.The researchers believe that the unicorn has been living in a remote valley for several months.The researchers believe that the unicorn has been living in a remote valley for several months.However, there could be problems.The researchers believe that certain genetic markers could be missing on the unicorn and might not be able to tell the animal's age.They also believe that different regions of the unicorn could be missing.\"]\n",
      "=============================================================================\n"
     ]
    }
   ],
   "source": [
    "!python generate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
