{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/batch/tasks/shared/LS_root/mounts/clusters/gptcomputeinstance/code/Users/atherfawaz/simpletransformers/examples/language_generation\n"
     ]
    }
   ],
   "source": [
    "#!pip install simpletransformers\n",
    "#%cd /anaconda/envs/azureml_py36/lib/python3.6/site-packages/simpletransformers/language_generation\n",
    "%cd /mnt/batch/tasks/shared/LS_root/mounts/clusters/gptcomputeinstance/code/Users/atherfawaz/simpletransformers/examples/language_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting language_generation_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile language_generation_model.py\n",
    "# %load language_generation_model.py\n",
    "# %load language_generation_model.py\n",
    "# %load language_generation_model.py\n",
    "# %load language_generation_model.py\n",
    "import argparse\n",
    "import logging\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from simpletransformers.config.global_args import global_args\n",
    "from simpletransformers.config.model_args import LanguageGenerationArgs\n",
    "from simpletransformers.language_generation.language_generation_utils import PREPROCESSING_FUNCTIONS\n",
    "from transformers import (\n",
    "    CTRLConfig,\n",
    "    CTRLLMHeadModel,\n",
    "    CTRLTokenizer,\n",
    "    GPT2Config,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    OpenAIGPTConfig,\n",
    "    OpenAIGPTLMHeadModel,\n",
    "    OpenAIGPTTokenizer,\n",
    "    TransfoXLConfig,\n",
    "    TransfoXLLMHeadModel,\n",
    "    TransfoXLTokenizer,\n",
    "    XLMConfig,\n",
    "    XLMTokenizer,\n",
    "    XLMWithLMHeadModel,\n",
    "    XLNetConfig,\n",
    "    XLNetLMHeadModel,\n",
    "    XLNetTokenizer,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MAX_LENGTH = int(10000)  # Hardcoded max length to avoid infinite loop\n",
    "\n",
    "\n",
    "class LanguageGenerationModel:\n",
    "    def __init__(\n",
    "        self, model_type, model_name, args=None, use_cuda=True, cuda_device=-1, **kwargs,\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Initializes a LanguageGenerationModel model.\n",
    "\n",
    "        Args:\n",
    "            model_type: The type of model (gpt2, ctrl, openai-gpt, xlnet, transfo-xl, xlm)\n",
    "            model_name: Default Transformer model name or path to a directory containing Transformer model file (pytorch_nodel.bin).\n",
    "            args (optional): Default args will be used if this parameter is not provided. If provided, it should be a dict containing the args that should be changed in the default args.\n",
    "            use_cuda (optional): Use GPU if available. Setting to False will force model to use CPU only.\n",
    "            cuda_device (optional): Specific GPU that should be used. Will use the first available GPU by default.\n",
    "            **kwargs (optional): For providing proxies, force_download, resume_download, cache_dir and other options specific to the 'from_pretrained' implementation where this will be supplied.\n",
    "        \"\"\"  # noqa: ignore flake8\"\n",
    "\n",
    "        MODEL_CLASSES = {\n",
    "            \"gpt2\": (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),\n",
    "            \"ctrl\": (CTRLConfig, CTRLLMHeadModel, CTRLTokenizer),\n",
    "            \"openai-gpt\": (OpenAIGPTConfig, OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n",
    "            \"xlnet\": (XLNetConfig, XLNetLMHeadModel, XLNetTokenizer),\n",
    "            \"transfo-xl\": (TransfoXLConfig, TransfoXLLMHeadModel, TransfoXLTokenizer),\n",
    "            \"xlm\": (XLMConfig, XLMWithLMHeadModel, XLMTokenizer),\n",
    "        }\n",
    "\n",
    "        self.args = self._load_model_args(model_name)\n",
    "\n",
    "        if isinstance(args, dict):\n",
    "            self.args.update_from_dict(args)\n",
    "        elif isinstance(args, LanguageGenerationArgs):\n",
    "            self.args = args\n",
    "\n",
    "        if \"sweep_config\" in kwargs:\n",
    "            sweep_config = kwargs.pop(\"sweep_config\")\n",
    "            sweep_values = {key: value[\"value\"] for key, value in sweep_config.as_dict().items() if key != \"_wandb\"}\n",
    "            self.args.update_from_dict(sweep_values)\n",
    "\n",
    "        if self.args.manual_seed:\n",
    "            random.seed(self.args.manual_seed)\n",
    "            np.random.seed(self.args.manual_seed)\n",
    "            torch.manual_seed(self.args.manual_seed)\n",
    "            if self.args.n_gpu > 0:\n",
    "                torch.cuda.manual_seed_all(self.args.manual_seed)\n",
    "\n",
    "        if use_cuda:\n",
    "            if torch.cuda.is_available():\n",
    "                if cuda_device == -1:\n",
    "                    self.device = torch.device(\"cuda\")\n",
    "                else:\n",
    "                    self.device = torch.device(f\"cuda:{cuda_device}\")\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"'use_cuda' set to True when cuda is unavailable.\"\n",
    "                    \"Make sure CUDA is available or set `use_cuda=False`.\"\n",
    "                )\n",
    "        else:\n",
    "            self.device = \"cpu\"\n",
    "\n",
    "        self.args.model_name = model_name\n",
    "        self.args.model_type = model_type\n",
    "\n",
    "        config_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]\n",
    "\n",
    "        if self.args.tokenizer_name:\n",
    "            self.tokenizer = tokenizer_class.from_pretrained(self.args.tokenizer_name, cache_dir=self.args.cache_dir)\n",
    "        else:\n",
    "            self.tokenizer = tokenizer_class.from_pretrained(model_name, cache_dir=self.args.cache_dir, **kwargs)\n",
    "            self.args.tokenizer_name = model_name\n",
    "\n",
    "        if self.args.config_name:\n",
    "            self.config = config_class.from_pretrained(self.args.config_name, cache_dir=self.args.cache_dir)\n",
    "        else:\n",
    "            self.config = config_class.from_pretrained(model_name, cache_dir=self.args.cache_dir, **kwargs)\n",
    "\n",
    "        self.model = model_class.from_pretrained(\n",
    "            model_name, config=self.config, cache_dir=self.args.cache_dir, **kwargs,\n",
    "        )\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def generate(self, prompt=None, args=None, verbose=True):\n",
    "        \n",
    "        print('prev max_length: ', self.args.max_length)\n",
    "        #self.args.max_length = 500\n",
    "        \"\"\"\n",
    "        Generate text using a LanguageGenerationModel\n",
    "\n",
    "        Args:\n",
    "            prompt (optional): A prompt text for the model. If given, will override args.prompt\n",
    "            args (optional): Optional changes to the args dict of the model. Any changes made will persist for the model.\n",
    "            verbose (optional): If verbose, generated text will be logged to the console.\n",
    "        Returns:\n",
    "            generated_sequences: Sequences of text generated by the model.\n",
    "        \"\"\"  # noqa: ignore flake8\"\n",
    "\n",
    "        model = self.model\n",
    "        tokenizer = self.tokenizer\n",
    "        device = self.device\n",
    "\n",
    "        if args:\n",
    "            self.args.update_from_dict(args)\n",
    "\n",
    "        if prompt:\n",
    "            self.args.prompt = prompt\n",
    "        elif not self.args.prompt:\n",
    "            self.args.prompt = input(\"Model prompt >>> \")\n",
    "\n",
    "        prompt_text = self.args.prompt\n",
    "        args = self.args\n",
    "\n",
    "        # Different models need different input formatting and/or extra arguments\n",
    "        requires_preprocessing = args.model_type in PREPROCESSING_FUNCTIONS.keys()\n",
    "        if requires_preprocessing:\n",
    "            prepare_input = PREPROCESSING_FUNCTIONS.get(args.model_type)\n",
    "            preprocessed_prompt_text = prepare_input(args, model, tokenizer, prompt_text)\n",
    "            encoded_prompt = tokenizer.encode(\n",
    "                preprocessed_prompt_text,\n",
    "                add_special_tokens=False,\n",
    "                return_tensors=\"pt\",\n",
    "                add_space_before_punct_symbol=True,\n",
    "            )\n",
    "        else:\n",
    "            encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "        encoded_prompt = encoded_prompt.to(device)\n",
    "\n",
    "        output_sequences = model.generate(\n",
    "            input_ids=encoded_prompt,\n",
    "            max_length=args.max_length + len(encoded_prompt[0]),\n",
    "            temperature=args.temperature,\n",
    "            top_k=args.top_k,\n",
    "            top_p=args.top_p,\n",
    "            repetition_penalty=args.repetition_penalty,\n",
    "            do_sample=args.do_sample,\n",
    "            num_return_sequences=args.num_return_sequences,\n",
    "        )\n",
    "\n",
    "        # Remove the batch dimension when returning multiple sequences\n",
    "        if len(output_sequences.shape) > 2:\n",
    "            output_sequences.squeeze_()\n",
    "\n",
    "        generated_sequences = []\n",
    "\n",
    "        for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "            if verbose:\n",
    "                logger.info(\"=== GENERATED SEQUENCE {} ===\".format(generated_sequence_idx + 1))\n",
    "            generated_sequence = generated_sequence.tolist()\n",
    "\n",
    "            # Decode text\n",
    "            text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
    "\n",
    "            # Remove all text after the stop token\n",
    "            text = text[: text.find(args.stop_token) if args.stop_token else None]\n",
    "\n",
    "            # Add the prompt at the beginning of the sequence. Remove the excess text that was used for pre-processing\n",
    "            total_sequence = (\n",
    "                prompt_text + text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True)) :]\n",
    "            )\n",
    "\n",
    "            generated_sequences.append(total_sequence)\n",
    "            if verbose:\n",
    "                logger.info(total_sequence)\n",
    "\n",
    "        return generated_sequences\n",
    "\n",
    "    def _save_model_args(self, output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        self.args.save(output_dir)\n",
    "\n",
    "    def _load_model_args(self, input_dir):\n",
    "        args = LanguageGenerationArgs()\n",
    "        args.load(input_dir)\n",
    "        return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /mnt/batch/tasks/shared/LS_root/mounts/clusters/gptcomputeinstance/code/users/atherfawaz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVIDIA/apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" --user ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -v --no-cache-dir ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile setup.sh\n",
    "\n",
    "pip install -v --no-cache-dir ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd simpletransformers/examples/language_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%cd apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "!nvcc --version "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "!pip install nvcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.4+cu100 torchvision==0.5.0+cu100 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python setup.py install --cuda_ext --cpp_ext "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd simpletransformers/examples/language_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install pytorch torchvision cudatoolkit=10.0 -c pytorch -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda update -n base -c defaults conda -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python data_prep.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:simpletransformers.language_modeling.language_modeling_utils: Creating features from dataset file at cache_dir/\n",
      "100%|█████████████████████████████████████| 4988/4988 [00:01<00:00, 3425.45it/s]\n",
      "100%|███████████████████████████████████| 2128/2128 [00:00<00:00, 107781.32it/s]\n",
      "INFO:simpletransformers.language_modeling.language_modeling_utils: Saving features into cached file cache_dir/gpt2_cached_lm_508_train.txt\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ImportError('/anaconda/envs/azureml_py36/lib/python3.6/site-packages/apex-0.1-py3.6-linux-x86_64.egg/amp_C.cpython-36m-x86_64-linux-gnu.so: undefined symbol: THPVariableClass',)\n",
      "INFO:simpletransformers.language_modeling.language_modeling_model: Training started\n",
      "Epoch 1 of 3:   0%|                                       | 0/3 [00:00<?, ?it/s]\n",
      "Running Epoch 0:   0%|                                  | 0/266 [00:00<?, ?it/s]\u001b[A/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Running loss: 3.735030Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:114: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "\n",
      "Running loss: 3.974058                          | 1/266 [00:04<17:41,  4.01s/it]\u001b[AGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "\n",
      "Running loss: 3.779020▏                         | 2/266 [00:06<15:05,  3.43s/it]\u001b[A\n",
      "Running loss: 3.807291▎                         | 3/266 [00:08<13:22,  3.05s/it]\u001b[A\n",
      "Running loss: 3.759928▍                         | 4/266 [00:10<12:11,  2.79s/it]\u001b[A\n",
      "Running loss: 3.876168▍                         | 5/266 [00:12<11:20,  2.61s/it]\u001b[A\n",
      "Running loss: 4.165684▌                         | 6/266 [00:14<10:45,  2.48s/it]\u001b[A\n",
      "Running loss: 3.601948▋                         | 7/266 [00:16<10:19,  2.39s/it]\u001b[A\n",
      "Running loss: 3.843935▊                         | 8/266 [00:19<10:01,  2.33s/it]\u001b[A\n",
      "Running loss: 3.952800▉                         | 9/266 [00:21<09:47,  2.29s/it]\u001b[A\n",
      "Running loss: 3.931996▉                        | 10/266 [00:23<09:38,  2.26s/it]\u001b[A\n",
      "Running loss: 3.785382█                        | 11/266 [00:25<09:31,  2.24s/it]\u001b[A\n",
      "Running loss: 3.885955█▏                       | 12/266 [00:27<09:27,  2.23s/it]\u001b[A\n",
      "Running loss: 3.825396█▏                       | 13/266 [00:30<09:22,  2.22s/it]\u001b[A\n",
      "Running loss: 3.949907█▎                       | 14/266 [00:32<09:20,  2.22s/it]\u001b[A\n",
      "Running loss: 3.749659█▍                       | 15/266 [00:34<09:16,  2.22s/it]\u001b[A\n",
      "Running loss: 3.675301█▌                       | 16/266 [00:36<09:15,  2.22s/it]\u001b[A\n",
      "Running loss: 3.917412█▌                       | 17/266 [00:39<09:12,  2.22s/it]\u001b[A\n",
      "Running loss: 3.856084█▋                       | 18/266 [00:41<09:10,  2.22s/it]\u001b[A\n",
      "Running loss: 4.333186█▊                       | 19/266 [00:43<09:09,  2.22s/it]\u001b[A\n",
      "Running loss: 3.528879█▉                       | 20/266 [00:45<09:06,  2.22s/it]\u001b[A\n",
      "Running loss: 4.043637█▉                       | 21/266 [00:47<09:03,  2.22s/it]\u001b[A\n",
      "Running loss: 3.830342██                       | 22/266 [00:50<09:01,  2.22s/it]\u001b[A\n",
      "Running loss: 3.766794██▏                      | 23/266 [00:52<09:00,  2.23s/it]\u001b[A\n",
      "Running loss: 3.995117██▎                      | 24/266 [00:54<08:58,  2.22s/it]\u001b[A\n",
      "Running loss: 3.770901██▎                      | 25/266 [00:56<08:56,  2.23s/it]\u001b[A\n",
      "Running loss: 3.768372██▍                      | 26/266 [00:59<08:53,  2.22s/it]\u001b[A\n",
      "Running loss: 3.673095██▌                      | 27/266 [01:01<08:50,  2.22s/it]\u001b[A\n",
      "Running loss: 3.913324██▋                      | 28/266 [01:03<08:48,  2.22s/it]\u001b[A\n",
      "Running loss: 3.681140██▋                      | 29/266 [01:05<08:46,  2.22s/it]\u001b[A\n",
      "Running loss: 3.843028██▊                      | 30/266 [01:07<08:43,  2.22s/it]\u001b[A\n",
      "Running loss: 3.798298██▉                      | 31/266 [01:10<08:42,  2.22s/it]\u001b[A\n",
      "Running loss: 3.811910███                      | 32/266 [01:12<08:40,  2.22s/it]\u001b[A\n",
      "Running loss: 3.844255███                      | 33/266 [01:14<08:37,  2.22s/it]\u001b[A\n",
      "Running loss: 3.704954███▏                     | 34/266 [01:16<08:34,  2.22s/it]\u001b[A\n",
      "Running loss: 3.882397███▎                     | 35/266 [01:19<08:33,  2.22s/it]\u001b[A\n",
      "Running loss: 3.810890███▍                     | 36/266 [01:21<08:31,  2.23s/it]\u001b[A\n",
      "Running loss: 3.830198███▍                     | 37/266 [01:23<08:29,  2.23s/it]\u001b[A\n",
      "Running loss: 4.206388███▌                     | 38/266 [01:25<08:27,  2.22s/it]\u001b[A\n",
      "Running loss: 3.590865███▋                     | 39/266 [01:27<08:24,  2.22s/it]\u001b[A\n",
      "Running loss: 3.764462███▊                     | 40/266 [01:30<08:22,  2.22s/it]\u001b[A\n",
      "Running loss: 3.492646███▊                     | 41/266 [01:32<08:20,  2.22s/it]\u001b[A\n",
      "Running loss: 3.717502███▉                     | 42/266 [01:34<08:17,  2.22s/it]\u001b[A\n",
      "Running loss: 3.819626████                     | 43/266 [01:36<08:15,  2.22s/it]\u001b[A\n",
      "Running loss: 4.028052████▏                    | 44/266 [01:39<08:13,  2.22s/it]\u001b[A\n",
      "Running loss: 3.694632████▏                    | 45/266 [01:41<08:11,  2.22s/it]\u001b[A\n",
      "Running loss: 3.616097████▎                    | 46/266 [01:43<08:08,  2.22s/it]\u001b[A\n",
      "Running loss: 3.715456████▍                    | 47/266 [01:45<08:06,  2.22s/it]\u001b[A\n",
      "Running loss: 3.622123████▌                    | 48/266 [01:47<08:04,  2.22s/it]\u001b[A\n",
      "Running loss: 3.824577████▌                    | 49/266 [01:50<08:01,  2.22s/it]\u001b[A/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "\n",
      "Running loss: 3.838652████▋                    | 50/266 [01:52<08:00,  2.23s/it]\u001b[A\n",
      "Running loss: 3.712995████▊                    | 51/266 [01:54<07:57,  2.22s/it]\u001b[A\n",
      "Running loss: 3.828487████▉                    | 52/266 [01:56<07:57,  2.23s/it]\u001b[A\n",
      "Running loss: 3.931159████▉                    | 53/266 [01:59<07:54,  2.23s/it]\u001b[A\n",
      "Running loss: 3.895260█████                    | 54/266 [02:01<07:51,  2.22s/it]\u001b[A\n",
      "Running loss: 3.908242█████▏                   | 55/266 [02:03<07:48,  2.22s/it]\u001b[A\n",
      "Running loss: 3.412286█████▎                   | 56/266 [02:05<07:45,  2.22s/it]\u001b[A\n",
      "Running loss: 3.804215█████▎                   | 57/266 [02:07<07:43,  2.22s/it]\u001b[A\n",
      "Running loss: 3.840338█████▍                   | 58/266 [02:10<07:40,  2.21s/it]\u001b[A\n",
      "Running loss: 3.679984█████▌                   | 59/266 [02:12<07:38,  2.22s/it]\u001b[A\n",
      "Running loss: 3.747784█████▋                   | 60/266 [02:14<07:37,  2.22s/it]\u001b[A\n",
      "Running loss: 3.713766█████▋                   | 61/266 [02:16<07:36,  2.22s/it]\u001b[A\n",
      "Running loss: 3.783565█████▊                   | 62/266 [02:19<07:33,  2.22s/it]\u001b[A\n",
      "Running loss: 3.715257█████▉                   | 63/266 [02:21<07:31,  2.22s/it]\u001b[A\n",
      "Running loss: 3.704710██████                   | 64/266 [02:23<07:28,  2.22s/it]\u001b[A\n",
      "Running loss: 3.649339██████                   | 65/266 [02:25<07:28,  2.23s/it]\u001b[A\n",
      "Running loss: 3.745199██████▏                  | 66/266 [02:27<07:26,  2.23s/it]\u001b[A\n",
      "Running loss: 3.717814██████▎                  | 67/266 [02:30<07:22,  2.22s/it]\u001b[A\n",
      "Running loss: 3.667292██████▍                  | 68/266 [02:32<07:20,  2.23s/it]\u001b[A\n",
      "Running loss: 3.739363██████▍                  | 69/266 [02:34<07:18,  2.22s/it]\u001b[A\n",
      "Running loss: 3.997947██████▌                  | 70/266 [02:36<07:14,  2.22s/it]\u001b[A\n",
      "Running loss: 3.941644██████▋                  | 71/266 [02:39<07:12,  2.22s/it]\u001b[A\n",
      "Running loss: 3.687329██████▊                  | 72/266 [02:41<07:09,  2.21s/it]\u001b[A\n",
      "Running loss: 3.532285██████▊                  | 73/266 [02:43<07:08,  2.22s/it]\u001b[A\n",
      "Running loss: 3.703306██████▉                  | 74/266 [02:45<07:07,  2.23s/it]\u001b[A\n",
      "Running loss: 3.844624███████                  | 75/266 [02:47<07:04,  2.22s/it]\u001b[A\n",
      "Running loss: 3.645805███████▏                 | 76/266 [02:50<07:02,  2.22s/it]\u001b[A\n",
      "Running loss: 3.593932███████▏                 | 77/266 [02:52<07:00,  2.22s/it]\u001b[A\n",
      "Running loss: 3.964932███████▎                 | 78/266 [02:54<06:57,  2.22s/it]\u001b[A\n",
      "Running loss: 3.936858███████▍                 | 79/266 [02:56<06:55,  2.22s/it]\u001b[A\n",
      "Running loss: 3.809457███████▌                 | 80/266 [02:59<06:52,  2.22s/it]\u001b[A\n",
      "Running loss: 3.722295███████▌                 | 81/266 [03:01<06:50,  2.22s/it]\u001b[A\n",
      "Running loss: 3.836044███████▋                 | 82/266 [03:03<06:47,  2.22s/it]\u001b[A\n",
      "Running loss: 4.018109███████▊                 | 83/266 [03:05<06:46,  2.22s/it]\u001b[A\n",
      "Running loss: 3.570590███████▉                 | 84/266 [03:07<06:44,  2.22s/it]\u001b[A\n",
      "Running loss: 3.785853███████▉                 | 85/266 [03:10<06:42,  2.22s/it]\u001b[A\n",
      "Running loss: 3.654570████████                 | 86/266 [03:12<06:40,  2.22s/it]\u001b[A\n",
      "Running loss: 3.735486████████▏                | 87/266 [03:14<06:37,  2.22s/it]\u001b[A\n",
      "Running loss: 3.525521████████▎                | 88/266 [03:16<06:35,  2.22s/it]\u001b[A\n",
      "Running loss: 3.819670████████▎                | 89/266 [03:19<06:33,  2.22s/it]\u001b[A\n",
      "Running loss: 3.761728████████▍                | 90/266 [03:21<06:31,  2.22s/it]\u001b[A\n",
      "Running loss: 3.812469████████▌                | 91/266 [03:23<06:29,  2.23s/it]\u001b[A\n",
      "Running loss: 4.021914████████▋                | 92/266 [03:25<06:27,  2.22s/it]\u001b[A\n",
      "Running loss: 3.631828████████▋                | 93/266 [03:27<06:24,  2.23s/it]\u001b[A\n",
      "Running loss: 3.658231████████▊                | 94/266 [03:30<06:22,  2.23s/it]\u001b[A\n",
      "Running loss: 3.623386████████▉                | 95/266 [03:32<06:20,  2.22s/it]\u001b[A\n",
      "Running loss: 3.721222█████████                | 96/266 [03:34<06:17,  2.22s/it]\u001b[A\n",
      "Running loss: 3.543328█████████                | 97/266 [03:36<06:15,  2.22s/it]\u001b[A\n",
      "Running loss: 3.485461█████████▏               | 98/266 [03:39<06:12,  2.22s/it]\u001b[A\n",
      "Running loss: 3.548349█████████▎               | 99/266 [03:41<06:09,  2.21s/it]\u001b[A\n",
      "Running loss: 3.855589█████████               | 100/266 [03:43<06:08,  2.22s/it]\u001b[A\n",
      "Running loss: 3.695530█████████               | 101/266 [03:45<06:07,  2.22s/it]\u001b[A\n",
      "Running loss: 3.689074█████████▏              | 102/266 [03:47<06:06,  2.23s/it]\u001b[A\n",
      "Running loss: 3.831737█████████▎              | 103/266 [03:50<06:03,  2.23s/it]\u001b[A\n",
      "Running loss: 3.579012█████████▍              | 104/266 [03:52<06:01,  2.23s/it]\u001b[A\n",
      "Running loss: 3.673149█████████▍              | 105/266 [03:54<05:59,  2.23s/it]\u001b[A\n",
      "Running loss: 3.744884█████████▌              | 106/266 [03:56<05:57,  2.24s/it]\u001b[A\n",
      "Running loss: 3.497264█████████▋              | 107/266 [03:59<05:55,  2.24s/it]\u001b[A\n",
      "Running loss: 3.587008█████████▋              | 108/266 [04:01<05:53,  2.24s/it]\u001b[A\n",
      "Running loss: 3.796457█████████▊              | 109/266 [04:03<05:50,  2.23s/it]\u001b[A\n",
      "Running loss: 4.093187█████████▉              | 110/266 [04:05<05:47,  2.23s/it]\u001b[A\n",
      "Running loss: 3.714155██████████              | 111/266 [04:08<05:45,  2.23s/it]\u001b[A\n",
      "Running loss: 3.689768██████████              | 112/266 [04:10<05:42,  2.23s/it]\u001b[A\n",
      "Running loss: 3.779044██████████▏             | 113/266 [04:12<05:40,  2.22s/it]\u001b[A\n",
      "Running loss: 3.629447██████████▎             | 114/266 [04:14<05:38,  2.23s/it]\u001b[A\n",
      "Running loss: 3.587021██████████▍             | 115/266 [04:16<05:36,  2.23s/it]\u001b[A\n",
      "Running loss: 3.490814██████████▍             | 116/266 [04:19<05:33,  2.22s/it]\u001b[A\n",
      "Running loss: 3.590646██████████▌             | 117/266 [04:21<05:31,  2.22s/it]\u001b[A\n",
      "Running loss: 3.794487██████████▋             | 118/266 [04:23<05:29,  2.23s/it]\u001b[A\n",
      "Running loss: 3.590076██████████▋             | 119/266 [04:25<05:29,  2.24s/it]\u001b[A\n",
      "Running loss: 3.607621██████████▊             | 120/266 [04:28<05:26,  2.23s/it]\u001b[A\n",
      "Running loss: 3.550750██████████▉             | 121/266 [04:30<05:22,  2.23s/it]\u001b[A\n",
      "Running loss: 3.619270███████████             | 122/266 [04:32<05:21,  2.23s/it]\u001b[A\n",
      "Running loss: 3.622996███████████             | 123/266 [04:34<05:18,  2.23s/it]\u001b[A\n",
      "Running loss: 3.505743███████████▏            | 124/266 [04:37<05:16,  2.23s/it]\u001b[A\n",
      "Running loss: 3.585987███████████▎            | 125/266 [04:39<05:15,  2.23s/it]\u001b[A\n",
      "Running loss: 3.542192███████████▎            | 126/266 [04:41<05:13,  2.24s/it]\u001b[A\n",
      "Running loss: 3.799016███████████▍            | 127/266 [04:43<05:11,  2.24s/it]\u001b[A\n",
      "Running loss: 3.753953███████████▌            | 128/266 [04:45<05:08,  2.24s/it]\u001b[A\n",
      "Running loss: 3.822138███████████▋            | 129/266 [04:48<05:05,  2.23s/it]\u001b[A\n",
      "Running loss: 3.590474███████████▋            | 130/266 [04:50<05:02,  2.23s/it]\u001b[A\n",
      "Running loss: 3.817744███████████▊            | 131/266 [04:52<05:00,  2.23s/it]\u001b[A\n",
      "Running loss: 3.839690███████████▉            | 132/266 [04:54<04:57,  2.22s/it]\u001b[A\n",
      "Running loss: 3.781272████████████            | 133/266 [04:57<04:55,  2.22s/it]\u001b[A\n",
      "Running loss: 3.632399████████████            | 134/266 [04:59<04:53,  2.23s/it]\u001b[A\n",
      "Running loss: 3.854373████████████▏           | 135/266 [05:01<04:52,  2.24s/it]\u001b[A\n",
      "Running loss: 3.600351████████████▎           | 136/266 [05:03<04:50,  2.24s/it]\u001b[A\n",
      "Running loss: 3.684272████████████▎           | 137/266 [05:06<04:47,  2.23s/it]\u001b[A\n",
      "Running loss: 3.538865████████████▍           | 138/266 [05:08<04:45,  2.23s/it]\u001b[A\n",
      "Running loss: 3.567755████████████▌           | 139/266 [05:10<04:42,  2.23s/it]\u001b[A\n",
      "Running loss: 3.620465████████████▋           | 140/266 [05:12<04:41,  2.23s/it]\u001b[A\n",
      "Running loss: 3.485618████████████▋           | 141/266 [05:14<04:38,  2.23s/it]\u001b[A\n",
      "Running loss: 3.929668████████████▊           | 142/266 [05:17<04:36,  2.23s/it]\u001b[A\n",
      "Running loss: 3.845532████████████▉           | 143/266 [05:19<04:34,  2.23s/it]\u001b[A\n",
      "Running loss: 3.567850████████████▉           | 144/266 [05:21<04:31,  2.22s/it]\u001b[A\n",
      "Running loss: 3.661157█████████████           | 145/266 [05:23<04:29,  2.23s/it]\u001b[A\n",
      "Running loss: 3.678085█████████████▏          | 146/266 [05:26<04:26,  2.22s/it]\u001b[A\n",
      "Running loss: 3.637348█████████████▎          | 147/266 [05:28<04:24,  2.22s/it]\u001b[A\n",
      "Running loss: 3.117918█████████████▎          | 148/266 [05:30<04:22,  2.22s/it]\u001b[A\n",
      "Running loss: 3.722892█████████████▍          | 149/266 [05:32<04:20,  2.22s/it]\u001b[A\n",
      "Running loss: 3.890591█████████████▌          | 150/266 [05:34<04:17,  2.22s/it]\u001b[A\n",
      "Running loss: 3.626623█████████████▌          | 151/266 [05:37<04:14,  2.22s/it]\u001b[A\n",
      "Running loss: 3.821570█████████████▋          | 152/266 [05:39<04:12,  2.22s/it]\u001b[A\n",
      "Running loss: 3.629842█████████████▊          | 153/266 [05:41<04:11,  2.22s/it]\u001b[A\n",
      "Running loss: 3.576339█████████████▉          | 154/266 [05:43<04:08,  2.22s/it]\u001b[A\n",
      "Running loss: 3.612467█████████████▉          | 155/266 [05:46<04:06,  2.22s/it]\u001b[A\n",
      "Running loss: 3.647740██████████████          | 156/266 [05:48<04:04,  2.22s/it]\u001b[A\n",
      "Running loss: 3.541242██████████████▏         | 157/266 [05:50<04:02,  2.22s/it]\u001b[A\n",
      "Running loss: 3.571814██████████████▎         | 158/266 [05:52<03:59,  2.22s/it]\u001b[A\n",
      "Running loss: 3.723376██████████████▎         | 159/266 [05:54<03:57,  2.22s/it]\u001b[A\n",
      "Running loss: 3.582448██████████████▍         | 160/266 [05:57<03:55,  2.22s/it]\u001b[A\n",
      "Running loss: 3.667809██████████████▌         | 161/266 [05:59<03:53,  2.22s/it]\u001b[A\n",
      "Running loss: 3.528695██████████████▌         | 162/266 [06:01<03:51,  2.23s/it]\u001b[A\n",
      "Running loss: 3.891052██████████████▋         | 163/266 [06:03<03:49,  2.23s/it]\u001b[A\n",
      "Running loss: 3.468117██████████████▊         | 164/266 [06:06<03:46,  2.22s/it]\u001b[A\n",
      "Running loss: 4.123591██████████████▉         | 165/266 [06:08<03:44,  2.22s/it]\u001b[A\n",
      "Running loss: 3.592676██████████████▉         | 166/266 [06:10<03:42,  2.22s/it]\u001b[A\n",
      "Running loss: 3.704806███████████████         | 167/266 [06:12<03:40,  2.23s/it]\u001b[A\n",
      "Running loss: 3.599655███████████████▏        | 168/266 [06:14<03:37,  2.22s/it]\u001b[A\n",
      "Running loss: 3.700731███████████████▏        | 169/266 [06:17<03:35,  2.22s/it]\u001b[A\n",
      "Running loss: 3.680424███████████████▎        | 170/266 [06:19<03:34,  2.23s/it]\u001b[A\n",
      "Running loss: 3.595166███████████████▍        | 171/266 [06:21<03:31,  2.23s/it]\u001b[A\n",
      "Running loss: 3.575129███████████████▌        | 172/266 [06:23<03:30,  2.24s/it]\u001b[A\n",
      "Running loss: 3.646393███████████████▌        | 173/266 [06:26<03:28,  2.24s/it]\u001b[A\n",
      "Running loss: 3.695966███████████████▋        | 174/266 [06:28<03:26,  2.24s/it]\u001b[A\n",
      "Running loss: 3.622839███████████████▊        | 175/266 [06:30<03:23,  2.24s/it]\u001b[A\n",
      "Running loss: 3.729928███████████████▉        | 176/266 [06:32<03:20,  2.23s/it]\u001b[A\n",
      "Running loss: 3.701985███████████████▉        | 177/266 [06:35<03:18,  2.23s/it]\u001b[A\n",
      "Running loss: 3.693956████████████████        | 178/266 [06:37<03:16,  2.23s/it]\u001b[A\n",
      "Running loss: 3.624830████████████████▏       | 179/266 [06:39<03:14,  2.23s/it]\u001b[A\n",
      "Running loss: 3.609833████████████████▏       | 180/266 [06:41<03:11,  2.23s/it]\u001b[A\n",
      "Running loss: 3.657470████████████████▎       | 181/266 [06:43<03:09,  2.23s/it]\u001b[A\n",
      "Running loss: 3.665190████████████████▍       | 182/266 [06:46<03:07,  2.23s/it]\u001b[A\n",
      "Running loss: 3.532802████████████████▌       | 183/266 [06:48<03:04,  2.23s/it]\u001b[A\n",
      "Running loss: 3.577411████████████████▌       | 184/266 [06:50<03:02,  2.23s/it]\u001b[A\n",
      "Running loss: 3.723848████████████████▋       | 185/266 [06:52<03:00,  2.23s/it]\u001b[A\n",
      "Running loss: 3.509254████████████████▊       | 186/266 [06:55<02:58,  2.23s/it]\u001b[A\n",
      "Running loss: 3.423260████████████████▊       | 187/266 [06:57<02:56,  2.23s/it]\u001b[A\n",
      "Running loss: 3.501628████████████████▉       | 188/266 [06:59<02:53,  2.23s/it]\u001b[A\n",
      "Running loss: 3.640746█████████████████       | 189/266 [07:01<02:51,  2.23s/it]\u001b[A\n",
      "Running loss: 3.598957█████████████████▏      | 190/266 [07:04<02:49,  2.23s/it]\u001b[A\n",
      "Running loss: 3.496739█████████████████▏      | 191/266 [07:06<02:47,  2.23s/it]\u001b[A\n",
      "Running loss: 3.555752█████████████████▎      | 192/266 [07:08<02:45,  2.23s/it]\u001b[A\n",
      "Running loss: 3.607055█████████████████▍      | 193/266 [07:10<02:42,  2.23s/it]\u001b[A\n",
      "Running loss: 3.522925█████████████████▌      | 194/266 [07:12<02:40,  2.23s/it]\u001b[A\n",
      "Running loss: 3.451785█████████████████▌      | 195/266 [07:15<02:38,  2.24s/it]\u001b[A\n",
      "Running loss: 3.491699█████████████████▋      | 196/266 [07:17<02:36,  2.24s/it]\u001b[A\n",
      "Running loss: 3.783482█████████████████▊      | 197/266 [07:19<02:33,  2.23s/it]\u001b[A\n",
      "Running loss: 3.658036█████████████████▊      | 198/266 [07:21<02:31,  2.23s/it]\u001b[A\n",
      "Running loss: 3.403680█████████████████▉      | 199/266 [07:24<02:29,  2.23s/it]\u001b[A\n",
      "Running loss: 3.877425██████████████████      | 200/266 [07:26<02:27,  2.23s/it]\u001b[A\n",
      "Running loss: 3.382557██████████████████▏     | 201/266 [07:28<02:25,  2.23s/it]\u001b[A\n",
      "Running loss: 3.446130██████████████████▏     | 202/266 [07:30<02:22,  2.23s/it]\u001b[A\n",
      "Running loss: 3.679947██████████████████▎     | 203/266 [07:33<02:20,  2.23s/it]\u001b[A\n",
      "Running loss: 3.875331██████████████████▍     | 204/266 [07:35<02:18,  2.23s/it]\u001b[A\n",
      "Running loss: 3.789699██████████████████▍     | 205/266 [07:37<02:15,  2.23s/it]\u001b[A\n",
      "Running loss: 3.743429██████████████████▌     | 206/266 [07:39<02:13,  2.23s/it]\u001b[A\n",
      "Running loss: 3.619246██████████████████▋     | 207/266 [07:41<02:11,  2.23s/it]\u001b[A\n",
      "Running loss: 3.598402██████████████████▊     | 208/266 [07:44<02:09,  2.23s/it]\u001b[A\n",
      "Running loss: 3.638540██████████████████▊     | 209/266 [07:46<02:07,  2.23s/it]\u001b[A\n",
      "Running loss: 3.645796██████████████████▉     | 210/266 [07:48<02:04,  2.23s/it]\u001b[A\n",
      "Running loss: 3.848477███████████████████     | 211/266 [07:50<02:02,  2.23s/it]\u001b[A\n",
      "Running loss: 3.985286███████████████████▏    | 212/266 [07:53<02:00,  2.23s/it]\u001b[A\n",
      "Running loss: 3.565941███████████████████▏    | 213/266 [07:55<01:58,  2.24s/it]\u001b[A\n",
      "Running loss: 3.437361███████████████████▎    | 214/266 [07:57<01:56,  2.24s/it]\u001b[A\n",
      "Running loss: 3.619516███████████████████▍    | 215/266 [07:59<01:53,  2.23s/it]\u001b[A\n",
      "Running loss: 3.687162███████████████████▍    | 216/266 [08:02<01:52,  2.24s/it]\u001b[A\n",
      "Running loss: 3.417405███████████████████▌    | 217/266 [08:04<01:49,  2.24s/it]\u001b[A\n",
      "Running loss: 3.753346███████████████████▋    | 218/266 [08:06<01:47,  2.24s/it]\u001b[A\n",
      "Running loss: 3.725912███████████████████▊    | 219/266 [08:08<01:44,  2.23s/it]\u001b[A\n",
      "Running loss: 3.723899███████████████████▊    | 220/266 [08:11<01:42,  2.23s/it]\u001b[A\n",
      "Running loss: 3.850507███████████████████▉    | 221/266 [08:13<01:40,  2.23s/it]\u001b[A\n",
      "Running loss: 3.594414████████████████████    | 222/266 [08:15<01:38,  2.23s/it]\u001b[A\n",
      "Running loss: 3.548125████████████████████    | 223/266 [08:17<01:35,  2.23s/it]\u001b[A\n",
      "Running loss: 3.705425████████████████████▏   | 224/266 [08:19<01:33,  2.23s/it]\u001b[A\n",
      "Running loss: 3.817931████████████████████▎   | 225/266 [08:22<01:31,  2.23s/it]\u001b[A\n",
      "Running loss: 3.990132████████████████████▍   | 226/266 [08:24<01:29,  2.23s/it]\u001b[A\n",
      "Running loss: 3.473597████████████████████▍   | 227/266 [08:26<01:26,  2.23s/it]\u001b[A\n",
      "Running loss: 3.536374████████████████████▌   | 228/266 [08:28<01:24,  2.24s/it]\u001b[A\n",
      "Running loss: 4.140744████████████████████▋   | 229/266 [08:31<01:23,  2.25s/it]\u001b[A\n",
      "Running loss: 3.615038████████████████████▊   | 230/266 [08:33<01:20,  2.24s/it]\u001b[A\n",
      "Running loss: 3.664507████████████████████▊   | 231/266 [08:35<01:18,  2.24s/it]\u001b[A\n",
      "Running loss: 3.528837████████████████████▉   | 232/266 [08:37<01:16,  2.24s/it]\u001b[A\n",
      "Running loss: 3.700702█████████████████████   | 233/266 [08:40<01:13,  2.23s/it]\u001b[A\n",
      "Running loss: 3.851393█████████████████████   | 234/266 [08:42<01:11,  2.23s/it]\u001b[A\n",
      "Running loss: 3.638880█████████████████████▏  | 235/266 [08:44<01:09,  2.23s/it]\u001b[A\n",
      "Running loss: 3.666238█████████████████████▎  | 236/266 [08:46<01:06,  2.23s/it]\u001b[A\n",
      "Running loss: 3.696438█████████████████████▍  | 237/266 [08:49<01:04,  2.24s/it]\u001b[A\n",
      "Running loss: 3.538098█████████████████████▍  | 238/266 [08:51<01:02,  2.23s/it]\u001b[A\n",
      "Running loss: 3.470221█████████████████████▌  | 239/266 [08:53<01:00,  2.23s/it]\u001b[A\n",
      "Running loss: 3.737953█████████████████████▋  | 240/266 [08:55<00:58,  2.24s/it]\u001b[A\n",
      "Running loss: 3.564426█████████████████████▋  | 241/266 [08:57<00:55,  2.24s/it]\u001b[A\n",
      "Running loss: 3.488452█████████████████████▊  | 242/266 [09:00<00:53,  2.24s/it]\u001b[A\n",
      "Running loss: 3.883528█████████████████████▉  | 243/266 [09:02<00:51,  2.24s/it]\u001b[A\n",
      "Running loss: 3.572126██████████████████████  | 244/266 [09:04<00:49,  2.23s/it]\u001b[A\n",
      "Running loss: 3.603982██████████████████████  | 245/266 [09:06<00:47,  2.24s/it]\u001b[A\n",
      "Running loss: 3.666142██████████████████████▏ | 246/266 [09:09<00:44,  2.24s/it]\u001b[A\n",
      "Running loss: 3.570994██████████████████████▎ | 247/266 [09:11<00:42,  2.23s/it]\u001b[A\n",
      "Running loss: 3.671267██████████████████████▍ | 248/266 [09:13<00:40,  2.23s/it]\u001b[A\n",
      "Running loss: 3.773538██████████████████████▍ | 249/266 [09:15<00:37,  2.23s/it]\u001b[A\n",
      "Running loss: 3.747431██████████████████████▌ | 250/266 [09:18<00:35,  2.24s/it]\u001b[A\n",
      "Running loss: 3.683595██████████████████████▋ | 251/266 [09:20<00:33,  2.23s/it]\u001b[A\n",
      "Running loss: 3.682187██████████████████████▋ | 252/266 [09:22<00:31,  2.23s/it]\u001b[A\n",
      "Running loss: 3.645418██████████████████████▊ | 253/266 [09:24<00:29,  2.24s/it]\u001b[A\n",
      "Running loss: 3.502420██████████████████████▉ | 254/266 [09:27<00:26,  2.24s/it]\u001b[A\n",
      "Running loss: 3.594914███████████████████████ | 255/266 [09:29<00:24,  2.24s/it]\u001b[A\n",
      "Running loss: 3.574634███████████████████████ | 256/266 [09:31<00:22,  2.24s/it]\u001b[A\n",
      "Running loss: 3.709693███████████████████████▏| 257/266 [09:33<00:20,  2.24s/it]\u001b[A\n",
      "Running loss: 3.403679███████████████████████▎| 258/266 [09:36<00:17,  2.24s/it]\u001b[A\n",
      "Running loss: 3.506872███████████████████████▎| 259/266 [09:38<00:15,  2.24s/it]\u001b[A\n",
      "Running loss: 3.594316███████████████████████▍| 260/266 [09:40<00:13,  2.24s/it]\u001b[A\n",
      "Running loss: 3.714724███████████████████████▌| 261/266 [09:42<00:11,  2.25s/it]\u001b[A\n",
      "Running loss: 3.489866███████████████████████▋| 262/266 [09:45<00:08,  2.25s/it]\u001b[A\n",
      "Running loss: 3.593341███████████████████████▋| 263/266 [09:47<00:06,  2.24s/it]\u001b[A\n",
      "Running loss: 3.618376███████████████████████▊| 264/266 [09:49<00:04,  2.24s/it]\u001b[A\n",
      "Running loss: 3.734603███████████████████████▉| 265/266 [09:51<00:02,  2.24s/it]\u001b[A\n",
      "Running Epoch 0: 100%|████████████████████████| 266/266 [09:53<00:00,  2.24s/it]\u001b[A/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "Epoch 2 of 3:  33%|██████████                    | 1/3 [10:15<20:31, 615.98s/it]\n",
      "Running loss: 3.695791                                  | 0/266 [00:00<?, ?it/s]\u001b[A\n",
      "Running loss: 3.580420                          | 1/266 [00:02<10:08,  2.29s/it]\u001b[A\n",
      "Running loss: 3.693349▏                         | 2/266 [00:04<10:00,  2.28s/it]\u001b[A\n",
      "Running loss: 3.655658▎                         | 3/266 [00:06<09:54,  2.26s/it]\u001b[A\n",
      "Running loss: 3.635150▍                         | 4/266 [00:08<09:47,  2.24s/it]\u001b[A\n",
      "Running loss: 3.648464▍                         | 5/266 [00:11<09:44,  2.24s/it]\u001b[A\n",
      "Running loss: 3.542908▌                         | 6/266 [00:13<09:41,  2.24s/it]\u001b[A\n",
      "Running loss: 3.546773▋                         | 7/266 [00:15<09:39,  2.24s/it]\u001b[A\n",
      "Running loss: 3.544055▊                         | 8/266 [00:17<09:37,  2.24s/it]\u001b[A\n",
      "Running loss: 3.881921▉                         | 9/266 [00:20<09:34,  2.24s/it]\u001b[A\n",
      "Running loss: 3.490679▉                        | 10/266 [00:22<09:33,  2.24s/it]\u001b[A\n",
      "Running loss: 3.494614█                        | 11/266 [00:24<09:30,  2.24s/it]\u001b[A\n",
      "Running loss: 3.634472█▏                       | 12/266 [00:26<09:29,  2.24s/it]\u001b[A\n",
      "Running loss: 3.924908█▏                       | 13/266 [00:29<09:27,  2.24s/it]\u001b[A\n",
      "Running loss: 3.618347█▎                       | 14/266 [00:31<09:23,  2.24s/it]\u001b[A\n",
      "Running loss: 3.744075█▍                       | 15/266 [00:33<09:22,  2.24s/it]\u001b[A\n",
      "Running loss: 3.785969█▌                       | 16/266 [00:35<09:21,  2.24s/it]\u001b[A\n",
      "Running loss: 3.551844█▌                       | 17/266 [00:38<09:19,  2.25s/it]\u001b[A\n",
      "Running loss: 3.499098█▋                       | 18/266 [00:40<09:16,  2.24s/it]\u001b[A\n",
      "Running loss: 3.688269█▊                       | 19/266 [00:42<09:14,  2.24s/it]\u001b[A\n",
      "Running loss: 3.677307█▉                       | 20/266 [00:44<09:11,  2.24s/it]\u001b[A\n",
      "Running loss: 3.565685█▉                       | 21/266 [00:47<09:07,  2.24s/it]\u001b[A\n",
      "Running loss: 3.484159██                       | 22/266 [00:49<09:06,  2.24s/it]\u001b[A\n",
      "Running loss: 3.886733██▏                      | 23/266 [00:51<09:03,  2.24s/it]\u001b[A\n",
      "Running loss: 3.360617██▎                      | 24/266 [00:53<09:01,  2.24s/it]\u001b[A\n",
      "Running loss: 3.659687██▎                      | 25/266 [00:55<09:00,  2.24s/it]\u001b[A\n",
      "Running loss: 3.705010██▍                      | 26/266 [00:58<08:58,  2.25s/it]\u001b[A\n",
      "Running loss: 3.580890██▌                      | 27/266 [01:00<08:55,  2.24s/it]\u001b[A\n",
      "Running loss: 3.656250██▋                      | 28/266 [01:02<08:53,  2.24s/it]\u001b[A\n",
      "Running loss: 3.715970██▋                      | 29/266 [01:04<08:51,  2.24s/it]\u001b[A\n",
      "Running loss: 3.781077██▊                      | 30/266 [01:07<08:48,  2.24s/it]\u001b[A\n",
      "Running loss: 3.518100██▉                      | 31/266 [01:09<08:47,  2.24s/it]\u001b[A\n",
      "Running loss: 3.450433███                      | 32/266 [01:11<08:45,  2.25s/it]\u001b[A\n",
      "Running loss: 3.276808███                      | 33/266 [01:13<08:43,  2.25s/it]\u001b[A\n",
      "Running loss: 3.693534███▏                     | 34/266 [01:16<08:42,  2.25s/it]\u001b[A\n",
      "Running loss: 3.732605███▎                     | 35/266 [01:18<08:38,  2.24s/it]\u001b[A\n",
      "Running loss: 3.384031███▍                     | 36/266 [01:20<08:35,  2.24s/it]\u001b[A\n",
      "Running loss: 3.629944███▍                     | 37/266 [01:22<08:32,  2.24s/it]\u001b[A\n",
      "Running loss: 3.722716███▌                     | 38/266 [01:25<08:29,  2.23s/it]\u001b[A\n",
      "Running loss: 3.732807███▋                     | 39/266 [01:27<08:27,  2.24s/it]\u001b[A\n",
      "Running loss: 3.425229███▊                     | 40/266 [01:29<08:24,  2.23s/it]\u001b[A\n",
      "Running loss: 3.659204███▊                     | 41/266 [01:31<08:23,  2.24s/it]\u001b[A\n",
      "Running loss: 3.562916███▉                     | 42/266 [01:34<08:21,  2.24s/it]\u001b[A\n",
      "Running loss: 3.638361████                     | 43/266 [01:36<08:21,  2.25s/it]\u001b[A\n",
      "Running loss: 3.727641████▏                    | 44/266 [01:38<08:19,  2.25s/it]\u001b[A\n",
      "Running loss: 3.696101████▏                    | 45/266 [01:40<08:16,  2.25s/it]\u001b[A\n",
      "Running loss: 3.458844████▎                    | 46/266 [01:43<08:13,  2.25s/it]\u001b[A\n",
      "Running loss: 3.742731████▍                    | 47/266 [01:45<08:10,  2.24s/it]\u001b[A\n",
      "Running loss: 3.642209████▌                    | 48/266 [01:47<08:08,  2.24s/it]\u001b[A\n",
      "Running loss: 3.463751████▌                    | 49/266 [01:49<08:06,  2.24s/it]\u001b[A\n",
      "Running loss: 3.492847████▋                    | 50/266 [01:52<08:04,  2.24s/it]\u001b[A\n",
      "Running loss: 3.467507████▊                    | 51/266 [01:54<08:03,  2.25s/it]\u001b[A\n",
      "Running loss: 3.658761████▉                    | 52/266 [01:56<08:01,  2.25s/it]\u001b[A\n",
      "Running loss: 3.688808████▉                    | 53/266 [01:58<07:58,  2.25s/it]\u001b[A\n",
      "Running loss: 3.493495█████                    | 54/266 [02:01<07:54,  2.24s/it]\u001b[A\n",
      "Running loss: 3.699118█████▏                   | 55/266 [02:03<07:52,  2.24s/it]\u001b[A\n",
      "Running loss: 3.835497█████▎                   | 56/266 [02:05<07:49,  2.24s/it]\u001b[A\n",
      "Running loss: 3.518030█████▎                   | 57/266 [02:07<07:47,  2.24s/it]\u001b[A\n",
      "Running loss: 3.514184█████▍                   | 58/266 [02:09<07:44,  2.23s/it]\u001b[A\n",
      "Running loss: 3.448586█████▌                   | 59/266 [02:12<07:42,  2.23s/it]\u001b[A\n",
      "Running loss: 3.507009█████▋                   | 60/266 [02:14<07:40,  2.24s/it]\u001b[A\n",
      "Running loss: 3.396154█████▋                   | 61/266 [02:16<07:38,  2.24s/it]\u001b[A\n",
      "Running loss: 3.847819█████▊                   | 62/266 [02:18<07:36,  2.24s/it]\u001b[A\n",
      "Running loss: 3.483091█████▉                   | 63/266 [02:21<07:34,  2.24s/it]\u001b[A\n",
      "Running loss: 3.637978██████                   | 64/266 [02:23<07:31,  2.24s/it]\u001b[A\n",
      "Running loss: 3.738050██████                   | 65/266 [02:25<07:30,  2.24s/it]\u001b[A\n",
      "Running loss: 3.530460██████▏                  | 66/266 [02:27<07:27,  2.24s/it]\u001b[A\n",
      "Running loss: 3.691822██████▎                  | 67/266 [02:30<07:25,  2.24s/it]\u001b[A\n",
      "Running loss: 3.463495██████▍                  | 68/266 [02:32<07:22,  2.24s/it]\u001b[A\n",
      "Running loss: 3.545319██████▍                  | 69/266 [02:34<07:20,  2.23s/it]\u001b[A\n",
      "Running loss: 3.389048██████▌                  | 70/266 [02:36<07:17,  2.23s/it]\u001b[A\n",
      "Running loss: 3.496031██████▋                  | 71/266 [02:39<07:15,  2.23s/it]\u001b[A\n",
      "Running loss: 3.643867██████▊                  | 72/266 [02:41<07:14,  2.24s/it]\u001b[A\n",
      "Running loss: 3.583524██████▊                  | 73/266 [02:43<07:12,  2.24s/it]\u001b[A\n",
      "Running loss: 3.494754██████▉                  | 74/266 [02:45<07:09,  2.24s/it]\u001b[A\n",
      "Running loss: 3.618231███████                  | 75/266 [02:47<07:06,  2.23s/it]\u001b[A\n",
      "Running loss: 3.752483███████▏                 | 76/266 [02:50<07:04,  2.23s/it]\u001b[A\n",
      "Running loss: 3.642878███████▏                 | 77/266 [02:52<07:02,  2.23s/it]\u001b[A\n",
      "Running loss: 3.540315███████▎                 | 78/266 [02:54<07:00,  2.24s/it]\u001b[A\n",
      "Running loss: 3.552079███████▍                 | 79/266 [02:56<06:58,  2.24s/it]\u001b[A\n",
      "Running loss: 3.727146███████▌                 | 80/266 [02:59<06:54,  2.23s/it]\u001b[A\n",
      "Running loss: 4.008892███████▌                 | 81/266 [03:01<06:53,  2.23s/it]\u001b[A\n",
      "Running loss: 3.752101███████▋                 | 82/266 [03:03<06:51,  2.24s/it]\u001b[A\n",
      "Running loss: 3.537976███████▊                 | 83/266 [03:05<06:47,  2.23s/it]\u001b[A\n",
      "Running loss: 3.827020███████▉                 | 84/266 [03:08<06:46,  2.23s/it]\u001b[A\n",
      "Running loss: 3.732284███████▉                 | 85/266 [03:10<06:44,  2.24s/it]\u001b[A\n",
      "Running loss: 3.842821████████                 | 86/266 [03:12<06:43,  2.24s/it]\u001b[A\n",
      "Running loss: 3.876125████████▏                | 87/266 [03:14<06:40,  2.24s/it]\u001b[A\n",
      "Running loss: 3.514453████████▎                | 88/266 [03:17<06:39,  2.24s/it]\u001b[A\n",
      "Running loss: 3.803151████████▎                | 89/266 [03:19<06:36,  2.24s/it]\u001b[A\n",
      "Running loss: 3.540866████████▍                | 90/266 [03:21<06:34,  2.24s/it]\u001b[A\n",
      "Running loss: 4.032852████████▌                | 91/266 [03:23<06:32,  2.24s/it]\u001b[A\n",
      "Running loss: 3.568640████████▋                | 92/266 [03:26<06:29,  2.24s/it]\u001b[A\n",
      "Running loss: 3.631726████████▋                | 93/266 [03:28<06:27,  2.24s/it]\u001b[A\n",
      "Running loss: 3.262319████████▊                | 94/266 [03:30<06:26,  2.24s/it]\u001b[A\n",
      "Running loss: 3.770614████████▉                | 95/266 [03:32<06:23,  2.24s/it]\u001b[A\n",
      "Running loss: 3.465902█████████                | 96/266 [03:35<06:21,  2.25s/it]\u001b[A\n",
      "Running loss: 3.515581█████████                | 97/266 [03:37<06:19,  2.25s/it]\u001b[A\n",
      "Running loss: 3.616115█████████▏               | 98/266 [03:39<06:16,  2.24s/it]\u001b[A\n",
      "Running loss: 3.534223█████████▎               | 99/266 [03:41<06:15,  2.25s/it]\u001b[A\n",
      "Running loss: 3.790573█████████               | 100/266 [03:43<06:12,  2.25s/it]\u001b[A\n",
      "Running loss: 3.404735█████████               | 101/266 [03:46<06:11,  2.25s/it]\u001b[A\n",
      "Running loss: 3.611748█████████▏              | 102/266 [03:48<06:08,  2.25s/it]\u001b[A\n",
      "Running loss: 3.553181█████████▎              | 103/266 [03:50<06:06,  2.25s/it]\u001b[A\n",
      "Running loss: 3.837204█████████▍              | 104/266 [03:52<06:03,  2.24s/it]\u001b[A\n",
      "Running loss: 3.617928█████████▍              | 105/266 [03:55<06:00,  2.24s/it]\u001b[A\n",
      "Running loss: 3.724151█████████▌              | 106/266 [03:57<05:58,  2.24s/it]\u001b[A\n",
      "Running loss: 3.595009█████████▋              | 107/266 [03:59<05:55,  2.24s/it]\u001b[A\n",
      "Running loss: 3.645696█████████▋              | 108/266 [04:01<05:53,  2.24s/it]\u001b[A\n",
      "Running loss: 3.543611█████████▊              | 109/266 [04:04<05:51,  2.24s/it]\u001b[A\n",
      "Running loss: 3.714771█████████▉              | 110/266 [04:06<05:49,  2.24s/it]\u001b[A\n",
      "Running loss: 3.810137██████████              | 111/266 [04:08<05:48,  2.25s/it]\u001b[A\n",
      "Running loss: 3.466410██████████              | 112/266 [04:10<05:46,  2.25s/it]\u001b[A\n",
      "Running loss: 3.603021██████████▏             | 113/266 [04:13<05:44,  2.25s/it]\u001b[A\n",
      "Running loss: 3.656428██████████▎             | 114/266 [04:15<05:41,  2.25s/it]\u001b[A\n",
      "Running loss: 3.804851██████████▍             | 115/266 [04:17<05:38,  2.24s/it]\u001b[A\n",
      "Running loss: 3.586173██████████▍             | 116/266 [04:19<05:36,  2.24s/it]\u001b[A\n",
      "Running loss: 3.314911██████████▌             | 117/266 [04:22<05:33,  2.24s/it]\u001b[A\n",
      "Running loss: 3.374128██████████▋             | 118/266 [04:24<05:31,  2.24s/it]\u001b[A\n",
      "Running loss: 3.606443██████████▋             | 119/266 [04:26<05:29,  2.24s/it]\u001b[A\n",
      "Running loss: 3.740083██████████▊             | 120/266 [04:28<05:27,  2.24s/it]\u001b[A\n",
      "Running loss: 3.856217██████████▉             | 121/266 [04:31<05:25,  2.25s/it]\u001b[A\n",
      "Running loss: 3.631431███████████             | 122/266 [04:33<05:23,  2.25s/it]\u001b[A\n",
      "Running loss: 3.755263███████████             | 123/266 [04:35<05:21,  2.25s/it]\u001b[A\n",
      "Running loss: 3.572283███████████▏            | 124/266 [04:37<05:18,  2.24s/it]\u001b[A\n",
      "Running loss: 3.439314███████████▎            | 125/266 [04:40<05:16,  2.25s/it]\u001b[A\n",
      "Running loss: 3.636779███████████▎            | 126/266 [04:42<05:14,  2.25s/it]\u001b[A\n",
      "Running loss: 3.603194███████████▍            | 127/266 [04:44<05:13,  2.26s/it]\u001b[A\n",
      "Running loss: 3.786311███████████▌            | 128/266 [04:46<05:10,  2.25s/it]\u001b[A\n",
      "Running loss: 3.753299███████████▋            | 129/266 [04:49<05:07,  2.25s/it]\u001b[A\n",
      "Running loss: 3.729416███████████▋            | 130/266 [04:51<05:04,  2.24s/it]\u001b[A\n",
      "Running loss: 3.508984███████████▊            | 131/266 [04:53<05:02,  2.24s/it]\u001b[A\n",
      "Running loss: 3.932180███████████▉            | 132/266 [04:55<05:00,  2.24s/it]\u001b[A\n",
      "Running loss: 3.499607████████████            | 133/266 [04:58<04:58,  2.25s/it]\u001b[A\n",
      "Running loss: 3.330719████████████            | 134/266 [05:00<04:56,  2.25s/it]\u001b[A\n",
      "Running loss: 3.697370████████████▏           | 135/266 [05:02<04:54,  2.25s/it]\u001b[A\n",
      "Running loss: 3.632831████████████▎           | 136/266 [05:04<04:52,  2.25s/it]\u001b[A\n",
      "Running loss: 3.610246████████████▎           | 137/266 [05:07<04:49,  2.25s/it]\u001b[A\n",
      "Running loss: 3.779069████████████▍           | 138/266 [05:09<04:47,  2.25s/it]\u001b[A\n",
      "Running loss: 3.448974████████████▌           | 139/266 [05:11<04:45,  2.25s/it]\u001b[A\n",
      "Running loss: 3.580198████████████▋           | 140/266 [05:13<04:42,  2.24s/it]\u001b[A\n",
      "Running loss: 3.780183████████████▋           | 141/266 [05:16<04:40,  2.24s/it]\u001b[A\n",
      "Running loss: 3.700149████████████▊           | 142/266 [05:18<04:38,  2.25s/it]\u001b[A\n",
      "Running loss: 3.434050████████████▉           | 143/266 [05:20<04:35,  2.24s/it]\u001b[A\n",
      "Running loss: 3.689327████████████▉           | 144/266 [05:22<04:33,  2.24s/it]\u001b[A\n",
      "Running loss: 3.754529█████████████           | 145/266 [05:24<04:31,  2.24s/it]\u001b[A\n",
      "Running loss: 3.720367█████████████▏          | 146/266 [05:27<04:28,  2.24s/it]\u001b[A\n",
      "Running loss: 3.508800█████████████▎          | 147/266 [05:29<04:26,  2.24s/it]\u001b[A\n",
      "Running loss: 3.576735█████████████▎          | 148/266 [05:31<04:24,  2.24s/it]\u001b[A\n",
      "Running loss: 3.461654█████████████▍          | 149/266 [05:33<04:21,  2.24s/it]\u001b[A\n",
      "Running loss: 3.662911█████████████▌          | 150/266 [05:36<04:20,  2.24s/it]\u001b[A\n",
      "Running loss: 3.108706█████████████▌          | 151/266 [05:38<04:18,  2.24s/it]\u001b[A\n",
      "Running loss: 3.480802█████████████▋          | 152/266 [05:40<04:15,  2.24s/it]\u001b[A\n",
      "Running loss: 3.699886█████████████▊          | 153/266 [05:42<04:13,  2.25s/it]\u001b[A\n",
      "Running loss: 3.578697█████████████▉          | 154/266 [05:45<04:10,  2.24s/it]\u001b[A\n",
      "Running loss: 3.638528█████████████▉          | 155/266 [05:47<04:09,  2.24s/it]\u001b[A\n",
      "Running loss: 3.611445██████████████          | 156/266 [05:49<04:06,  2.24s/it]\u001b[A\n",
      "Running loss: 3.666763██████████████▏         | 157/266 [05:51<04:03,  2.24s/it]\u001b[A\n",
      "Running loss: 3.520867██████████████▎         | 158/266 [05:54<04:02,  2.24s/it]\u001b[A\n",
      "Running loss: 3.919669██████████████▎         | 159/266 [05:56<04:00,  2.24s/it]\u001b[A\n",
      "Running loss: 3.543684██████████████▍         | 160/266 [05:58<03:58,  2.25s/it]\u001b[A\n",
      "Running loss: 3.589100██████████████▌         | 161/266 [06:00<03:56,  2.25s/it]\u001b[A\n",
      "Running loss: 3.585111██████████████▌         | 162/266 [06:03<03:53,  2.25s/it]\u001b[A\n",
      "Running loss: 3.320333██████████████▋         | 163/266 [06:05<03:51,  2.24s/it]\u001b[A\n",
      "Running loss: 3.765830██████████████▊         | 164/266 [06:07<03:49,  2.25s/it]\u001b[A\n",
      "Running loss: 3.580057██████████████▉         | 165/266 [06:09<03:46,  2.24s/it]\u001b[A\n",
      "Running loss: 3.914853██████████████▉         | 166/266 [06:12<03:44,  2.25s/it]\u001b[A\n",
      "Running loss: 3.639575███████████████         | 167/266 [06:14<03:43,  2.25s/it]\u001b[A\n",
      "Running loss: 3.843592███████████████▏        | 168/266 [06:16<03:40,  2.25s/it]\u001b[A\n",
      "Running loss: 3.434448███████████████▏        | 169/266 [06:18<03:37,  2.25s/it]\u001b[A\n",
      "Running loss: 3.611067███████████████▎        | 170/266 [06:21<03:35,  2.24s/it]\u001b[A\n",
      "Running loss: 3.912031███████████████▍        | 171/266 [06:23<03:32,  2.24s/it]\u001b[A\n",
      "Running loss: 3.380528███████████████▌        | 172/266 [06:25<03:30,  2.24s/it]\u001b[A\n",
      "Running loss: 3.774253███████████████▌        | 173/266 [06:27<03:28,  2.24s/it]\u001b[A\n",
      "Running loss: 3.550815███████████████▋        | 174/266 [06:30<03:25,  2.24s/it]\u001b[A\n",
      "Running loss: 3.501628███████████████▊        | 175/266 [06:32<03:23,  2.24s/it]\u001b[A\n",
      "Running loss: 4.049306███████████████▉        | 176/266 [06:34<03:22,  2.24s/it]\u001b[A\n",
      "Running loss: 3.690373███████████████▉        | 177/266 [06:36<03:19,  2.24s/it]\u001b[A\n",
      "Running loss: 3.640500████████████████        | 178/266 [06:38<03:17,  2.24s/it]\u001b[A\n",
      "Running loss: 3.606668████████████████▏       | 179/266 [06:41<03:14,  2.23s/it]\u001b[A\n",
      "Running loss: 3.336052████████████████▏       | 180/266 [06:43<03:12,  2.23s/it]\u001b[A\n",
      "Running loss: 3.787403████████████████▎       | 181/266 [06:45<03:09,  2.23s/it]\u001b[A\n",
      "Running loss: 3.406323████████████████▍       | 182/266 [06:47<03:07,  2.24s/it]\u001b[A\n",
      "Running loss: 3.616796████████████████▌       | 183/266 [06:50<03:06,  2.25s/it]\u001b[A\n",
      "Running loss: 3.686199████████████████▌       | 184/266 [06:52<03:03,  2.24s/it]\u001b[A\n",
      "Running loss: 3.731136████████████████▋       | 185/266 [06:54<03:01,  2.25s/it]\u001b[A\n",
      "Running loss: 3.455389████████████████▊       | 186/266 [06:56<02:59,  2.25s/it]\u001b[A\n",
      "Running loss: 3.452237████████████████▊       | 187/266 [06:59<02:56,  2.24s/it]\u001b[A\n",
      "Running loss: 3.670643████████████████▉       | 188/266 [07:01<02:54,  2.24s/it]\u001b[A\n",
      "Running loss: 3.699261█████████████████       | 189/266 [07:03<02:52,  2.24s/it]\u001b[A\n",
      "Running loss: 3.670360█████████████████▏      | 190/266 [07:05<02:49,  2.23s/it]\u001b[A\n",
      "Running loss: 3.441464█████████████████▏      | 191/266 [07:08<02:46,  2.23s/it]\u001b[A\n",
      "Running loss: 3.611655█████████████████▎      | 192/266 [07:10<02:44,  2.22s/it]\u001b[A\n",
      "Running loss: 3.714052█████████████████▍      | 193/266 [07:12<02:42,  2.23s/it]\u001b[A\n",
      "Running loss: 3.369041█████████████████▌      | 194/266 [07:14<02:41,  2.24s/it]\u001b[A\n",
      "Running loss: 3.635158█████████████████▌      | 195/266 [07:17<02:38,  2.24s/it]\u001b[A\n",
      "Running loss: 3.511612█████████████████▋      | 196/266 [07:19<02:37,  2.24s/it]\u001b[A\n",
      "Running loss: 3.616500█████████████████▊      | 197/266 [07:21<02:34,  2.25s/it]\u001b[A\n",
      "Running loss: 3.345796█████████████████▊      | 198/266 [07:23<02:32,  2.24s/it]\u001b[A\n",
      "Running loss: 3.747782█████████████████▉      | 199/266 [07:26<02:30,  2.25s/it]\u001b[A\n",
      "Running loss: 3.656044██████████████████      | 200/266 [07:28<02:28,  2.24s/it]\u001b[A\n",
      "Running loss: 3.613437██████████████████▏     | 201/266 [07:30<02:25,  2.24s/it]\u001b[A\n",
      "Running loss: 3.704773██████████████████▏     | 202/266 [07:32<02:23,  2.23s/it]\u001b[A\n",
      "Running loss: 3.614079██████████████████▎     | 203/266 [07:34<02:20,  2.24s/it]\u001b[A\n",
      "Running loss: 3.820429██████████████████▍     | 204/266 [07:37<02:18,  2.24s/it]\u001b[A\n",
      "Running loss: 3.477539██████████████████▍     | 205/266 [07:39<02:17,  2.25s/it]\u001b[A\n",
      "Running loss: 3.804761██████████████████▌     | 206/266 [07:41<02:14,  2.24s/it]\u001b[A\n",
      "Running loss: 3.640164██████████████████▋     | 207/266 [07:43<02:12,  2.25s/it]\u001b[A\n",
      "Running loss: 3.578808██████████████████▊     | 208/266 [07:46<02:10,  2.25s/it]\u001b[A\n",
      "Running loss: 3.479403██████████████████▊     | 209/266 [07:48<02:07,  2.24s/it]\u001b[A\n",
      "Running loss: 3.651753██████████████████▉     | 210/266 [07:50<02:05,  2.25s/it]\u001b[A\n",
      "Running loss: 3.638973███████████████████     | 211/266 [07:52<02:03,  2.24s/it]\u001b[A\n",
      "Running loss: 3.454381███████████████████▏    | 212/266 [07:55<02:00,  2.24s/it]\u001b[A\n",
      "Running loss: 3.660910███████████████████▏    | 213/266 [07:57<01:59,  2.25s/it]\u001b[A\n",
      "Running loss: 3.703463███████████████████▎    | 214/266 [07:59<01:56,  2.24s/it]\u001b[A\n",
      "Running loss: 3.376252███████████████████▍    | 215/266 [08:01<01:54,  2.25s/it]\u001b[A\n",
      "Running loss: 3.753429███████████████████▍    | 216/266 [08:04<01:52,  2.25s/it]\u001b[A\n",
      "Running loss: 3.550422███████████████████▌    | 217/266 [08:06<01:50,  2.25s/it]\u001b[A\n",
      "Running loss: 3.679799███████████████████▋    | 218/266 [08:08<01:47,  2.25s/it]\u001b[A\n",
      "Running loss: 3.697463███████████████████▊    | 219/266 [08:10<01:45,  2.25s/it]\u001b[A\n",
      "Running loss: 3.865016███████████████████▊    | 220/266 [08:13<01:43,  2.25s/it]\u001b[A\n",
      "Running loss: 3.645301███████████████████▉    | 221/266 [08:15<01:41,  2.25s/it]\u001b[A\n",
      "Running loss: 3.677865████████████████████    | 222/266 [08:17<01:39,  2.25s/it]\u001b[A\n",
      "Running loss: 3.565711████████████████████    | 223/266 [08:19<01:36,  2.25s/it]\u001b[A\n",
      "Running loss: 3.540164████████████████████▏   | 224/266 [08:22<01:34,  2.24s/it]\u001b[A\n",
      "Running loss: 3.258417████████████████████▎   | 225/266 [08:24<01:31,  2.24s/it]\u001b[A\n",
      "Running loss: 3.668491████████████████████▍   | 226/266 [08:26<01:29,  2.23s/it]\u001b[A\n",
      "Running loss: 3.845670████████████████████▍   | 227/266 [08:28<01:27,  2.24s/it]\u001b[A\n",
      "Running loss: 3.749373████████████████████▌   | 228/266 [08:31<01:25,  2.24s/it]\u001b[A\n",
      "Running loss: 3.522289████████████████████▋   | 229/266 [08:33<01:23,  2.24s/it]\u001b[A\n",
      "Running loss: 3.644399████████████████████▊   | 230/266 [08:35<01:20,  2.25s/it]\u001b[A\n",
      "Running loss: 3.611114████████████████████▊   | 231/266 [08:37<01:18,  2.25s/it]\u001b[A\n",
      "Running loss: 3.332941████████████████████▉   | 232/266 [08:40<01:16,  2.25s/it]\u001b[A\n",
      "Running loss: 4.155396█████████████████████   | 233/266 [08:42<01:14,  2.25s/it]\u001b[A\n",
      "Running loss: 3.641333█████████████████████   | 234/266 [08:44<01:11,  2.25s/it]\u001b[A\n",
      "Running loss: 3.777021█████████████████████▏  | 235/266 [08:46<01:09,  2.24s/it]\u001b[A\n",
      "Running loss: 3.563168█████████████████████▎  | 236/266 [08:49<01:07,  2.24s/it]\u001b[A\n",
      "Running loss: 3.216715█████████████████████▍  | 237/266 [08:51<01:05,  2.25s/it]\u001b[A\n",
      "Running loss: 3.581863█████████████████████▍  | 238/266 [08:53<01:02,  2.24s/it]\u001b[A\n",
      "Running loss: 3.653044█████████████████████▌  | 239/266 [08:55<01:00,  2.24s/it]\u001b[A\n",
      "Running loss: 3.536070█████████████████████▋  | 240/266 [08:58<00:58,  2.24s/it]\u001b[A\n",
      "Running loss: 3.440008█████████████████████▋  | 241/266 [09:00<00:56,  2.24s/it]\u001b[A\n",
      "Running loss: 3.885893█████████████████████▊  | 242/266 [09:02<00:53,  2.25s/it]\u001b[A\n",
      "Running loss: 3.575075█████████████████████▉  | 243/266 [09:04<00:51,  2.25s/it]\u001b[A\n",
      "Running loss: 3.248344██████████████████████  | 244/266 [09:07<00:49,  2.25s/it]\u001b[A\n",
      "Running loss: 3.735613██████████████████████  | 245/266 [09:09<00:47,  2.25s/it]\u001b[A\n",
      "Running loss: 3.535986██████████████████████▏ | 246/266 [09:11<00:44,  2.25s/it]\u001b[A\n",
      "Running loss: 3.849576██████████████████████▎ | 247/266 [09:13<00:42,  2.25s/it]\u001b[A\n",
      "Running loss: 3.514325██████████████████████▍ | 248/266 [09:15<00:40,  2.25s/it]\u001b[A\n",
      "Running loss: 3.536718██████████████████████▍ | 249/266 [09:18<00:38,  2.24s/it]\u001b[A\n",
      "Running loss: 3.587943██████████████████████▌ | 250/266 [09:20<00:36,  2.25s/it]\u001b[A\n",
      "Running loss: 3.538495██████████████████████▋ | 251/266 [09:22<00:33,  2.24s/it]\u001b[A\n",
      "Running loss: 3.867658██████████████████████▋ | 252/266 [09:24<00:31,  2.24s/it]\u001b[A\n",
      "Running loss: 3.890156██████████████████████▊ | 253/266 [09:27<00:29,  2.24s/it]\u001b[A\n",
      "Running loss: 3.366465██████████████████████▉ | 254/266 [09:29<00:26,  2.24s/it]\u001b[A\n",
      "Running loss: 3.524735███████████████████████ | 255/266 [09:31<00:24,  2.25s/it]\u001b[A\n",
      "Running loss: 3.468444███████████████████████ | 256/266 [09:33<00:22,  2.25s/it]\u001b[A\n",
      "Running loss: 3.592862███████████████████████▏| 257/266 [09:36<00:20,  2.25s/it]\u001b[A\n",
      "Running loss: 3.456322███████████████████████▎| 258/266 [09:38<00:17,  2.24s/it]\u001b[A\n",
      "Running loss: 3.680390███████████████████████▎| 259/266 [09:40<00:15,  2.24s/it]\u001b[A\n",
      "Running loss: 3.433045███████████████████████▍| 260/266 [09:42<00:13,  2.24s/it]\u001b[A\n",
      "Running loss: 3.503708███████████████████████▌| 261/266 [09:45<00:11,  2.24s/it]\u001b[A\n",
      "Running loss: 3.426819███████████████████████▋| 262/266 [09:47<00:08,  2.24s/it]\u001b[A\n",
      "Running loss: 3.519709███████████████████████▋| 263/266 [09:49<00:06,  2.24s/it]\u001b[A\n",
      "Running loss: 3.494952███████████████████████▊| 264/266 [09:51<00:04,  2.25s/it]\u001b[A\n",
      "Running loss: 3.834327███████████████████████▉| 265/266 [09:54<00:02,  2.24s/it]\u001b[A\n",
      "Epoch 3 of 3:  67%|████████████████████          | 2/3 [20:25<10:14, 614.14s/it]\u001b[A\n",
      "Running loss: 3.697341                                  | 0/266 [00:00<?, ?it/s]\u001b[A\n",
      "Running loss: 3.467710                          | 1/266 [00:02<10:11,  2.31s/it]\u001b[A\n",
      "Running loss: 3.578335▏                         | 2/266 [00:04<10:03,  2.29s/it]\u001b[A\n",
      "Running loss: 3.542067▎                         | 3/266 [00:06<09:57,  2.27s/it]\u001b[A\n",
      "Running loss: 3.877695▍                         | 4/266 [00:09<09:53,  2.27s/it]\u001b[A\n",
      "Running loss: 3.730253▍                         | 5/266 [00:11<09:48,  2.25s/it]\u001b[A\n",
      "Running loss: 3.717685▌                         | 6/266 [00:13<09:45,  2.25s/it]\u001b[A\n",
      "Running loss: 3.534581▋                         | 7/266 [00:15<09:42,  2.25s/it]\u001b[A\n",
      "Running loss: 3.830373▊                         | 8/266 [00:17<09:38,  2.24s/it]\u001b[A\n",
      "Running loss: 3.698032▉                         | 9/266 [00:20<09:38,  2.25s/it]\u001b[A\n",
      "Running loss: 3.512473▉                        | 10/266 [00:22<09:34,  2.24s/it]\u001b[A\n",
      "Running loss: 3.563247█                        | 11/266 [00:24<09:30,  2.24s/it]\u001b[A\n",
      "Running loss: 3.822708█▏                       | 12/266 [00:26<09:29,  2.24s/it]\u001b[A\n",
      "Running loss: 3.525477█▏                       | 13/266 [00:29<09:28,  2.25s/it]\u001b[A\n",
      "Running loss: 3.795313█▎                       | 14/266 [00:31<09:25,  2.24s/it]\u001b[A\n",
      "Running loss: 3.727917█▍                       | 15/266 [00:33<09:21,  2.24s/it]\u001b[A\n",
      "Running loss: 3.358336█▌                       | 16/266 [00:35<09:18,  2.23s/it]\u001b[A\n",
      "Running loss: 3.577627█▌                       | 17/266 [00:38<09:15,  2.23s/it]\u001b[A\n",
      "Running loss: 3.930671█▋                       | 18/266 [00:40<09:15,  2.24s/it]\u001b[A\n",
      "Running loss: 3.513223█▊                       | 19/266 [00:42<09:14,  2.24s/it]\u001b[A\n",
      "Running loss: 3.519410█▉                       | 20/266 [00:44<09:11,  2.24s/it]\u001b[A\n",
      "Running loss: 3.683553█▉                       | 21/266 [00:47<09:08,  2.24s/it]\u001b[A\n",
      "Running loss: 3.365857██                       | 22/266 [00:49<09:06,  2.24s/it]\u001b[A\n",
      "Running loss: 3.772808██▏                      | 23/266 [00:51<09:04,  2.24s/it]\u001b[A\n",
      "Running loss: 3.761924██▎                      | 24/266 [00:53<09:00,  2.23s/it]\u001b[A\n",
      "Running loss: 3.486387██▎                      | 25/266 [00:56<09:00,  2.24s/it]\u001b[A\n",
      "Running loss: 3.391454██▍                      | 26/266 [00:58<08:58,  2.25s/it]\u001b[A\n",
      "Running loss: 3.564135██▌                      | 27/266 [01:00<08:57,  2.25s/it]\u001b[A\n",
      "Running loss: 3.628897██▋                      | 28/266 [01:02<08:55,  2.25s/it]\u001b[A\n",
      "Running loss: 3.450366██▋                      | 29/266 [01:05<08:53,  2.25s/it]\u001b[A\n",
      "Running loss: 3.799826██▊                      | 30/266 [01:07<08:49,  2.25s/it]\u001b[A\n",
      "Running loss: 3.440333██▉                      | 31/266 [01:09<08:46,  2.24s/it]\u001b[A\n",
      "Running loss: 3.592690███                      | 32/266 [01:11<08:44,  2.24s/it]\u001b[A\n",
      "Running loss: 3.526558███                      | 33/266 [01:14<08:42,  2.24s/it]\u001b[A\n",
      "Running loss: 3.638525███▏                     | 34/266 [01:16<08:39,  2.24s/it]\u001b[A\n",
      "Running loss: 3.757009███▎                     | 35/266 [01:18<08:36,  2.24s/it]\u001b[A\n",
      "Running loss: 3.609931███▍                     | 36/266 [01:20<08:34,  2.24s/it]\u001b[A\n",
      "Running loss: 3.515179███▍                     | 37/266 [01:22<08:32,  2.24s/it]\u001b[A\n",
      "Running loss: 3.841314███▌                     | 38/266 [01:25<08:31,  2.25s/it]\u001b[A\n",
      "Running loss: 3.535479███▋                     | 39/266 [01:27<08:29,  2.24s/it]\u001b[A\n",
      "Running loss: 3.475672███▊                     | 40/266 [01:29<08:26,  2.24s/it]\u001b[A\n",
      "Running loss: 3.376032███▊                     | 41/266 [01:31<08:24,  2.24s/it]\u001b[A\n",
      "Running loss: 3.534484███▉                     | 42/266 [01:34<08:20,  2.24s/it]\u001b[A\n",
      "Running loss: 3.704376████                     | 43/266 [01:36<08:19,  2.24s/it]\u001b[A\n",
      "Running loss: 3.760230████▏                    | 44/266 [01:38<08:24,  2.27s/it]\u001b[A\n",
      "Running loss: 3.502912████▏                    | 45/266 [01:41<08:19,  2.26s/it]\u001b[A\n",
      "Running loss: 3.568499████▎                    | 46/266 [01:43<08:16,  2.26s/it]\u001b[A\n",
      "Running loss: 3.808192████▍                    | 47/266 [01:45<08:12,  2.25s/it]\u001b[A\n",
      "Running loss: 3.451641████▌                    | 48/266 [01:47<08:09,  2.25s/it]\u001b[A\n",
      "Running loss: 3.704184████▌                    | 49/266 [01:49<08:09,  2.25s/it]\u001b[A\n",
      "Running loss: 3.873614████▋                    | 50/266 [01:52<08:06,  2.25s/it]\u001b[A\n",
      "Running loss: 3.345167████▊                    | 51/266 [01:54<08:03,  2.25s/it]\u001b[A\n",
      "Running loss: 3.550394████▉                    | 52/266 [01:56<08:01,  2.25s/it]\u001b[A\n",
      "Running loss: 3.467477████▉                    | 53/266 [01:59<07:59,  2.25s/it]\u001b[A\n",
      "Running loss: 3.531886█████                    | 54/266 [02:01<07:58,  2.25s/it]\u001b[A\n",
      "Running loss: 3.734417█████▏                   | 55/266 [02:03<07:55,  2.25s/it]\u001b[A\n",
      "Running loss: 3.782567█████▎                   | 56/266 [02:05<07:50,  2.24s/it]\u001b[A\n",
      "Running loss: 3.668961█████▎                   | 57/266 [02:07<07:48,  2.24s/it]\u001b[A\n",
      "Running loss: 3.574765█████▍                   | 58/266 [02:10<07:44,  2.23s/it]\u001b[A\n",
      "Running loss: 3.679017█████▌                   | 59/266 [02:12<07:41,  2.23s/it]\u001b[A\n",
      "Running loss: 3.657759█████▋                   | 60/266 [02:14<07:40,  2.24s/it]\u001b[A\n",
      "Running loss: 3.593481█████▋                   | 61/266 [02:16<07:38,  2.24s/it]\u001b[A\n",
      "Running loss: 3.798784█████▊                   | 62/266 [02:19<07:36,  2.24s/it]\u001b[A\n",
      "Running loss: 3.506216█████▉                   | 63/266 [02:21<07:32,  2.23s/it]\u001b[A\n",
      "Running loss: 3.594065██████                   | 64/266 [02:23<07:31,  2.24s/it]\u001b[A\n",
      "Running loss: 3.636405██████                   | 65/266 [02:25<07:29,  2.24s/it]\u001b[A\n",
      "Running loss: 3.660508██████▏                  | 66/266 [02:28<07:28,  2.24s/it]\u001b[A\n",
      "Running loss: 3.911993██████▎                  | 67/266 [02:30<07:26,  2.24s/it]\u001b[A\n",
      "Running loss: 3.735901██████▍                  | 68/266 [02:32<07:24,  2.24s/it]\u001b[A\n",
      "Running loss: 3.501228██████▍                  | 69/266 [02:34<07:21,  2.24s/it]\u001b[A\n",
      "Running loss: 3.353950██████▌                  | 70/266 [02:37<07:19,  2.24s/it]\u001b[A\n",
      "Running loss: 3.563893██████▋                  | 71/266 [02:39<07:18,  2.25s/it]\u001b[A\n",
      "Running loss: 3.394034██████▊                  | 72/266 [02:41<07:15,  2.25s/it]\u001b[A\n",
      "Running loss: 3.585479██████▊                  | 73/266 [02:43<07:13,  2.25s/it]\u001b[A\n",
      "Running loss: 3.571384██████▉                  | 74/266 [02:46<07:11,  2.25s/it]\u001b[A\n",
      "Running loss: 3.446815███████                  | 75/266 [02:48<07:09,  2.25s/it]\u001b[A\n",
      "Running loss: 3.345121███████▏                 | 76/266 [02:50<07:05,  2.24s/it]\u001b[A\n",
      "Running loss: 3.382513███████▏                 | 77/266 [02:52<07:02,  2.24s/it]\u001b[A\n",
      "Running loss: 3.665443███████▎                 | 78/266 [02:55<07:01,  2.24s/it]\u001b[A\n",
      "Running loss: 3.704391███████▍                 | 79/266 [02:57<07:00,  2.25s/it]\u001b[A\n",
      "Running loss: 3.961232███████▌                 | 80/266 [02:59<06:56,  2.24s/it]\u001b[A\n",
      "Running loss: 3.502201███████▌                 | 81/266 [03:01<06:54,  2.24s/it]\u001b[A\n",
      "Running loss: 3.664875███████▋                 | 82/266 [03:03<06:53,  2.25s/it]\u001b[A\n",
      "Running loss: 3.959206███████▊                 | 83/266 [03:06<06:52,  2.25s/it]\u001b[A\n",
      "Running loss: 3.927804███████▉                 | 84/266 [03:08<06:49,  2.25s/it]\u001b[A\n",
      "Running loss: 3.557107███████▉                 | 85/266 [03:10<06:47,  2.25s/it]\u001b[A\n",
      "Running loss: 3.423592████████                 | 86/266 [03:12<06:44,  2.25s/it]\u001b[A\n",
      "Running loss: 3.614623████████▏                | 87/266 [03:15<06:42,  2.25s/it]\u001b[A\n",
      "Running loss: 3.350626████████▎                | 88/266 [03:17<06:39,  2.24s/it]\u001b[A\n",
      "Running loss: 3.783874████████▎                | 89/266 [03:19<06:36,  2.24s/it]\u001b[A\n",
      "Running loss: 3.547702████████▍                | 90/266 [03:21<06:34,  2.24s/it]\u001b[A\n",
      "Running loss: 3.395224████████▌                | 91/266 [03:24<06:33,  2.25s/it]\u001b[A\n",
      "Running loss: 3.649066████████▋                | 92/266 [03:26<06:30,  2.24s/it]\u001b[A\n",
      "Running loss: 3.674139████████▋                | 93/266 [03:28<06:27,  2.24s/it]\u001b[A\n",
      "Running loss: 3.601841████████▊                | 94/266 [03:30<06:24,  2.23s/it]\u001b[A\n",
      "Running loss: 3.625664████████▉                | 95/266 [03:33<06:21,  2.23s/it]\u001b[A\n",
      "Running loss: 3.591256█████████                | 96/266 [03:35<06:19,  2.23s/it]\u001b[A\n",
      "Running loss: 3.686051█████████                | 97/266 [03:37<06:18,  2.24s/it]\u001b[A\n",
      "Running loss: 3.603370█████████▏               | 98/266 [03:39<06:15,  2.24s/it]\u001b[A\n",
      "Running loss: 3.524194█████████▎               | 99/266 [03:42<06:14,  2.24s/it]\u001b[A\n",
      "Running loss: 3.573968█████████               | 100/266 [03:44<06:12,  2.24s/it]\u001b[A\n",
      "Running loss: 3.471278█████████               | 101/266 [03:46<06:10,  2.24s/it]\u001b[A\n",
      "Running loss: 3.667934█████████▏              | 102/266 [03:48<06:07,  2.24s/it]\u001b[A\n",
      "Running loss: 3.387749█████████▎              | 103/266 [03:51<06:04,  2.24s/it]\u001b[A\n",
      "Running loss: 3.608084█████████▍              | 104/266 [03:53<06:01,  2.23s/it]\u001b[A\n",
      "Running loss: 3.739021█████████▍              | 105/266 [03:55<05:59,  2.23s/it]\u001b[A\n",
      "Running loss: 3.705296█████████▌              | 106/266 [03:57<05:57,  2.23s/it]\u001b[A\n",
      "Running loss: 3.695473█████████▋              | 107/266 [03:59<05:54,  2.23s/it]\u001b[A\n",
      "Running loss: 3.469823█████████▋              | 108/266 [04:02<05:53,  2.24s/it]\u001b[A\n",
      "Running loss: 3.276206█████████▊              | 109/266 [04:04<05:51,  2.24s/it]\u001b[A\n",
      "Running loss: 3.469239█████████▉              | 110/266 [04:06<05:49,  2.24s/it]\u001b[A\n",
      "Running loss: 3.547244██████████              | 111/266 [04:08<05:48,  2.25s/it]\u001b[A\n",
      "Running loss: 3.502894██████████              | 112/266 [04:11<05:45,  2.24s/it]\u001b[A\n",
      "Running loss: 3.490857██████████▏             | 113/266 [04:13<05:43,  2.24s/it]\u001b[A\n",
      "Running loss: 3.697602██████████▎             | 114/266 [04:15<05:41,  2.25s/it]\u001b[A\n",
      "Running loss: 3.376033██████████▍             | 115/266 [04:17<05:39,  2.25s/it]\u001b[A\n",
      "Running loss: 3.666067██████████▍             | 116/266 [04:20<05:36,  2.24s/it]\u001b[A\n",
      "Running loss: 3.487411██████████▌             | 117/266 [04:22<05:34,  2.24s/it]\u001b[A\n",
      "Running loss: 3.686646██████████▋             | 118/266 [04:24<05:31,  2.24s/it]\u001b[A\n",
      "Running loss: 3.771793██████████▋             | 119/266 [04:26<05:29,  2.24s/it]\u001b[A\n",
      "Running loss: 3.667411██████████▊             | 120/266 [04:29<05:28,  2.25s/it]\u001b[A\n",
      "Running loss: 3.463842██████████▉             | 121/266 [04:31<05:25,  2.25s/it]\u001b[A\n",
      "Running loss: 3.613987███████████             | 122/266 [04:33<05:23,  2.25s/it]\u001b[A\n",
      "Running loss: 3.370419███████████             | 123/266 [04:35<05:22,  2.25s/it]\u001b[A\n",
      "Running loss: 3.494377███████████▏            | 124/266 [04:38<05:19,  2.25s/it]\u001b[A\n",
      "Running loss: 3.568424███████████▎            | 125/266 [04:40<05:17,  2.25s/it]\u001b[A\n",
      "Running loss: 3.785396███████████▎            | 126/266 [04:42<05:15,  2.25s/it]\u001b[A\n",
      "Running loss: 3.732293███████████▍            | 127/266 [04:44<05:12,  2.25s/it]\u001b[A\n",
      "Running loss: 3.542280███████████▌            | 128/266 [04:47<05:10,  2.25s/it]\u001b[A\n",
      "Running loss: 3.413188███████████▋            | 129/266 [04:49<05:08,  2.25s/it]\u001b[A\n",
      "Running loss: 3.713639███████████▋            | 130/266 [04:51<05:06,  2.25s/it]\u001b[A\n",
      "Running loss: 3.455393███████████▊            | 131/266 [04:53<05:03,  2.25s/it]\u001b[A\n",
      "Running loss: 3.838083███████████▉            | 132/266 [04:56<05:01,  2.25s/it]\u001b[A\n",
      "Running loss: 3.707538████████████            | 133/266 [04:58<04:59,  2.25s/it]\u001b[A\n",
      "Running loss: 3.937672████████████            | 134/266 [05:00<04:56,  2.25s/it]\u001b[A\n",
      "Running loss: 3.654037████████████▏           | 135/266 [05:02<04:54,  2.25s/it]\u001b[A\n",
      "Running loss: 3.419302████████████▎           | 136/266 [05:05<04:52,  2.25s/it]\u001b[A\n",
      "Running loss: 3.762856████████████▎           | 137/266 [05:07<04:49,  2.25s/it]\u001b[A\n",
      "Running loss: 3.534576████████████▍           | 138/266 [05:09<04:47,  2.25s/it]\u001b[A\n",
      "Running loss: 3.509892████████████▌           | 139/266 [05:11<04:44,  2.24s/it]\u001b[A\n",
      "Running loss: 3.444451████████████▋           | 140/266 [05:14<04:42,  2.24s/it]\u001b[A\n",
      "Running loss: 3.646516████████████▋           | 141/266 [05:16<04:40,  2.24s/it]\u001b[A\n",
      "Running loss: 3.704657████████████▊           | 142/266 [05:18<04:37,  2.24s/it]\u001b[A\n",
      "Running loss: 3.535771████████████▉           | 143/266 [05:20<04:35,  2.24s/it]\u001b[A\n",
      "Running loss: 3.481126████████████▉           | 144/266 [05:23<04:33,  2.24s/it]\u001b[A\n",
      "Running loss: 3.699349█████████████           | 145/266 [05:25<04:31,  2.24s/it]\u001b[A\n",
      "Running loss: 3.259766█████████████▏          | 146/266 [05:27<04:29,  2.24s/it]\u001b[A\n",
      "Running loss: 3.555630█████████████▎          | 147/266 [05:29<04:27,  2.24s/it]\u001b[A\n",
      "Running loss: 3.618227█████████████▎          | 148/266 [05:32<04:25,  2.25s/it]\u001b[A\n",
      "Running loss: 3.512896█████████████▍          | 149/266 [05:34<04:22,  2.25s/it]\u001b[A\n",
      "Running loss: 3.533412█████████████▌          | 150/266 [05:36<04:20,  2.24s/it]\u001b[A\n",
      "Running loss: 3.461826█████████████▌          | 151/266 [05:38<04:18,  2.25s/it]\u001b[A\n",
      "Running loss: 3.330531█████████████▋          | 152/266 [05:41<04:15,  2.24s/it]\u001b[A\n",
      "Running loss: 3.403873█████████████▊          | 153/266 [05:43<04:13,  2.24s/it]\u001b[A\n",
      "Running loss: 3.641382█████████████▉          | 154/266 [05:45<04:11,  2.25s/it]\u001b[A\n",
      "Running loss: 3.799828█████████████▉          | 155/266 [05:47<04:09,  2.25s/it]\u001b[A\n",
      "Running loss: 3.266981██████████████          | 156/266 [05:50<04:06,  2.24s/it]\u001b[A\n",
      "Running loss: 3.586671██████████████▏         | 157/266 [05:52<04:03,  2.24s/it]\u001b[A\n",
      "Running loss: 3.650709██████████████▎         | 158/266 [05:54<04:01,  2.24s/it]\u001b[A\n",
      "Running loss: 3.578637██████████████▎         | 159/266 [05:56<03:59,  2.24s/it]\u001b[A\n",
      "Running loss: 3.588117██████████████▍         | 160/266 [05:58<03:57,  2.24s/it]\u001b[A\n",
      "Running loss: 3.602434██████████████▌         | 161/266 [06:01<03:55,  2.24s/it]\u001b[A\n",
      "Running loss: 3.577958██████████████▌         | 162/266 [06:03<03:53,  2.24s/it]\u001b[A\n",
      "Running loss: 3.657752██████████████▋         | 163/266 [06:05<03:51,  2.25s/it]\u001b[A\n",
      "Running loss: 3.563283██████████████▊         | 164/266 [06:07<03:48,  2.24s/it]\u001b[A\n",
      "Running loss: 3.620239██████████████▉         | 165/266 [06:10<03:46,  2.24s/it]\u001b[A\n",
      "Running loss: 3.446429██████████████▉         | 166/266 [06:12<03:43,  2.24s/it]\u001b[A\n",
      "Running loss: 3.502070███████████████         | 167/266 [06:14<03:41,  2.24s/it]\u001b[A\n",
      "Running loss: 3.588846███████████████▏        | 168/266 [06:16<03:39,  2.24s/it]\u001b[A\n",
      "Running loss: 3.478475███████████████▏        | 169/266 [06:19<03:37,  2.24s/it]\u001b[A\n",
      "Running loss: 3.615623███████████████▎        | 170/266 [06:21<03:35,  2.25s/it]\u001b[A\n",
      "Running loss: 3.498575███████████████▍        | 171/266 [06:23<03:33,  2.25s/it]\u001b[A\n",
      "Running loss: 3.839960███████████████▌        | 172/266 [06:25<03:31,  2.25s/it]\u001b[A\n",
      "Running loss: 3.478840███████████████▌        | 173/266 [06:28<03:28,  2.24s/it]\u001b[A\n",
      "Running loss: 3.618913███████████████▋        | 174/266 [06:30<03:25,  2.24s/it]\u001b[A\n",
      "Running loss: 3.495436███████████████▊        | 175/266 [06:32<03:23,  2.24s/it]\u001b[A\n",
      "Running loss: 3.370769███████████████▉        | 176/266 [06:34<03:21,  2.23s/it]\u001b[A\n",
      "Running loss: 3.301923███████████████▉        | 177/266 [06:37<03:19,  2.24s/it]\u001b[A\n",
      "Running loss: 3.490118████████████████        | 178/266 [06:39<03:17,  2.24s/it]\u001b[A\n",
      "Running loss: 3.606306████████████████▏       | 179/266 [06:41<03:15,  2.24s/it]\u001b[A\n",
      "Running loss: 3.835163████████████████▏       | 180/266 [06:43<03:13,  2.25s/it]\u001b[A\n",
      "Running loss: 3.642295████████████████▎       | 181/266 [06:46<03:11,  2.25s/it]\u001b[A\n",
      "Running loss: 3.493879████████████████▍       | 182/266 [06:48<03:08,  2.25s/it]\u001b[A\n",
      "Running loss: 3.724117████████████████▌       | 183/266 [06:50<03:06,  2.25s/it]\u001b[A\n",
      "Running loss: 3.662128████████████████▌       | 184/266 [06:52<03:04,  2.25s/it]\u001b[A\n",
      "Running loss: 3.582476████████████████▋       | 185/266 [06:55<03:02,  2.25s/it]\u001b[A\n",
      "Running loss: 3.555274████████████████▊       | 186/266 [06:57<03:00,  2.26s/it]\u001b[A\n",
      "Running loss: 3.566388████████████████▊       | 187/266 [06:59<02:58,  2.26s/it]\u001b[A\n",
      "Running loss: 3.837493████████████████▉       | 188/266 [07:01<02:55,  2.25s/it]\u001b[A\n",
      "Running loss: 3.616323█████████████████       | 189/266 [07:04<02:52,  2.24s/it]\u001b[A\n",
      "Running loss: 3.333458█████████████████▏      | 190/266 [07:06<02:50,  2.24s/it]\u001b[A\n",
      "Running loss: 3.606761█████████████████▏      | 191/266 [07:08<02:48,  2.24s/it]\u001b[A\n",
      "Running loss: 3.771029█████████████████▎      | 192/266 [07:10<02:45,  2.24s/it]\u001b[A\n",
      "Running loss: 3.762702█████████████████▍      | 193/266 [07:13<02:44,  2.25s/it]\u001b[A\n",
      "Running loss: 3.358633█████████████████▌      | 194/266 [07:15<02:41,  2.24s/it]\u001b[A\n",
      "Running loss: 3.576899█████████████████▌      | 195/266 [07:17<02:39,  2.25s/it]\u001b[A\n",
      "Running loss: 3.486046█████████████████▋      | 196/266 [07:19<02:36,  2.24s/it]\u001b[A\n",
      "Running loss: 3.589564█████████████████▊      | 197/266 [07:21<02:34,  2.24s/it]\u001b[A\n",
      "Running loss: 3.361923█████████████████▊      | 198/266 [07:24<02:32,  2.24s/it]\u001b[A\n",
      "Running loss: 3.393617█████████████████▉      | 199/266 [07:26<02:29,  2.23s/it]\u001b[A\n",
      "Running loss: 3.698326██████████████████      | 200/266 [07:28<02:27,  2.24s/it]\u001b[A\n",
      "Running loss: 3.502585██████████████████▏     | 201/266 [07:30<02:25,  2.23s/it]\u001b[A\n",
      "Running loss: 3.491693██████████████████▏     | 202/266 [07:33<02:23,  2.24s/it]\u001b[A\n",
      "Running loss: 3.492625██████████████████▎     | 203/266 [07:35<02:21,  2.24s/it]\u001b[A\n",
      "Running loss: 3.884349██████████████████▍     | 204/266 [07:37<02:18,  2.24s/it]\u001b[A\n",
      "Running loss: 3.691956██████████████████▍     | 205/266 [07:39<02:16,  2.24s/it]\u001b[A\n",
      "Running loss: 3.428030██████████████████▌     | 206/266 [07:42<02:14,  2.25s/it]\u001b[A\n",
      "Running loss: 3.593771██████████████████▋     | 207/266 [07:44<02:12,  2.24s/it]\u001b[A\n",
      "Running loss: 3.854661██████████████████▊     | 208/266 [07:46<02:09,  2.23s/it]\u001b[A\n",
      "Running loss: 3.475698██████████████████▊     | 209/266 [07:48<02:07,  2.24s/it]\u001b[A\n",
      "Running loss: 3.590592██████████████████▉     | 210/266 [07:51<02:05,  2.24s/it]\u001b[A\n",
      "Running loss: 3.497056███████████████████     | 211/266 [07:53<02:02,  2.24s/it]\u001b[A\n",
      "Running loss: 3.748821███████████████████▏    | 212/266 [07:55<02:01,  2.24s/it]\u001b[A\n",
      "Running loss: 3.380934███████████████████▏    | 213/266 [07:57<01:58,  2.24s/it]\u001b[A\n",
      "Running loss: 3.423829███████████████████▎    | 214/266 [08:00<01:56,  2.25s/it]\u001b[A\n",
      "Running loss: 3.547565███████████████████▍    | 215/266 [08:02<01:54,  2.25s/it]\u001b[A\n",
      "Running loss: 3.476053███████████████████▍    | 216/266 [08:04<01:52,  2.25s/it]\u001b[A\n",
      "Running loss: 3.939791███████████████████▌    | 217/266 [08:06<01:50,  2.25s/it]\u001b[A\n",
      "Running loss: 3.609024███████████████████▋    | 218/266 [08:09<01:48,  2.26s/it]\u001b[A\n",
      "Running loss: 3.290210███████████████████▊    | 219/266 [08:11<01:45,  2.25s/it]\u001b[A\n",
      "Running loss: 3.734245███████████████████▊    | 220/266 [08:13<01:43,  2.25s/it]\u001b[A\n",
      "Running loss: 3.803663███████████████████▉    | 221/266 [08:15<01:41,  2.24s/it]\u001b[A\n",
      "Running loss: 3.597159████████████████████    | 222/266 [08:18<01:38,  2.24s/it]\u001b[A\n",
      "Running loss: 3.586133████████████████████    | 223/266 [08:20<01:36,  2.24s/it]\u001b[A\n",
      "Running loss: 3.606621████████████████████▏   | 224/266 [08:22<01:34,  2.24s/it]\u001b[A\n",
      "Running loss: 3.692593████████████████████▎   | 225/266 [08:24<01:32,  2.25s/it]\u001b[A\n",
      "Running loss: 3.533951████████████████████▍   | 226/266 [08:27<01:30,  2.25s/it]\u001b[A\n",
      "Running loss: 3.827588████████████████████▍   | 227/266 [08:29<01:27,  2.25s/it]\u001b[A\n",
      "Running loss: 3.780291████████████████████▌   | 228/266 [08:31<01:25,  2.25s/it]\u001b[A\n",
      "Running loss: 3.571990████████████████████▋   | 229/266 [08:33<01:22,  2.24s/it]\u001b[A\n",
      "Running loss: 3.807088████████████████████▊   | 230/266 [08:36<01:20,  2.24s/it]\u001b[A\n",
      "Running loss: 3.641552████████████████████▊   | 231/266 [08:38<01:18,  2.24s/it]\u001b[A\n",
      "Running loss: 3.414850████████████████████▉   | 232/266 [08:40<01:16,  2.24s/it]\u001b[A\n",
      "Running loss: 3.831720█████████████████████   | 233/266 [08:42<01:13,  2.24s/it]\u001b[A\n",
      "Running loss: 3.536300█████████████████████   | 234/266 [08:45<01:11,  2.25s/it]\u001b[A\n",
      "Running loss: 3.366492█████████████████████▏  | 235/266 [08:47<01:09,  2.24s/it]\u001b[A\n",
      "Running loss: 3.725624█████████████████████▎  | 236/266 [08:49<01:07,  2.25s/it]\u001b[A\n",
      "Running loss: 3.482903█████████████████████▍  | 237/266 [08:51<01:04,  2.24s/it]\u001b[A\n",
      "Running loss: 3.617116█████████████████████▍  | 238/266 [08:53<01:02,  2.24s/it]\u001b[A\n",
      "Running loss: 3.803805█████████████████████▌  | 239/266 [08:56<01:00,  2.24s/it]\u001b[A\n",
      "Running loss: 3.780881█████████████████████▋  | 240/266 [08:58<00:58,  2.25s/it]\u001b[A\n",
      "Running loss: 3.697334█████████████████████▋  | 241/266 [09:00<00:56,  2.24s/it]\u001b[A\n",
      "Running loss: 3.640245█████████████████████▊  | 242/266 [09:02<00:53,  2.24s/it]\u001b[A\n",
      "Running loss: 3.492630█████████████████████▉  | 243/266 [09:05<00:52,  2.29s/it]\u001b[A\n",
      "Running loss: 3.388576██████████████████████  | 244/266 [09:07<00:50,  2.28s/it]\u001b[A\n",
      "Running loss: 3.434927██████████████████████  | 245/266 [09:09<00:47,  2.26s/it]\u001b[A\n",
      "Running loss: 3.673299██████████████████████▏ | 246/266 [09:12<00:45,  2.26s/it]\u001b[A\n",
      "Running loss: 3.630603██████████████████████▎ | 247/266 [09:14<00:42,  2.25s/it]\u001b[A\n",
      "Running loss: 3.523982██████████████████████▍ | 248/266 [09:16<00:40,  2.24s/it]\u001b[A\n",
      "Running loss: 3.516264██████████████████████▍ | 249/266 [09:18<00:38,  2.25s/it]\u001b[A\n",
      "Running loss: 3.719332██████████████████████▌ | 250/266 [09:21<00:35,  2.25s/it]\u001b[A\n",
      "Running loss: 3.586469██████████████████████▋ | 251/266 [09:23<00:33,  2.25s/it]\u001b[A\n",
      "Running loss: 3.448092██████████████████████▋ | 252/266 [09:25<00:31,  2.25s/it]\u001b[A\n",
      "Running loss: 3.528883██████████████████████▊ | 253/266 [09:27<00:29,  2.25s/it]\u001b[A\n",
      "Running loss: 3.538435██████████████████████▉ | 254/266 [09:30<00:27,  2.26s/it]\u001b[A\n",
      "Running loss: 3.625338███████████████████████ | 255/266 [09:32<00:24,  2.25s/it]\u001b[A\n",
      "Running loss: 3.489028███████████████████████ | 256/266 [09:34<00:22,  2.25s/it]\u001b[A\n",
      "Running loss: 3.309607███████████████████████▏| 257/266 [09:36<00:20,  2.25s/it]\u001b[A\n",
      "Running loss: 3.726869███████████████████████▎| 258/266 [09:39<00:17,  2.24s/it]\u001b[A\n",
      "Running loss: 3.968659███████████████████████▎| 259/266 [09:41<00:15,  2.24s/it]\u001b[A\n",
      "Running loss: 3.666569███████████████████████▍| 260/266 [09:43<00:13,  2.24s/it]\u001b[A\n",
      "Running loss: 3.494933███████████████████████▌| 261/266 [09:45<00:11,  2.25s/it]\u001b[A\n",
      "Running loss: 3.671666███████████████████████▋| 262/266 [09:48<00:08,  2.25s/it]\u001b[A\n",
      "Running loss: 3.539416███████████████████████▋| 263/266 [09:50<00:06,  2.25s/it]\u001b[A\n",
      "Running loss: 3.439602███████████████████████▊| 264/266 [09:52<00:04,  2.25s/it]\u001b[A\n",
      "Running loss: 3.698367███████████████████████▉| 265/266 [09:54<00:02,  2.25s/it]\u001b[A\n",
      "Epoch 3 of 3: 100%|██████████████████████████████| 3/3 [30:39<00:00, 613.92s/it]\u001b[A\n",
      "INFO:simpletransformers.language_modeling.language_modeling_model: Training of gpt2 model complete. Saved to outputs/fine-tuned/.\n",
      "INFO:simpletransformers.language_modeling.language_modeling_utils: Creating features from dataset file at cache_dir/\n",
      "100%|███████████████████████████████████████| 983/983 [00:00<00:00, 1285.39it/s]\n",
      "100%|█████████████████████████████████████| 323/323 [00:00<00:00, 434357.23it/s]\n",
      "INFO:simpletransformers.language_modeling.language_modeling_utils: Saving features into cached file cache_dir/gpt2_cached_lm_508_test.txt\n",
      "Running Evaluation:  98%|██████████████████████▍| 40/41 [00:32<00:00,  1.23it/s]Traceback (most recent call last):\n",
      "  File \"fine_tune.py\", line 26, in <module>\n",
      "    model.eval_model(\"data/test.txt\")\n",
      "  File \"/anaconda/envs/azureml_py36/lib/python3.6/site-packages/simpletransformers/language_modeling/language_modeling_model.py\", line 770, in eval_model\n",
      "    result = self.evaluate(eval_dataset, output_dir, verbose=verbose, silent=silent, **kwargs)\n",
      "  File \"/anaconda/envs/azureml_py36/lib/python3.6/site-packages/simpletransformers/language_modeling/language_modeling_model.py\", line 814, in evaluate\n",
      "    outputs = model(inputs, masked_lm_labels=labels) if args.mlm else model(inputs, labels=labels)\n",
      "  File \"/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 156, in forward\n",
      "    return self.gather(outputs, self.output_device)\n",
      "  File \"/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/nn/parallel/data_parallel.py\", line 168, in gather\n",
      "    return gather(outputs, output_device, dim=self.dim)\n",
      "  File \"/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\", line 68, in gather\n",
      "    res = gather_map(outputs)\n",
      "  File \"/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\", line 63, in gather_map\n",
      "    return type(out)(map(gather_map, zip(*outputs)))\n",
      "  File \"/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\", line 63, in gather_map\n",
      "    return type(out)(map(gather_map, zip(*outputs)))\n",
      "  File \"/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/nn/parallel/scatter_gather.py\", line 55, in gather_map\n",
      "    return Gather.apply(target_device, dim, *outputs)\n",
      "  File \"/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py\", line 68, in forward\n",
      "    return comm.gather(inputs, ctx.dim, ctx.target_device)\n",
      "  File \"/anaconda/envs/azureml_py36/lib/python3.6/site-packages/torch/cuda/comm.py\", line 165, in gather\n",
      "    return torch._C._gather(tensors, dim, destination)\n",
      "RuntimeError: Gather got an input of invalid size: got [2, 1, 12, 508, 64], but expected [2, 2, 12, 508, 64]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python fine_tune.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prev max_length:  500\n",
      "WARNING:transformers.modeling_utils:Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "=============================================================================\n",
      "[\"Facebook founder and CEO Mark Zuckerberg hopes that the social media platform will spur around four million people to sign up as new voters in the upcoming 2020 U.S. Presidential Elections. Should this be true, a figure of four million new voters would be double the number of new voters the company claimed to have helped register before the 2016 Presidential Elections and the 2018 U.S. Midterms.A new poll for the U.S. News & World Report reveals that 42% of Americans, or 1.3 million, now live in the Northeast. Nearly half of Americans are from the South. And the gap is widening at the edges. Just under 42% are from the West. And that's the only part of America that is growing older: the top half of a share of people over 65 have a child born in the U.S. (a family whose birth country is not represented on the United States Census).                   It's a big deal that people don't even know that the American voter is still in the top five percent of each demographic group. While there is a big jump in support for Democrats in the Northeast, there are growing signs that they are growing less popular among Democrats nationwide.  On the one hand, that was reported by NBC News last month, which noted that the Democrats have now slipped out of the top five percent by three percentage points in the United States. This was down from last month, which saw that Democrats received 41% of the Latino vote in the Republican National Convention. This month it looked like that percentage would climb again, to 45%. And in a survey conducted by U.S. News & World Report, 51% of Americans said they were undecided when the poll was released, down from 51% when it was conducted in August 2013.  Now, there is concern that Democrats may actually have lost their grip on the top five percent of the population in key swing states like Florida and Pennsylvania, which could mean that many may be drawn to them on the other hand, and thus will have to rely on the party that controls them to win the White House. But for now, Republicans in South Carolina have a lead of nine percentage points over Democrats in the state. In many ways, the two biggest Democratic victories this year were in the South Carolina primary and the New Hampshire primary, where they beat each other in the Democratic caucuses. But South Carolina is also home to more young Republicans like Mike Huckabee and Bobby Jindal, and Republicans can also take advantage of those young voters by picking them up in early voting states like Ohio, Florida and Ohio. This is a huge step toward getting into the Democratic Party, since Republicans are losing their best and brightest. As this new poll shows, young people are turning out in droves in November for a reason: The state is\"]\n",
      "=============================================================================\n",
      "prev max_length:  500\n",
      "WARNING:transformers.modeling_utils:Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "=============================================================================\n",
      "['Neutrinos are perhaps one of the most elusive yet ubiquitous particles around us. Researchers at CERN have invested heavily in detecting these ghastly particles with the T2K experiment, which is a leading neutrino oscillation experiment in Japan. However, scientists are looking to upgrade the experiment’s detector to yield more precise results. Plastic scintillators are frequently employed in such neutrino oscillation experiments, where they reconstruct the final state of the neutrino interaction. The upgraded detector requires a two-tonne polystyrene-based plastic scintillator detector that is segmented into 1 cm^3 cubes. These small cubes yield precise results but require finer granularity which ultimately makes the detector assembly harder. So, it is important that CERN upgrade its detector to get the next generation of neutrino detectors into the lab. A significant step forward would be to get the detector more accurate and to produce the new scintillation tubes which would allow the new experiments to be made in the new space. An even more important step would be to expand the testing of the detector by three days or longer for each sample and the results to be submitted to a peer-reviewed journal. With the results in hand, future CERN neutrino oscillations could make it easier to find and control neutrino in the future. In total, 1,500 particles from 15 nuclei of neutrinos were detected over the course of four weeks, representing more than 100 billion new neutrino detectors.’ The announcement was made after a weeklong meeting of the international community of neutrino scientists and scientists at CERN at Cern.’ The meeting concluded with the following announcement:’\"’’ In the coming months, in preparation, CERN scientists shall work toward further improvement of its detector.’ For now, it is necessary to concentrate on the next phase of the project that will be a major step forward to the discovery of the new nuclei and the results obtained.’ For all practical purposes, CERN has decided to initiate the first phase of a long-term project to detect neutrinos.’ However, until a scientific result has been obtained, the results are only accessible to a small number of neutrinos. The aim of the project is to collect and analyze neutrinos in large concentrations, to increase the sensitivity of the detector and increase the potential for more rapid detection of neutrino in the future.’’’ There is a long way to go before the new nuclei and nuclei will be detected. For example, the current detection accuracy is not very reliable; there are many candidates and many errors in the detector which could be exploited by the scientific community. Thus, the project is important for a wide range of scientific research objectives, including neutrino detection detection and neutrino chemistry experiments. However, the results are only one part of a larger task at CERN’s new scientific facility.’ There is already a considerable amount of work under way to make observations and detect neutrinos. However, there is still much work that needs to be done before the new neutr']\n",
      "=============================================================================\n",
      "prev max_length:  500\n",
      "WARNING:transformers.modeling_utils:Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "=============================================================================\n",
      "[\"Nvidia, the American tech giant has partnered up with renowned German automobile manufacturer Mercedes-Benz to develop the most sophisticated and advanced computing architecture ever deployed in an automobile. The announcement came after Mercedes-Benz CEO Ola Källenius and Nvidia founder and CEO Jensen Huang met in a live-streamed media event. Both confirmed that the next generation of Nvidia graphics chips will be the next generation of GPUs designed to support multi-GPU architectures.       I am still not sold on Nvidia's decision to go public with their new, new graphics architecture. Not much has changed about NVIDIA in the past year and a half. It does seem that they are moving in a much more thoughtful direction than they were at the beginning. In 2011, the company was the most profitable company in the company, with a profit of nearly $1.7 billion (US$11.7 billion), and the largest sales group of its time. This was mostly the result of high-end graphics cards, such as the NVIDIA GeForce 1060. It also allowed them to bring performance to those with low-end chipsets such as the GeForce GTX 1080 Ti and the GTX 970. The company was able to keep its share price low, but then the stock fell to under $20 per share. In 2012, the company announced it was cutting back on its graphics support, including support of DirectX 12 in its next generation of chips. Although they weren't able to take advantage of AMD's decision to invest in AMD and Nvidia's GPUs, they were able to continue growing the company by announcing the availability of new, smaller graphics cards such as the NVIDIA Quadro graphics cards, which will also be offered alongside AMD's new Pascal chips.  Now, all of that has changed. NVIDIA announced that its new flagship GeForce GTX 790 is ready for its launch on October 26th. The company expects that it will become the first GPU with NVIDIA's Pascal and Pascal-based technology in about 5 years to reach market power, with a single-chip architecture. The new Radeon 900 series chips are expected to be released later this year.  The company's decision to make up for the $40 per watt loss that NVIDIA announced to the public was the first of its kind in a long time in a high-end graphics card company. But this is not good news for the company.  Not only does AMD struggle to meet its customer requirements for its high-end GPUs and their high-end graphics processing units, it is also struggling to maintain a manufacturing budget for its new and smaller chipsets.  The company has long struggled to meet customer demand for its high-end chipmakers like Nvidia's NVLink line of GPUs that were designed to beat the price of traditional PCI Express cards. But the company is considering moving the production\"]\n",
      "=============================================================================\n",
      "prev max_length:  500\n",
      "WARNING:transformers.modeling_utils:Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "=============================================================================\n",
      "['Zebra has been living in a homeless shelter for over 8 years - from the start of this summer. When we were in a small homeless shelter - we had to sit on the corner of the porch in front of the shelter, waiting on a group of people waiting for them to leave. It was the quietest, most uneventful time we had ever been in New York City. The staff had been working late this morning, just as he was preparing for a shift, waiting for all of us in a small tent at an entrance called the Little Havana Hotel and Casino. In the back was a little dingy shack that wasn\\'t used yet. My father and I both had their own tents, but because the rooms are so small, we couldn\\'t see the rest of the room. When we got home they put me in a plastic box, in the shape of a big plastic bag. To me it looked like the box was for a movie I watched (which was about 10 minutes long!), or to put it another way, a movie of me and the people I was with on stage.   Now I live in the living room that once was his home, in a room where the television is kept, and where the computer goes. As we sat there, there was nowhere to go, and we watched the movie in a darkened room. Then came the movie about how it\\'s like there\\'s no one there, and I was like, \"what?\" - and then the movie about a little guy on a bed who gets out and takes all the money from a bank he stole from and then just goes out and shows up again. The movie about it being about money is very entertaining. For me it\\'s the perfect time to work out how I\\'ve always been able to do this - and then come the movies.  In the back of my mind, I thought that was the perfect time to go to the movies. Then the movie came along - this time I had a couple of times to go where I never went when I went to the movies before. It was like when I get to sleep a lot without a whole lot of sleep. I was just trying to go to sleep at night, like I never did before - and then I realized, \\'why can\\'t I sleep? It just doesn\\'t feel good.\\' I didn\\'t think about it, or anything, except for my parents and siblings who were sitting there in front of me and staring at my phone, waiting for']\n",
      "=============================================================================\n"
     ]
    }
   ],
   "source": [
    "!python generate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
